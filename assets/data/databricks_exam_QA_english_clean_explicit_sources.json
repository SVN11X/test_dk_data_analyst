[
  {
    "question": "A data analyst has been tasked with optimizing a Databricks SQL query for a large dataset. What should you consider when trying to improve query performance?",
    "options": [
      "Increasing the size of the cluster to handle the data",
      "Partitioning the data into smaller chunks",
      "Using a higher level of parallelism for the query",
      "Increasing the timeout for the query"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Partitioning the data can significantly improve query performance by allowing the system to read only relevant partitions.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which layer of the Medallion Architecture is responsible for providing a unified view of data from various sources?",
    "options": [
      "Bronze layer",
      "Silver layer",
      "Gold layer",
      "None of the above"
    ],
    "answer": 2,
    "category": "Medallion Architecture",
    "explanation": "The Gold layer provides a unified, refined view of data ready for consumption in analytics and reporting.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "A senior data analyst for a retail company wants to create a dashboard to track sales performance. He is deciding whether the company should invest in Databricks SQL to aid with the requirement. Which of the following features of Databricks SQL would be most helpful to take the decision?",
    "options": [
      "The ability to query data across multiple data sources",
      "The ability to ingest streaming data in real-time",
      "The ability to create visualizations using BI tools such as Tableau and Power BI",
      "The ability to analyze unstructured data such as customer reviews"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "Databricks SQL's integration with BI tools facilitates creating dashboards to track sales performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst has been asked to create a Databricks SQL query that will summarize sales data by product category and month. Which SQL function can you use to accomplish this?",
    "options": [
      "AVG",
      "SUM",
      "GROUP BY",
      "ORDER BY"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "The GROUP BY clause is used to aggregate data across specified columns, allowing summarization by product category and month.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What is the purpose of the Medallion Architecture in data processing?",
    "options": [
      "To provide a layered approach to data refinement",
      "To enforce security protocols across data pipelines",
      "To integrate machine learning models into data flows",
      "To visualize data using built-in dashboards"
    ],
    "answer": 0,
    "category": "Medallion Architecture",
    "explanation": "The Medallion Architecture structures data processing into Bronze, Silver, and Gold layers for systematic refinement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What features does Data Explorer in Databricks offer to simplify the management of data, and how do they improve the data management process?",
    "options": [
      "Data Explorer provides a visual interface for creating and managing tables, making it easier to navigate and organize data.",
      "Data Explorer allows users to create and edit SQL queries directly within the interface, reducing the need to switch between different tools.",
      "Data Explorer offers data profiling and visualization tools that can help users better understand the structure and content of their data.",
      "All of the above"
    ],
    "answer": 3,
    "category": "Data Management",
    "explanation": "All listed features are offered by Data Explorer, enhancing data management by simplifying navigation, querying, and understanding data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst at a healthcare company is tasked with managing a Databricks table containing personally identifiable information (PII) data, including patients' names and medical histories. The analyst wants to ensure that only authorized personnel can access the table. Which of the following Databricks tools can the analyst use to enforce table ownership and restrict access to the PII data?",
    "options": [
      "Delta Lake",
      "Access Control Lists",
      "Apache Spark",
      "Structured Streaming"
    ],
    "answer": 1,
    "category": "Data Security",
    "explanation": "Access Control Lists (ACLs) allow the analyst to set permissions, ensuring only authorized users can access sensitive data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A healthcare organization has a Lakehouse that stores data on patient appointments. The data analyst needs to find the average duration of appointments for each doctor. Which of the following SQL statements will return the correct results?",
    "options": [
      "SELECT doctor_id, AVG(duration) as avg_duration FROM appointments GROUP BY doctor_id;",
      "SELECT doctor_id, AVG(duration) as avg_duration FROM appointments GROUP BY doctor_id HAVING avg_duration > 0;",
      "SELECT doctor_id, SUM(duration)/COUNT() as avg_duration FROM appointments GROUP BY doctor_id;",
      "SELECT doctor_id, duration/COUNT() as avg_duration FROM appointments GROUP BY doctor_id;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Using AVG() function and grouping by doctor_id calculates the average duration per doctor.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A manufacturing company wants to use data from sensors installed on the machinery to continually monitor the performance of its production line. Which of the following Databricks SQL features would be most beneficial in this situation?",
    "options": [
      "Databricks SQL can be used to ingest streaming data in real-time.",
      "Databricks SQL can be used to design and create visualizations using BI tools.",
      "Databricks SQL can be used to query data across multiple data sources.",
      "Databricks SQL can be used to handle unstructured data."
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "The ability to ingest streaming data in real-time is crucial for monitoring live sensor data from machinery.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst of a large online retailer wants to integrate Databricks SQL with Partner Connect to obtain real-time data on customer behavior from a social media platform. Which of the following steps would the data analyst take to achieve the desired outcome?",
    "options": [
      "Use Databricks SQL to ingest the data from the social media platform and then connect it to Partner Connect.",
      "Use Partner Connect to ingest the data from the social media platform and then connect it to Databricks SQL.",
      "Use an ETL tool to ingest the data from the social media platform and then connect it to both Partner Connect and Databricks SQL.",
      "Use an API to ingest the data from the social media platform and then connect it to both Partner Connect and Databricks SQL."
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Partner Connect should be used to ingest data from the social media platform and then connect it to Databricks SQL for analysis.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following statements about the silver layer in the medallion architecture is true?",
    "options": [
      "The silver layer is where data is transformed and processed for analytics use",
      "The silver layer is where raw data is stored in its original format",
      "The silver layer is optimized for fast querying",
      "The silver layer is the largest of the three layers"
    ],
    "answer": 0,
    "category": "Medallion Architecture",
    "explanation": "In the silver layer, data is cleaned and transformed, making it ready for analytics purposes.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature enables tracking and managing machine learning experiments?",
    "options": [
      "Delta Lake",
      "MLflow",
      "Koalas",
      "GraphFrames"
    ],
    "answer": 1,
    "category": "Machine Learning",
    "explanation": "MLflow is an open-source platform for managing the ML lifecycle, including experimentation and deployment.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of using Delta Lake over traditional data lakes?",
    "options": [
      "Higher storage costs",
      "ACID transactions and data versioning",
      "Limited compatibility with data processing tools",
      "Inability to handle structured data"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake provides ACID compliance and data versioning, improving data reliability and consistency.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A company needs to analyze a large amount of data stored in its Hadoop cluster. Which of the following best describes the benefit of using Databricks SQL with a Hadoop cluster?",
    "options": [
      "Databricks SQL provides faster query processing than traditional Hadoop tools.",
      "Databricks SQL allows users to store and analyze data directly in Hadoop.",
      "Databricks SQL provides more advanced security features than Hadoop.",
      "Databricks SQL provides better support for unstructured data than Hadoop."
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Databricks SQL offers faster query processing due to its optimized engine, which is more efficient than traditional Hadoop tools.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following statements accurately describes the role of Delta Lake in the architecture of Databricks SQL?",
    "options": [
      "Delta Lake provides data ingestion capabilities for Databricks SQL.",
      "Delta Lake is a data storage layer that provides high-performance querying capabilities for Databricks SQL.",
      "Delta Lake is a transactional storage layer that provides ACID compliance for data processing in Databricks SQL.",
      "Delta Lake provides integration capabilities for Databricks SQL with other BI tools and platforms."
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Delta Lake ensures data reliability and integrity by providing ACID transactions in data processing.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A healthcare company stores patient information in a table in Databricks. The company needs to ensure that only authorized personnel can access the table. Which of the following actions would best address this security concern?",
    "options": [
      "Assigning table ownership to a generic company account",
      "Granting access to the table to all employees",
      "Implementing role-based access control with specific privileges assigned to individual users",
      "Storing the patient information in an unsecured Excel file"
    ],
    "answer": 2,
    "category": "Data Security",
    "explanation": "Role-based access control allows precise permission settings, ensuring only authorized personnel can access sensitive data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A large retail company has a Lakehouse that stores data on purchases made by their stores. The data analyst needs to find the total revenue generated by each store for January. Which of the following SQL statements will return the correct results?",
    "options": [
      "SELECT store_id, SUM(total_sales) as revenue FROM purchase WHERE date >= '2023-01-01' AND date <= '2023-01-31' GROUP BY store_id ORDER BY revenue DESC;",
      "SELECT store_id, SUM(total_sales) as revenue FROM purchase WHERE date BETWEEN '2023-01-01' AND '2023-01-31' GROUP BY store_id ORDER BY revenue DESC LIMIT 5;",
      "SELECT store_id, SUM(total_sales) as revenue FROM purchase WHERE date >= '2023-01-01' AND date <= '2023-01-31' GROUP BY store_id HAVING revenue > 0 ORDER BY revenue DESC;",
      "SELECT store_id, SUM(total_sales) as revenue FROM purchase WHERE date >= '2023-01-01' AND date <= '2023-01-31' GROUP BY store_id HAVING revenue > 0 ORDER BY revenue ASC;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A correctly calculates total revenue per store for January without unnecessary clauses.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which language(s) can be used within notebooks for data analysis?",
    "options": [
      "Python",
      "R",
      "SQL",
      "All of the above"
    ],
    "answer": 3,
    "category": "Data Analysis",
    "explanation": "Databricks notebooks support multiple languages including Python, R, and SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst has been given a requirement of creating a Delta Lake table in Databricks that can be efficiently queried using a specific column as the partitioning column. Which data format and partitioning strategy should the data analyst choose?",
    "options": [
      "Parquet file format and partition by hash",
      "Delta file format and partition by range",
      "ORC file format and partition by list",
      "CSV file format and partition by round-robin"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Using Parquet format and partitioning by hash on a specific column allows for efficient querying and data distribution.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst needs to find out the top 5 customers based on the total amount they spent on purchases in the last 30 days from the sales table. Which of the following Databricks SQL statements will yield the correct result?",
    "options": [
      "SELECT TOP 5 customer_id, SUM(price) as total_spent FROM sales WHERE date >= DATEADD(day, -30, GETDATE()) GROUP BY customer_id ORDER BY total_spent DESC;",
      "SELECT customer_id, SUM(price) as total_spent FROM sales WHERE date >= DATEADD(day, -30, GETDATE()) GROUP BY customer_id ORDER BY total_spent DESC LIMIT 5;",
      "SELECT customer_id, SUM(price) as total_spent FROM sales WHERE date >= DATEADD(day, -30, GETDATE()) GROUP BY customer_id HAVING total_spent > 0 ORDER BY total_spent DESC LIMIT 5;",
      "SELECT customer_id, SUM(price) as total_spent FROM sales WHERE date BETWEEN DATEADD(day, -30, GETDATE()) AND GETDATE() GROUP BY customer_id ORDER BY total_spent DESC LIMIT 5;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Using LIMIT 5 after ORDER BY returns the top 5 customers based on total_spent in the last 30 days.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst has created a Delta Lake table in Databricks and wants to OPTIMIZE the performance of queries that filter on a specific column. Which Delta Lake feature should the data analyst use to improve query performance?",
    "options": [
      "Indexing",
      "Partitioning",
      "Caching",
      "Z-Ordering"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "Z-Ordering organizes data files based on column values, improving the performance of queries that filter on that column.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Delta Lake supports schema evolution, which allows for changes to the schema of a table without requiring a full rewrite of the table. Which of the following is not a supported schema evolution operation?",
    "options": [
      "Adding a new column",
      "Removing a column",
      "Renaming a column",
      "Changing the data type of a column"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Removing a column is not supported in schema evolution due to potential data loss and incompatibility.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst is working on a project to analyze a large dataset using Databricks SQL. The dataset is too large to fit in memory, so the analyst needs to use a distributed computing approach. Which Databricks SQL feature will best suit their needs?",
    "options": [
      "Dashboards",
      "Medallion architecture",
      "Compute",
      "Streaming data"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "The Compute feature provides scalable distributed computing resources suitable for large datasets.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst wants to create a view in Databricks that displays only the top 10% of customers based on their total spending. Which SQL query would achieve this goal?",
    "options": [
      "SELECT * FROM customers ORDER BY total_spend DESC LIMIT 10%",
      "SELECT * FROM customers WHERE total_spend > PERCENTILE(total_spend, 90)",
      "SELECT * FROM customers WHERE total_spend > (SELECT PERCENTILE(total_spend, 90) FROM customers)",
      "SELECT * FROM customers ORDER BY total_spend DESC OFFSET 10%"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C correctly filters customers whose total_spend exceeds the 90th percentile.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "A data analyst is working with a Delta Lake table which includes changing the data types of a column. Which SQL statement should the data analyst use to modify the column data type?",
    "options": [
      "ALTER TABLE table_name ADD COLUMN column_name datatype",
      "ALTER TABLE table_name DROP COLUMN column_name",
      "ALTER TABLE table_name ALTER COLUMN column_name datatype",
      "ALTER TABLE table_name RENAME COLUMN column_name TO new_column_name"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "The ALTER COLUMN command is used to modify the data type of an existing column in a table.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst needs to calculate the 90th percentile of sales amounts in a large dataset. Which function should they use in Databricks SQL?",
    "options": [
      "PERCENTILE()",
      "APPROX_PERCENTILE()",
      "MEDIAN()",
      "MODE()"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "APPROX_PERCENTILE() efficiently computes percentiles on large datasets.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What is the primary advantage of using Z-Ordering in Delta Lake?",
    "options": [
      "It compresses the data to save storage space",
      "It optimizes data skipping for faster query performance",
      "It replicates data for high availability",
      "It encrypts data for security purposes"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Z-Ordering improves data skipping by clustering similar data together, enhancing query performance.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst has created a view in Databricks that references multiple tables in different databases. The data analyst wants to ensure that the view is always up to date with the latest data in the underlying tables. Which of the following Databricks feature should the data analyst use to achieve this?",
    "options": [
      "Materialized views",
      "Delta caches",
      "Databricks Delta streams",
      "Databricks SQL Analytics"
    ],
    "answer": 2,
    "category": "Data Management",
    "explanation": "Databricks Delta streams allow continuous updates to data, ensuring views reflect the latest data from underlying tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "A data analyst in a healthcare company has recently started using Databricks SQL. Her team is struggling to OPTIMIZE query performance on large datasets. What can the data analyst do to improve query performance in Databricks SQL?",
    "options": [
      "Use caching to store frequently used data in memory and reduce query execution time",
      "Use distributed query processing to parallelize query execution across multiple nodes",
      "OPTIMIZE table partitions and indexes to improve query performance",
      "All of the above"
    ],
    "answer": 3,
    "category": "Databricks SQL",
    "explanation": "All the listed methods can collectively improve query performance on large datasets.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Delta Lake provides many benefits over traditional data lakes. In which of the following scenarios would Delta Lake not be the best choice?",
    "options": [
      "When data is mostly unstructured and does not require any schema enforcement",
      "When data is primarily accessed through batch processing",
      "When data is stored in a single file and does not require partitioning",
      "When data requires frequent updates and rollbacks"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake excels in streaming and interactive queries; for pure batch processing, traditional data lakes may suffice.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which feature of Databricks allows data analysts to create and manage continuous data pipelines with ease?",
    "options": [
      "Delta Lake",
      "Databricks SQL",
      "Delta Live Tables",
      "MLflow"
    ],
    "answer": 2,
    "category": "Data Engineering",
    "explanation": "Delta Live Tables simplifies building and managing data pipelines with declarative configurations.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "What does ACID stand for in Delta Lake?",
    "options": [
      "Atomicity, Consistency, Isolation, Durability",
      "Automation, Consistency, Isolation, Durability",
      "Atomicity, Control, Integrity, Durability",
      "Automation, Control, Integrity, Durability"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "ACID in Delta Lake stands for Atomicity, Consistency, Isolation, and Durability, which are key properties for reliable transactions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Qual comando SQL é usado para alterar o nome de uma table existente?",
    "options": [
      "ALTER TABLE... RENAME TO",
      "UPDATE TABLE... SET NAME",
      "RENAME TABLE... TO",
      "CHANGE TABLE NAME TO"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'ALTER TABLE... RENAME TO' é usado para renomear uma table existente no SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is 'Photon'?",
    "options": [
      "A high-performance engine for executing SQL queries",
      "A data governance tool",
      "A machine learning library",
      "A visualization tool"
    ],
    "answer": 0,
    "category": "Optimization",
    "explanation": "'Photon' is a native execution engine in Databricks that enhances the performance of SQL queries.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'COUNT()' function do in SQL?",
    "options": [
      "Counts the number of rows in a result set",
      "Calculates the sum of a column",
      "Returns the average value of a column",
      "Finds the maximum value in a column"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "'COUNT()' returns the number of rows that match a specified condition in a SQL query.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'Job Scheduler'?",
    "options": [
      "To manage data streams",
      "To schedule and monitor the execution of jobs",
      "To monitor machine learning models",
      "To partition datasets"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "The 'Job Scheduler' is used to schedule and monitor jobs, such as notebooks or scripts, in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "What is the purpose of 'Cache()' in Apache Spark?",
    "options": [
      "To store intermediate results in memory for faster access",
      "To partition data across nodes",
      "To OPTIMIZE SQL queries",
      "To remove unused data"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'Cache()' is used to store intermediate RDDs or DataFrames in memory for faster access during subsequent operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'MERGE INTO' command do in Delta Lake?",
    "options": [
      "Combines multiple tables into one",
      "Inserts, updates, or deletes data in a table based on specified conditions",
      "Deletes old versions of data",
      "Partitions data for faster queries"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'MERGE INTO' allows users to perform upserts (inserts, updates, and deletes) in a Delta Lake table based on specified conditions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Delta Live Tables' feature used for?",
    "options": [
      "To execute batch processing jobs",
      "To create and manage real-time data pipelines",
      "To store metadata and configurations",
      "To manage security settings for data clusters"
    ],
    "answer": 1,
    "category": "Real-Time Processing",
    "explanation": "'Delta Live Tables' is used for creating and managing real-time data pipelines, allowing continuous data processing.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "Which feature in Databricks is used to track and manage machine learning experiments?",
    "options": [
      "Photon Engine",
      "MLflow",
      "Structured Streaming",
      "Delta Live Tables"
    ],
    "answer": 1,
    "category": "Machine Learning",
    "explanation": "MLflow is a tool integrated with Databricks for managing machine learning experiments and tracking model performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of a Job Cluster in Databricks?",
    "options": [
      "A cluster used for interactive analysis",
      "A cluster created to run a specific job and terminated after completion",
      "A cluster that runs continuously in the background",
      "A cluster used only for storage"
    ],
    "answer": 1,
    "category": "Job Management",
    "explanation": "A Job Cluster is created to execute a specific job and is automatically terminated after the job is completed.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "What does the 'REVOKE' command do in SQL?",
    "options": [
      "Grants permissions to a user",
      "Removes permissions from a user",
      "Creates a new user",
      "Updates data in a table"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "The 'REVOKE' command is used to remove previously granted permissions from a user or role.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary function of Databricks SQL?",
    "options": [
      "To create machine learning models",
      "To analyze data using SQL queries and create visualizations",
      "To manage distributed clusters",
      "To automate job scheduling"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Databricks SQL is primarily used for querying data using SQL and creating visualizations.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What does 'Lazy Evaluation' mean in Apache Spark?",
    "options": [
      "Operations are executed immediately",
      "Operations are delayed until an action is triggered",
      "Operations are optimized in real-time",
      "Data is split into partitions"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "In Spark, 'Lazy Evaluation' means that transformations are not executed until an action is called.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the function of 'DataFrames' in Apache Spark?",
    "options": [
      "To represent structured data with named columns",
      "To store unstructured data in key-value pairs",
      "To cache data for repeated access",
      "To schedule jobs for execution"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'DataFrames' in Apache Spark represent structured data in named columns, allowing efficient querying and manipulation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Qual é a finalidade do comando 'TRUNCATE TABLE' no SQL?",
    "options": [
      "Excluir uma table",
      "Remover todas as rows de uma table",
      "Atualizar registros específicos",
      "Reduzir o tamanho físico da table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O 'TRUNCATE TABLE' remove todas as rows de uma table de forma eficiente, mas mantém sua estrutura.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'ORDER BY' clause do in SQL?",
    "options": [
      "Filters data based on conditions",
      "Groups records based on values",
      "Limits the number of returned rows",
      "Sorts the results of a query"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "'ORDER BY' is used to sort the result set in either ascending or descending order based on one or more columns.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main purpose of 'Time Travel' in Delta Lake?",
    "options": [
      "To OPTIMIZE query performance",
      "To store machine learning models",
      "To query previous versions of data",
      "To delete old data"
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "'Time Travel' allows users to query earlier versions of data for auditing or recovery purposes.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of a 'Job Cluster'?",
    "options": [
      "To store data",
      "To run scheduled jobs and terminate automatically after completion",
      "To execute interactive queries",
      "To manage user permissions"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "A 'Job Cluster' in Databricks is created to run a job and is terminated once the job is completed.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which of the following Databricks features enables real-time stream processing?",
    "options": [
      "Delta Lake",
      "Spark SQL",
      "Structured Streaming",
      "Databricks Jobs"
    ],
    "answer": 2,
    "category": "Real-Time Processing",
    "explanation": "Structured Streaming allows real-time processing of data streams in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary benefit of using Delta Lake?",
    "options": [
      "Creating visualizations",
      "Supporting ACID transactions",
      "Training machine learning models",
      "Integrating with external databases"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake supports ACID transactions, ensuring data consistency and reliability.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which Databricks feature enables real-time stream processing?",
    "options": [
      "Delta Lake",
      "Structured Streaming",
      "Photon Engine",
      "SQL Analytics"
    ],
    "answer": 1,
    "category": "Real-Time Processing",
    "explanation": "Structured Streaming allows real-time stream processing of data in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature provides high-performance query execution for SQL workloads?",
    "options": [
      "Photon Engine",
      "Delta Live Tables",
      "Auto Scaling",
      "MLflow"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Photon Engine is a high-performance query execution engine optimized for running SQL workloads in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary role of the 'Cluster Manager' in Databricks?",
    "options": [
      "To manage the execution of SQL queries",
      "To create and monitor clusters for data processing",
      "To manage user permissions across clusters",
      "To partition data for better performance"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "'Cluster Manager' creates and monitors clusters, ensuring that they are running smoothly for data processing tasks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which feature allows Databricks users to track and manage machine learning models?",
    "options": [
      "SQL Analytics",
      "MLflow",
      "Delta Live Tables",
      "Spark SQL"
    ],
    "answer": 1,
    "category": "Machine Learning",
    "explanation": "MLflow is a tool in Databricks that allows users to track, manage, and deploy machine learning models.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main advantage of using 'Photon' in Databricks?",
    "options": [
      "Improves query performance",
      "Enhances data security",
      "Simplifies machine learning model training",
      "Automates cluster scaling"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Photon is a high-performance query execution engine in Databricks, designed to improve SQL query performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which tool in Databricks is used to track and manage machine learning models?",
    "options": [
      "Delta Lake",
      "SQL Analytics",
      "MLflow",
      "Structured Streaming"
    ],
    "answer": 2,
    "category": "Machine Learning",
    "explanation": "MLflow is a platform integrated with Databricks to track, manage, and deploy machine learning models.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Delta Lake feature supports querying of older versions of data?",
    "options": [
      "Z-Ordering",
      "Data Skew",
      "Schema Evolution",
      "Time Travel"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "'Time Travel' allows users to query earlier versions of data for auditing or rollback purposes.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does the 'DROP TABLE' command do in SQL?",
    "options": [
      "Deletes all rows in a table",
      "Removes a table from the database",
      "Deletes duplicate records",
      "Creates a new empty table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'DROP TABLE' command deletes a table and all its data from the database.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of 'MLflow'?",
    "options": [
      "To manage and track machine learning models",
      "To schedule and monitor cluster performance",
      "To visualize data in real-time",
      "To store and manage data pipelines"
    ],
    "answer": 0,
    "category": "Machine Learning",
    "explanation": "'MLflow' is used to manage, track, and deploy machine learning models in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'Data Skew' in distributed data processing?",
    "options": [
      "When data is evenly distributed across partitions",
      "When a large portion of the data is concentrated in a few keys, causing performance issues",
      "When data is duplicated across nodes",
      "When data is encrypted"
    ],
    "answer": 1,
    "category": "Data Engineering",
    "explanation": "'Data Skew' occurs when some partitions process significantly more data than others, leading to an imbalance in processing.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command is used to remove a table from a SQL database?",
    "options": [
      "DELETE",
      "DROP",
      "TRUNCATE",
      "REMOVE"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'DROP' command is used to remove a table from a SQL database, including its structure and data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'CREATE VIEW' statement in SQL?",
    "options": [
      "To create a new table from an existing table",
      "To create a virtual table based on the result of a query",
      "To insert data into an existing table",
      "To delete a table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'CREATE VIEW' statement creates a virtual table based on the result of a query, allowing easier querying and data access.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Qual é a finalidade do comando 'DROP DATABASE' no SQL?",
    "options": [
      "Remover um banco de data e todos os seus objetos",
      "Atualizar registros em um banco de data",
      "Criar um novo banco de data",
      "Listar todos os bancos de data"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'DROP DATABASE' exclui completamente um banco de data e todos os seus objetos associados.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Job Scheduler' used for?",
    "options": [
      "To create real-time pipelines",
      "To manage permissions",
      "To schedule and manage job executions",
      "To configure cluster settings"
    ],
    "answer": 2,
    "category": "Job Management",
    "explanation": "The 'Job Scheduler' in Databricks allows users to schedule jobs, including notebooks, to run at specified intervals.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which feature in Delta Lake allows users to query previous versions of data?",
    "options": [
      "Z-Ordering",
      "Time Travel",
      "Data Skew",
      "Cluster Mode"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake's 'Time Travel' feature allows users to access previous versions of data for auditing or rollback.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the primary purpose of the 'GRANT' command in SQL?",
    "options": [
      "To update records",
      "To create a new table",
      "To provide permissions to a user",
      "To delete a database"
    ],
    "answer": 2,
    "category": "Data Governance",
    "explanation": "The 'GRANT' command is used to provide specific permissions to a user or role in SQL.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "What is a 'DataFrame' in Apache Spark?",
    "options": [
      "A distributed collection of data organized into named columns",
      "A type of SQL table",
      "A method for partitioning data",
      "A method for caching data in memory"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "A 'DataFrame' is a distributed collection of data organized into named columns, similar to a table in SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which SQL function returns the maximum value of a column?",
    "options": [
      "SUM()",
      "MAX()",
      "COUNT()",
      "AVG()"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "'MAX()' is used to return the largest value in a specific column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of 'Cluster Auto-Scaling' in Databricks?",
    "options": [
      "To allow manual scaling of clusters",
      "To automatically adjust cluster size based on workload demand",
      "To reduce cluster costs by limiting active nodes",
      "To partition data for better performance"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "Cluster Auto-Scaling automatically adjusts the size of a cluster based on the workload, optimizing resource use.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of 'Cluster Termination' in Databricks?",
    "options": [
      "To prevent overuse of compute resources",
      "To pause cluster operations during peak hours",
      "To terminate the job execution before completion",
      "To provide version control on clusters"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "Cluster Termination stops clusters after a period of inactivity to prevent the overuse of compute resources and reduce costs.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command in SQL is used to remove all rows from a table while keeping the structure?",
    "options": [
      "DELETE",
      "TRUNCATE",
      "DROP",
      "REMOVE"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The TRUNCATE command deletes all rows from a table but retains its structure for future data insertion.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which function is used in SQL to calculate the total sum of a numeric column?",
    "options": [
      "COUNT()",
      "SUM()",
      "AVG()",
      "MIN()"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'SUM()' function is used to calculate the total sum of values in a numeric column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the function of the 'Spark UI' in Databricks?",
    "options": [
      "To visualize datasets",
      "To manage job scheduling",
      "To monitor job execution and performance",
      "To update cluster settings"
    ],
    "answer": 2,
    "category": "Monitoring",
    "explanation": "The 'Spark UI' allows users to monitor job execution, track performance, and troubleshoot issues in Spark jobs.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In Databricks, what is the main advantage of 'Unity Catalog'?",
    "options": [
      "It improves query performance",
      "It provides centralized management of data governance",
      "It enables real-time data processing",
      "It reduces costs for cluster usage"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "'Unity Catalog' allows for centralized management of metadata, permissions, and data governance in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "What does the 'MERGE INTO' command do in Delta Lake?",
    "options": [
      "Updates and inserts data into a table based on conditions",
      "Combines multiple tables into one",
      "Deletes data from multiple tables",
      "Removes duplicates from a table"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "The 'MERGE INTO' command allows users to update and insert data into a Delta Lake table based on specified conditions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which SQL command is used to remove a column from a table?",
    "options": [
      "ALTER TABLE... DROP COLUMN",
      "DELETE COLUMN",
      "REMOVE COLUMN",
      "ALTER COLUMN... DELETE"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'ALTER TABLE... DROP COLUMN' command is used to remove a specific column from a table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is a benefit of using Delta Live Tables in Databricks?",
    "options": [
      "Simplifies real-time analytics pipelines",
      "Provides machine learning libraries",
      "Enhances job scheduling",
      "Supports external database integration"
    ],
    "answer": 0,
    "category": "Real-Time Processing",
    "explanation": "Delta Live Tables simplifies the development of real-time data pipelines by automating operations such as data ingestion and processing.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "What does the 'ALTER TABLE... ADD COLUMN' command do in SQL?",
    "options": [
      "Adds a new column to an existing table",
      "Modifies data in a column",
      "Renames a column",
      "Deletes a column from a table"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'ALTER TABLE... ADD COLUMN' command is used to add a new column to an existing table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command in SQL is used to limit the number of rows returned?",
    "options": [
      "HAVING",
      "LIMIT",
      "ORDER BY",
      "GROUP BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'LIMIT' clause is used to restrict the number of rows returned in a SQL query result set.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'GROUP BY' clause do?",
    "options": [
      "Groups rows that have the same values into summary rows",
      "Sorts the results of a query",
      "Filters the results of a query",
      "Limits the number of rows returned by a query"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'GROUP BY' clause groups rows with the same values into summary rows, typically used with aggregate functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Delta Lake feature ensures that only data matching the table's schema can be written?",
    "options": [
      "Data Versioning",
      "Schema Enforcement",
      "Schema Evolution",
      "Time Travel"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Schema Enforcement' ensures that only data conforming to the table's schema is allowed.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the role of the 'Spark UI' in Databricks?",
    "options": [
      "To manage cluster configurations",
      "To visualize datasets",
      "To monitor job execution and performance",
      "To track machine learning models"
    ],
    "answer": 2,
    "category": "Monitoring",
    "explanation": "The 'Spark UI' provides detailed information on job execution, including task times and stages, allowing performance monitoring.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "What does 'Lazy Evaluation' in Spark mean?",
    "options": [
      "Transformations are executed immediately",
      "Actions are delayed until triggered by a transformation",
      "Transformations are delayed until an action is called",
      "Data is split into multiple partitions"
    ],
    "answer": 2,
    "category": "Apache Spark",
    "explanation": "'Lazy Evaluation' means that transformations in Spark are not executed until an action is called, optimizing performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which feature in Databricks simplifies the creation of real-time data pipelines?",
    "options": [
      "Photon Engine",
      "Delta Live Tables",
      "Job Scheduler",
      "Cluster Manager"
    ],
    "answer": 1,
    "category": "Real-Time Processing",
    "explanation": "Delta Live Tables simplifies real-time analytics pipelines by automating data ingestion, transformation, and validation.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "In Databricks, what is 'Schema Evolution' used for in Delta Lake?",
    "options": [
      "To update the schema automatically as new data is ingested",
      "To enforce strict schema validation on tables",
      "To increase the query performance",
      "To manage machine learning models"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "'Schema Evolution' in Delta Lake allows the schema of a table to automatically update as new data is ingested, supporting changes in data structure.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which clause is used in SQL to filter rows after an aggregation has been performed?",
    "options": [
      "WHERE",
      "GROUP BY",
      "HAVING",
      "ORDER BY"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "The 'HAVING' clause filters the result set after an aggregation is performed, unlike 'WHERE' which filters before aggregation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a high-level API in Spark?",
    "options": [
      "Cluster Manager",
      "RDD",
      "DataFrame",
      "Task Scheduler"
    ],
    "answer": 2,
    "category": "Apache Spark",
    "explanation": "The DataFrame API in Spark provides a high-level abstraction for working with structured data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'OPTIMIZE' command do in Delta Lake?",
    "options": [
      "Increases query performance by organizing data files",
      "Deletes old versions of data",
      "Adds new data to a table",
      "Reverts to previous table versions"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "The 'OPTIMIZE' command improves query performance by compacting small data files into larger, more efficient ones.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following statements is true about Delta Lake's 'time travel' feature?",
    "options": [
      "It allows you to view previous versions of data",
      "It speeds up job execution",
      "It optimizes queries",
      "It is only available in Databricks SQL"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Time travel in Delta Lake allows users to view and query previous versions of the data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of 'Delta Live Tables' in Databricks?",
    "options": [
      "Automating the ingestion and processing of data in real-time",
      "Providing a faster SQL execution engine",
      "Creating machine learning models",
      "Encrypting data during queries"
    ],
    "answer": 0,
    "category": "Real-Time Processing",
    "explanation": "'Delta Live Tables' automates the process of ingesting and processing real-time data streams in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "What is the role of 'SparkContext' in Spark?",
    "options": [
      "It manages connections and configuration to the Spark cluster",
      "It creates data visualizations",
      "It executes SQL queries",
      "It stores data in tables"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'SparkContext' is the entry point for connecting to a Spark cluster and managing configurations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a key feature of Delta Lake?",
    "options": [
      "Versioning",
      "Batch processing only",
      "Machine learning support",
      "No schema enforcement"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Delta Lake's versioning feature allows you to track and manage multiple versions of data over time.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does the 'Photon Engine' in Databricks provide?",
    "options": [
      "Improved security for SQL queries",
      "A high-performance execution engine for SQL workloads",
      "Real-time stream processing",
      "Visualization capabilities for large datasets"
    ],
    "answer": 1,
    "category": "Optimization",
    "explanation": "The 'Photon Engine' provides high-performance execution for SQL queries, improving the speed of analytical workloads.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'EXPLAIN' command in SQL?",
    "options": [
      "To execute a query",
      "To provide the execution plan of a query",
      "To update records in a table",
      "To delete a table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'EXPLAIN' command in SQL is used to display the execution plan for a query.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which command is used to rename an existing table?",
    "options": [
      "ALTER TABLE... RENAME TO",
      "RENAME TABLE",
      "UPDATE TABLE",
      "CHANGE TABLE NAME"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'ALTER TABLE... RENAME TO' command is used to rename an existing table in SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of 'Data Lineage' in Data Governance?",
    "options": [
      "Tracking the history of data from source to destination",
      "Encrypting data at rest",
      "Optimizing data partitioning",
      "Managing SQL queries"
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "'Data Lineage' tracks the movement and transformation of data from its origin to its final destination, ensuring transparency and traceability.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html"
    ]
  },
  {
    "question": "In Databricks, what is 'Command Mode' used for in notebooks?",
    "options": [
      "To execute SQL queries",
      "To navigate and edit cells in notebooks",
      "To switch between cluster environments",
      "To monitor cluster resources"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "'Command Mode' allows users to navigate and edit cells within Databricks notebooks efficiently.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of 'Notebook Widgets'?",
    "options": [
      "To create interactive elements in notebooks",
      "To schedule the execution of notebooks",
      "To run SQL queries in real-time",
      "To manage user permissions within notebooks"
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "'Notebook Widgets' allow the creation of interactive elements such as dropdowns and text inputs within Databricks notebooks.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "What is the function of the 'HAVING' clause in SQL?",
    "options": [
      "To filter rows before aggregation",
      "To limit the number of rows returned",
      "To filter rows after aggregation",
      "To group rows by common values"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "The 'HAVING' clause is used to filter rows after an aggregation is performed.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'Job Scheduler' in Databricks?",
    "options": [
      "To manage and schedule the execution of jobs",
      "To monitor cluster performance",
      "To store data from streaming sources",
      "To create notebooks for data visualization"
    ],
    "answer": 0,
    "category": "Job Management",
    "explanation": "'Job Scheduler' manages the scheduling and execution of jobs, allowing tasks to run at specific times.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In SQL, what is the function of the 'HAVING' clause?",
    "options": [
      "To filter rows before aggregation",
      "To limit the number of rows returned",
      "To filter rows after aggregation",
      "To order the results"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "The 'HAVING' clause is used to filter records after aggregation has been performed.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of 'Time Travel' in Delta Lake?",
    "options": [
      "To partition data for faster querying",
      "To retrieve historical versions of data",
      "To enforce schema validation",
      "To encrypt data at rest"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Time Travel' allows users to access previous versions of data for purposes such as auditing or recovery.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What type of consistency does Delta Lake offer?",
    "options": [
      "Eventual consistency",
      "Strong consistency",
      "No consistency",
      "Weak consistency"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake provides strong consistency, meaning that once a transaction is committed, all subsequent queries will see the result.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following statements is true about Delta Lake's schema enforcement?",
    "options": [
      "It prevents invalid data from being written",
      "It allows any data format",
      "It enforces schema only at the database level",
      "It cannot be modified after table creation"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Schema enforcement in Delta Lake ensures that only data that matches the table's schema can be written to the table.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which SQL clause is used to filter data after aggregation?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "HAVING",
      "LIMIT"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "'HAVING' is used to filter data after an aggregation has been performed, such as with GROUP BY.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'OPTIMIZE' command in Delta Lake do?",
    "options": [
      "Compacts small files into larger ones for query efficiency",
      "Deletes old versions of data",
      "Adds partitions to improve performance",
      "Reverts to a previous table version"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "The 'OPTIMIZE' command compacts small data files into larger, more efficient ones to improve query performance.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which function in SQL is used to group rows that have the same values in specified columns?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "HAVING",
      "JOIN"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The GROUP BY clause groups rows that share the same values in specified columns, allowing for aggregate functions.",
    "sources": [
      "https://docs.databricks.com/en/sharing/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'Cluster Manager'?",
    "options": [
      "Manages resource allocation for clusters",
      "Creates visualizations of datasets",
      "Schedules jobs in Databricks",
      "Writes SQL queries"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "The 'Cluster Manager' in Databricks is responsible for managing the allocation of computational resources to clusters.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command is used to create a Delta table in Databricks?",
    "options": [
      "CREATE DELTA TABLE",
      "CREATE TABLE USING DELTA",
      "CREATE EXTERNAL TABLE",
      "CREATE TABLE"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "The 'CREATE TABLE USING DELTA' command is used to create a Delta Lake table in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does 'Checkpointing' do in Spark Streaming?",
    "options": [
      "It optimizes data partitioning",
      "It saves the state of streaming data to ensure fault tolerance",
      "It reduces the size of data files",
      "It replicates data across nodes"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Checkpointing' in Spark Streaming saves the state of streaming jobs to disk, enabling recovery in case of failure.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which programming language is not officially supported in Databricks notebooks?",
    "options": [
      "Python",
      "R",
      "Java",
      "Scala"
    ],
    "answer": 2,
    "category": "Databricks Notebooks",
    "explanation": "Databricks notebooks officially support Python, R, Scala, and SQL, but not Java.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main benefit of 'Photon' in Databricks?",
    "options": [
      "Improves SQL query performance",
      "Increases security for SQL queries",
      "Enhances data partitioning features",
      "Enables real-time collaboration"
    ],
    "answer": 0,
    "category": "Optimization",
    "explanation": "'Photon' is a native execution engine that improves SQL query performance in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is a Delta Lake transaction log used for?",
    "options": [
      "Storing metadata and tracking changes",
      "Optimizing query performance",
      "Storing machine learning models",
      "Running SQL queries"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "The transaction log records every change made to a Delta Lake table, enabling features like time travel and ACID transactions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does 'Schema Evolution' in Delta Lake allow?",
    "options": [
      "Automatic schema updates as new data is ingested",
      "Data encryption during query execution",
      "Partitioning of data by columns",
      "Time travel queries"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "'Schema Evolution' allows Delta Lake tables to automatically update their schema as new data with different structures is added.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the function of the 'USE DATABASE' command in SQL?",
    "options": [
      "Creates a new database",
      "Switches to a specified database for subsequent queries",
      "Deletes a database",
      "Creates a backup of a database"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'USE DATABASE' command switches the context to a specified database, so all subsequent queries run in that database.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Cluster Auto-Termination' in Databricks do?",
    "options": [
      "Automatically resizes the cluster based on workload",
      "Terminates clusters after a period of inactivity",
      "Restarts clusters after failure",
      "Upgrades cluster software automatically"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "'Cluster Auto-Termination' shuts down clusters that remain idle for a specified period, saving resources.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature automatically adjusts the size of a cluster based on workload?",
    "options": [
      "Time Travel",
      "Auto Loader",
      "Cluster Auto-scaling",
      "Photon Engine"
    ],
    "answer": 2,
    "category": "Cluster Management",
    "explanation": "Cluster Auto-scaling automatically increases or decreases the number of nodes in a cluster based on workload.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Job Clustering' do in Databricks?",
    "options": [
      "It creates clusters specifically for the execution of scheduled jobs",
      "It optimizes SQL queries for faster execution",
      "It manages permissions for job execution",
      "It stores metadata for jobs"
    ],
    "answer": 0,
    "category": "Job Management",
    "explanation": "'Job Clustering' creates clusters that are automatically terminated after the job is executed, optimizing resource use.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which Databricks feature allows you to track the version history of Delta Lake tables?",
    "options": [
      "Schema Evolution",
      "Time Travel",
      "Delta Metrics",
      "Photon"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Time Travel allows you to view and query previous versions of Delta Lake tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Qual é a finalidade do comando 'CREATE DATABASE' no SQL?",
    "options": [
      "Atualizar registros em uma table",
      "Criar um novo banco de data",
      "Excluir um banco de data existente",
      "Adicionar uma nova column a uma table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'CREATE DATABASE' é usado para criar um novo banco de data no sistema.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Auto Loader' do in Databricks?",
    "options": [
      "Automates the ingestion of real-time or batch data into Delta Lake",
      "Schedules and monitors cluster performance",
      "Deletes outdated files from Delta Lake",
      "Partitions data automatically for better performance"
    ],
    "answer": 0,
    "category": "Ingestão de data",
    "explanation": "'Auto Loader' automates the continuous ingestion of both batch and real-time data into Delta Lake for processing.",
    "sources": [
      "https://docs.databricks.com/en/ingestion/auto-loader/index.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is 'Checkpointing' used for in Spark Streaming?",
    "options": [
      "To OPTIMIZE partitioning",
      "To save the state of a streaming job for fault tolerance",
      "To encrypt data during streaming",
      "To increase data partition size"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Checkpointing' saves the state of a streaming job in Spark to enable recovery in case of a failure.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which of the following is a feature of Databricks notebooks?",
    "options": [
      "Support for multiple languages",
      "Version control integration",
      "Real-time collaboration",
      "All of the above"
    ],
    "answer": 3,
    "category": "Databricks Notebooks",
    "explanation": "Databricks notebooks support multiple languages, real-time collaboration, and integration with version control systems.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'Notebook Widget' used for?",
    "options": [
      "To add interactivity to notebooks",
      "To track machine learning experiments",
      "To manage cluster resources",
      "To visualize data"
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "Notebook Widgets add interactivity, such as dropdowns or text boxes, to notebooks for dynamic inputs.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "What is the default file format used by Delta Lake?",
    "options": [
      "Parquet",
      "CSV",
      "JSON",
      "ORC"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Delta Lake stores data in Parquet format by default.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what is 'Job Execution Context' used for?",
    "options": [
      "To manage data ingestion in real-time",
      "To provide metadata and context for job execution",
      "To partition data for performance",
      "To cache data during job execution"
    ],
    "answer": 1,
    "category": "Job Management",
    "explanation": "'Job Execution Context' provides metadata, runtime variables, and information about job execution within Databricks.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which SQL command is used to change data in an existing table?",
    "options": [
      "ALTER TABLE",
      "UPDATE",
      "INSERT INTO",
      "DROP"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'UPDATE' command is used to modify existing records in a table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Persist' do in Apache Spark?",
    "options": [
      "Partitions data across nodes",
      "Stores data in memory or disk for reuse",
      "Shuffles data between nodes",
      "Cleans up unused data"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "'Persist' allows Spark to store data in memory or on disk, so it can be reused in later stages of a job without recomputation.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which command in SQL is used to rename a table?",
    "options": [
      "ALTER TABLE... RENAME TO",
      "RENAME TABLE",
      "UPDATE TABLE",
      "CHANGE TABLE NAME"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'ALTER TABLE... RENAME TO' command is used to rename a table in SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which function in SQL is used to calculate the average of values in a column?",
    "options": [
      "SUM()",
      "AVG()",
      "COUNT()",
      "MIN()"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'AVG()' function returns the average of the values in a numeric column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main benefit of Z-Ordering in Delta Lake?",
    "options": [
      "It compresses data for faster queries",
      "It organizes data to improve query performance",
      "It provides data encryption",
      "It enables time travel"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Z-Ordering optimizes the physical layout of data to improve the performance of queries that filter on specific columns.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does the 'EXPLAIN' command in SQL do?",
    "options": [
      "Executes the query without returning results",
      "Provides a detailed query execution plan",
      "Limits the number of rows returned",
      "Creates a temporary table from a query"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'EXPLAIN' command provides insight into how a SQL query will be executed, showing the query plan.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main purpose of 'Cluster Termination' in Databricks?",
    "options": [
      "To pause job execution",
      "To delete temporary data files",
      "To stop clusters after inactivity to save costs",
      "To partition data for better querying"
    ],
    "answer": 2,
    "category": "Cluster Management",
    "explanation": "'Cluster Termination' stops clusters after a specified period of inactivity, helping to save computational resources and reduce costs.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks tool is used for managing machine learning lifecycle?",
    "options": [
      "SQL Analytics",
      "Delta Lake",
      "MLflow",
      "Structured Streaming"
    ],
    "answer": 2,
    "category": "Machine Learning",
    "explanation": "MLflow is a tool integrated with Databricks to manage the end-to-end machine learning lifecycle.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Delta Lake, what does ACID stand for?",
    "options": [
      "Atomicity, Consistency, Isolation, Durability",
      "Automation, Consistency, Isolation, Durability",
      "Atomicity, Control, Integrity, Durability",
      "Automation, Control, Isolation, Durability"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "ACID stands for Atomicity, Consistency, Isolation, and Durability, which are critical properties of reliable transactions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of the 'Job UI'?",
    "options": [
      "To execute SQL queries",
      "To manage machine learning models",
      "To monitor and debug jobs in real-time",
      "To schedule jobs for future execution"
    ],
    "answer": 2,
    "category": "Monitoring",
    "explanation": "The 'Job UI' allows users to monitor and debug jobs in real-time, providing detailed insights into job performance.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "What is the purpose of the 'GRANT' command in SQL?",
    "options": [
      "To update data in a table",
      "To remove a database",
      "To give permissions to a user",
      "To create a new database"
    ],
    "answer": 2,
    "category": "Data Governance",
    "explanation": "The 'GRANT' command is used to provide specific permissions to users on database objects.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "Which SQL function is used to calculate the average value of a column?",
    "options": [
      "SUM()",
      "MAX()",
      "AVG()",
      "COUNT()"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "'AVG()' is used to calculate the average value from a specified column in SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'Cluster Auto-Scaling' in Databricks?",
    "options": [
      "Automatically adjusts cluster size based on workload",
      "Manually scales the cluster",
      "Optimizes SQL queries for performance",
      "Provides encryption for data processing"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "'Cluster Auto-Scaling' in Databricks automatically adjusts the cluster's size based on the current workload, optimizing resource usage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature allows users to track the lifecycle of machine learning models?",
    "options": [
      "Photon",
      "MLflow",
      "Delta Live Tables",
      "Databricks File System"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "'MLflow' allows users to track and manage the lifecycle of machine learning models, from experimentation to deployment.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature enables collaboration on notebooks in real-time?",
    "options": [
      "MLflow",
      "Delta Lake",
      "Real-Time Collaboration",
      "Git Integration"
    ],
    "answer": 2,
    "category": "Databricks Notebooks",
    "explanation": "Databricks supports real-time collaboration on notebooks, allowing multiple users to work on the same notebook simultaneously.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'Managed Table'?",
    "options": [
      "A table stored outside Databricks",
      "A table where both data and metadata are managed by Databricks",
      "A temporary table used for SQL queries",
      "A view created from a query"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "A 'Managed Table' in Databricks is fully managed by Databricks, including both the data and its metadata.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "What does 'Schema Evolution' in Delta Lake enable?",
    "options": [
      "Automatic schema changes to accommodate new data",
      "Data encryption for queries",
      "Real-time streaming capabilities",
      "Partitioning data across nodes"
    ],
    "answer": 0,
    "category": "Data Engineering",
    "explanation": "'Schema Evolution' allows Delta Lake to automatically update the schema when new data with different structures is ingested.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which Databricks feature allows real-time collaboration on notebooks?",
    "options": [
      "Photon Engine",
      "Real-Time Collaboration",
      "Delta Live Tables",
      "Job Scheduler"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Databricks supports real-time collaboration, allowing multiple users to edit and run code in notebooks simultaneously.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of Databricks 'Auto Loader'?",
    "options": [
      "To manually load data into Databricks",
      "To automate the ingestion of streaming data",
      "To create data visualizations",
      "To manage SQL queries"
    ],
    "answer": 1,
    "category": "Ingestão de data",
    "explanation": "The 'Auto Loader' feature automates the ingestion of streaming data into Databricks tables, optimizing for scalability.",
    "sources": [
      "https://docs.databricks.com/en/ingestion/auto-loader/index.html"
    ]
  },
  {
    "question": "Which Databricks feature helps to OPTIMIZE queries for faster performance?",
    "options": [
      "Databricks Photon",
      "MLflow",
      "Delta Live Tables",
      "SQL Analytics"
    ],
    "answer": 0,
    "category": "Optimization",
    "explanation": "Databricks Photon is a vectorized engine that optimizes query execution for faster performance.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "No SQL, para que serve a cláusula 'UNION'?",
    "options": [
      "Combinar os resultados de duas ou mais querys sem duplicatas",
      "Encontrar interseção entre duas querys",
      "Atualizar múltiplas tables ao mesmo tempo",
      "Excluir registros duplicados em uma table"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A cláusula 'UNION' combina os resultados de duas ou mais querys, removendo duplicatas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'Lazy Evaluation' in Spark?",
    "options": [
      "Executing transformations immediately",
      "Delaying execution of transformations until an action is called",
      "Automatically partitioning data",
      "Caching data in memory"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "'Lazy Evaluation' delays the execution of transformations until an action like collect or save is triggered, optimizing performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Z-Ordering' in Delta Lake OPTIMIZE?",
    "options": [
      "Data encryption",
      "Query performance for specific columns",
      "Machine learning model training",
      "Data insertion speed"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Z-Ordering' improves query performance by physically reorganizing data for more efficient access to specific columns.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Qual comando SQL é usado para adicionar uma nova column a uma table existente?",
    "options": [
      "ALTER TABLE... ADD COLUMN",
      "UPDATE TABLE... ADD COLUMN",
      "INSERT INTO... COLUMN",
      "CREATE COLUMN IN TABLE"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'ALTER TABLE... ADD COLUMN' é usado para adicionar uma nova column a uma table existente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what does the 'Notebook Revision History' feature do?",
    "options": [
      "Creates backups of data in Delta Lake",
      "Tracks changes and version history of notebooks",
      "Schedules the execution of notebooks",
      "Partitions data for performance optimization"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "'Notebook Revision History' tracks changes and versions of notebooks, allowing users to review and revert to earlier versions.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "What is the role of 'Checkpointing' in Apache Spark?",
    "options": [
      "To partition data for better performance",
      "To store intermediate results on disk for fault tolerance",
      "To cache data in memory for quick access",
      "To split large datasets into smaller batches"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Checkpointing' stores intermediate results on disk, providing fault tolerance in streaming applications.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of Delta Lake's 'Time Travel' feature?",
    "options": [
      "Data encryption",
      "Query optimization",
      "Querying older versions of data",
      "Partitioning data"
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "'Time Travel' in Delta Lake allows users to access previous versions of data for auditing or rollback.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the function of the 'MERGE INTO' command in Delta Lake?",
    "options": [
      "To combine two datasets into one",
      "To delete data from a table",
      "To update and insert data based on conditions",
      "To alter the schema of a table"
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "MERGE INTO is used to update and insert data into a Delta Lake table based on specified conditions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Qual comando SQL é usado para remover uma table do banco de data?",
    "options": [
      "DELETE TABLE",
      "DROP TABLE",
      "REMOVE TABLE",
      "CLEAR TABLE"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'DROP TABLE' é utilizado para remover completamente uma table e seus data do banco de data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does 'Time Travel' in Delta Lake enable?",
    "options": [
      "Faster query execution",
      "Access to previous versions of data",
      "Automatic schema enforcement",
      "Machine learning integration"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Time Travel' in Delta Lake allows users to query and access historical versions of data for auditing or rollback purposes.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does the 'COUNT()' function in SQL return?",
    "options": [
      "The total number of rows",
      "The sum of a column",
      "The average of values",
      "The maximum value in a column"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'COUNT()' function returns the total number of rows that match a specified condition.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of Delta Lake's time travel feature?",
    "options": [
      "Viewing older versions of data",
      "Faster query execution",
      "Schema enforcement",
      "Support for machine learning"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Time travel allows users to access previous versions of data for auditing or recovery.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following features allows Databricks to automatically adjust the number of nodes in a cluster?",
    "options": [
      "Photon",
      "Auto-Loader",
      "Auto-Scaling",
      "Unity Catalog"
    ],
    "answer": 2,
    "category": "Cluster Management",
    "explanation": "'Auto-Scaling' dynamically adjusts the number of nodes in a cluster based on the workload, optimizing resource usage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Apache Spark, what is the function of 'Broadcast Join'?",
    "options": [
      "A join operation that replicates a small table to all nodes",
      "A join operation that distributes data evenly across nodes",
      "A join that only works with streaming data",
      "A join that filters out duplicate records"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'Broadcast Join' in Spark replicates a small dataset to all nodes to OPTIMIZE joins with large datasets.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "What is the main role of 'Delta Live Tables' in Databricks?",
    "options": [
      "To create and manage batch data pipelines",
      "To create and manage real-time data pipelines",
      "To partition data for optimization",
      "To manage SQL queries across clusters"
    ],
    "answer": 1,
    "category": "Real-Time Processing",
    "explanation": "'Delta Live Tables' helps in creating and managing real-time data pipelines for continuous data ingestion and processing.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "What is the role of 'Broadcast Join' in Apache Spark?",
    "options": [
      "It distributes large datasets across nodes",
      "It replicates a small table to all nodes for faster joins with larger datasets",
      "It caches data in memory",
      "It optimizes SQL queries"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "'Broadcast Join' replicates a small table to all nodes, optimizing join operations with larger datasets.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'SUM()' function do in SQL?",
    "options": [
      "It counts the number of rows in a table",
      "It adds together the values in a column",
      "It calculates the average value of a column",
      "It returns the highest value in a column"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'SUM()' function calculates the sum of values in a numeric column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Delta Lake, what does 'Schema Enforcement' ensure?",
    "options": [
      "Data encryption is applied",
      "Data meets the schema requirements before being written",
      "Queries are optimized automatically",
      "Data is partitioned across multiple nodes"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Schema Enforcement' ensures that only data conforming to the defined schema is written to a Delta Lake table.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the benefit of using 'Photon' in Databricks?",
    "options": [
      "Improved data encryption",
      "Faster SQL query performance",
      "Increased machine learning model accuracy",
      "Real-time streaming capabilities"
    ],
    "answer": 1,
    "category": "Optimization",
    "explanation": "'Photon' is a high-performance execution engine that speeds up SQL query performance in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of Databricks Auto Scaling?",
    "options": [
      "Adjusting cluster size based on workload",
      "Preventing all downtime",
      "Only scaling up clusters",
      "Requires manual adjustments"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "Databricks Auto Scaling automatically adjusts the size of a cluster based on the workload to OPTIMIZE resource usage.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "What is the primary benefit of Delta Lake's ACID transactions?",
    "options": [
      "Improved machine learning performance",
      "Guaranteed consistency and durability of data",
      "Real-time data visualization",
      "Faster query execution"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake supports ACID transactions, which guarantee that data changes are consistent, durable, and reliable.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what does 'Auto Loader' automate?",
    "options": [
      "The process of monitoring cluster performance",
      "The ingestion of real-time data streams into Delta Lake",
      "The deployment of machine learning models",
      "The creation of notebooks"
    ],
    "answer": 1,
    "category": "Ingestão de data",
    "explanation": "'Auto Loader' simplifies the ingestion of real-time or batch data streams into Delta Lake for continuous processing.",
    "sources": [
      "https://docs.databricks.com/en/ingestion/auto-loader/index.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What does the 'EXPLAIN' command in SQL do?",
    "options": [
      "It provides the execution plan for a query",
      "It updates records in a table",
      "It creates a new table",
      "It deletes all rows in a table"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'EXPLAIN' command shows the execution plan that the SQL engine will use to run a query.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature allows real-time collaboration on notebooks?",
    "options": [
      "Photon Engine",
      "Delta Live Tables",
      "Real-Time Collaboration",
      "Job Scheduler"
    ],
    "answer": 2,
    "category": "Databricks Notebooks",
    "explanation": "Databricks allows real-time collaboration on notebooks, enabling multiple users to edit and run code simultaneously.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Databricks feature helps to manage metadata and permissions across the platform?",
    "options": [
      "Databricks Runtime",
      "Unity Catalog",
      "Databricks SQL",
      "Structured Streaming"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "'Unity Catalog' is the governance feature that manages metadata, permissions, and lineage in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html",
      "https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html"
    ]
  },
  {
    "question": "What is the role of 'Streaming Dataframes' in Databricks?",
    "options": [
      "To store batch data in memory",
      "To manage the lifecycle of real-time data streams",
      "To visualize real-time data",
      "To merge batch and streaming data for analysis"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Streaming Dataframes' manage the flow of real-time data, allowing continuous analysis and manipulation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of 'Streaming Query Listener' in Structured Streaming?",
    "options": [
      "To monitor real-time streaming data queries",
      "To cache streaming data for later use",
      "To partition streaming data into smaller chunks",
      "To merge multiple streams into one"
    ],
    "answer": 0,
    "category": "Streaming",
    "explanation": "'Streaming Query Listener' in Structured Streaming helps monitor the status of streaming queries, tracking progress and errors.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of 'Watermarking' in Structured Streaming?",
    "options": [
      "To discard late-arriving data",
      "To encrypt sensitive data in streams",
      "To partition streaming data for better performance",
      "To control access to streaming data"
    ],
    "answer": 0,
    "category": "Streaming",
    "explanation": "'Watermarking' is used in Structured Streaming to discard data that arrives after a specified time limit, reducing the state needed for aggregations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which tool in Databricks helps to monitor the performance and execution of jobs?",
    "options": [
      "Job UI",
      "Structured Streaming",
      "Spark UI",
      "MLflow"
    ],
    "answer": 2,
    "category": "Monitoring",
    "explanation": "The Spark UI allows you to monitor the execution of Spark jobs and diagnose performance issues.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "What is a 'Z-ORDER' optimization in Delta Lake?",
    "options": [
      "A way to reduce storage cost",
      "A technique to organize data for efficient querying",
      "A method for encrypting data",
      "A feature to enable data shuffling"
    ],
    "answer": 1,
    "category": "Data Engineering",
    "explanation": "'Z-ORDER' is a technique used to OPTIMIZE queries by clustering data based on specific columns to reduce scan time.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which SQL command is used to add a new column to an existing table?",
    "options": [
      "ALTER TABLE... ADD COLUMN",
      "UPDATE TABLE... ADD COLUMN",
      "CREATE COLUMN",
      "ADD COLUMN TO TABLE"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'ALTER TABLE... ADD COLUMN' command adds a new column to an existing table in SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which SQL function is used to find the maximum value in a column?",
    "options": [
      "MAX()",
      "MIN()",
      "SUM()",
      "AVG()"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The 'MAX()' function returns the largest value from a specified column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which feature of Databricks clusters helps to scale resources automatically based on workload?",
    "options": [
      "Delta Lake",
      "Photon Engine",
      "Auto Scaling",
      "Job Scheduler"
    ],
    "answer": 2,
    "category": "Cluster Management",
    "explanation": "Auto Scaling adjusts the resources allocated to a Databricks cluster based on the workload, improving efficiency.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of 'Unity Catalog' in Databricks?",
    "options": [
      "To track and manage machine learning models",
      "To OPTIMIZE query performance",
      "To govern and manage metadata and permissions",
      "To create real-time data pipelines"
    ],
    "answer": 2,
    "category": "Data Governance",
    "explanation": "'Unity Catalog' provides a centralized platform for managing metadata, governance, and permissions across Databricks.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "What is the primary purpose of Databricks SQL?",
    "options": [
      "To provide a collaborative environment for data science",
      "To manage and organize data lakes",
      "To analyze data using SQL queries and create visualizations",
      "To train machine learning models"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "Databricks SQL is primarily used for querying data with SQL and creating visualizations.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'workspace'?",
    "options": [
      "A collaborative environment for projects",
      "A virtual machine",
      "A storage system",
      "A cluster management tool"
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "A 'workspace' in Databricks is a collaborative environment where users can share notebooks and work on projects together.",
    "sources": [
      "https://docs.databricks.com/en/sharing/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of 'Auto Termination' for clusters?",
    "options": [
      "To ensure that clusters stop after job completion or inactivity",
      "To run real-time queries continuously",
      "To improve SQL query execution speed",
      "To enable user collaboration in real-time"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "'Auto Termination' ensures that clusters stop automatically after a job is completed or after a period of inactivity, reducing costs.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In Databricks, what is the impact on a dashboard if the configured refresh rate for the dashboard is set to be more frequent than the 'Auto Stop' setting of the SQL warehouse?",
    "options": [
      "The warehouse automatically adjusts its 'Auto Stop' setting to match the dashboard's refresh rate.",
      "The dashboard will continue to refresh at the set interval, the SQL warehouse will continue running.",
      "The dashboard will not be refreshed.",
      "The dashboard will stop refreshing once the warehouse enters 'Auto Stop' mode, potentially leading to outdated data being displayed.",
      "The warehouse's 'Auto Stop' setting is irrelevant, as the dashboard's refresh rate does not interact with warehouse settings."
    ],
    "answer": 3,
    "category": "Dashboard Refresh",
    "explanation": "When the warehouse enters 'Auto Stop' mode, the dashboard cannot refresh, which could result in outdated data.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In a Databricks Lakehouse, you are working with silver-level data that needs deduplication based on the customer_id and interaction_date columns. Which approach would be the most efficient for cleaning this data in Databricks?",
    "options": [
      "Use DELETE FROM CustomerInteractions WHERE... to remove duplicate rows based on a subquery that identifies the earliest interactions.",
      "Utilize Delta Lake's time travel feature to revert CustomerInteractions to a state before duplicates were introduced.",
      "Employ SELECT DISTINCT customer_id, interaction_date,... FROM CustomerInteractions to create a distinct list of interactions.",
      "Create a new table from CustomerInteractions using GROUP BY customer_id, interaction_date and aggregate functions to capture the latest interaction.",
      "Implement a WINDOW function partitioned by customer_id and ordered by interaction_date to rank interactions and delete lower-ranked duplicates."
    ],
    "answer": 4,
    "category": "Data Cleaning",
    "explanation": "Using a WINDOW function with ranking provides an efficient way to identify and retain the latest interactions while removing duplicates.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following statements best describes the function of Databricks SQL queries in the Databricks platform?",
    "options": [
      "Databricks SQL queries are primarily used for configuring cluster settings and managing user permissions.",
      "Databricks SQL queries provide a dedicated environment to write, test, and run SQL code for data analysis and processing.",
      "Databricks SQL queries are used exclusively for scheduling jobs and automating workflows, without any SQL code execution capabilities.",
      "Databricks SQL queries are a feature for visualizing data but do not support writing or running SQL code.",
      "Databricks SQL queries are designed to write and execute machine learning models only, and not for general SQL code execution."
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Databricks SQL provides an environment specifically for writing, testing, and running SQL code for data processing and analysis, making it ideal for analysts and data scientists.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, how does the persistence of data differ between a view and a temporary view?",
    "options": [
      "Both views and temporary views do not store data physically, but a view persists beyond the session.",
      "A view stores data physically, whereas a temporary view only exists during the session.",
      "A view is session-specific, while a temporary view persists across sessions.",
      "Temporary views allow data modifications, unlike regular views.",
      "Both views and temporary views store data physically in the workspace."
    ],
    "answer": 0,
    "category": "SQL Views",
    "explanation": "Views persist beyond the session while temporary views exist only within the session without storing data physically.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In Azure Databricks, when using a table visualization type for SQL queries, which of the following statements accurately reflects its capabilities and limitations?",
    "options": [
      "Table visualizations primarily function to display graphical representations like charts and graphs, rather than tabular data.",
      "Table visualizations in Databricks are primarily used for external data export and are not suitable for in-dashboard data presentation.",
      "They allow for manual reordering, hiding, and formatting of data columns, but do not perform data aggregations within the result set.",
      "They are limited to displaying only numerical data and cannot handle textual or categorical data.",
      "Table visualizations in Databricks automatically aggregate data within the result set, providing a summary view."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Table visualizations in Databricks allow formatting and organizing data but do not perform aggregations within the visualization itself.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following statements is true regarding MERGE INTO, INSERT INTO, and COPY INTO?",
    "options": [
      "COPY INTO is used for updating existing records and inserting new records, MERGE INTO is only for inserting new records, and INSERT INTO is not used in Databricks.",
      "MERGE INTO is suitable for updating existing records and inserting new records, while INSERT INTO is used only for adding new records, and COPY INTO is used for loading data from files.",
      "INSERT INTO can be used for both updating existing records and inserting new records, while MERGE INTO is only for inserting new records, and COPY INTO is not used in Databricks.",
      "MERGE INTO and INSERT INTO perform the same functions, and COPY INTO is not a recognized command in Databricks.",
      "INSERT INTO and COPY INTO are both used for inserting new records, but COPY INTO is specifically for loading data from external sources, and MERGE INTO is for updating existing records only."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "MERGE INTO is typically used for upserts, handling both inserts and updates, while INSERT INTO is used for inserting new records, and COPY INTO is often used to load data from external sources.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html"
    ]
  },
  {
    "question": "What are the essential steps to execute a basic SQL query in Databricks?",
    "options": [
      "Manually enter data into Databricks tables, write a SQL query in a text file, and use an external tool to execute the query.",
      "Import data into a Databricks dataset, use a BI tool to run the SQL query, and export the results to a CSV file.",
      "Create a data frame in Python or Scala, apply a SQL query to the data frame, and display the results.",
      "Open SQL Editor, select a SQL warehouse, specify the query, run the query.",
      "Write a SQL query in a Databricks notebook, validate the syntax, execute the query, and view the results."
    ],
    "answer": 4,
    "category": "SQL Execution",
    "explanation": "The essential steps in Databricks for executing a SQL query involve writing it in a notebook, validating syntax, executing, and then viewing results.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "When conducting a sophisticated analysis in data analytics, which activity best illustrates the thoughtful integration of varied datasets?",
    "options": [
      "Focusing exclusively on data from the system with the largest volume of data.",
      "Deploying the latest machine learning algorithms without considering data sources.",
      "Randomly alternating between systems for data retrieval to maintain diversity.",
      "Separating datasets to preserve the uniqueness of each system's data.",
      "Synchronizing and unifying data from diverse systems to ensure consistency in analytics."
    ],
    "answer": 4,
    "category": "Data Integration",
    "explanation": "Synchronizing and unifying data ensures consistency across analytics, which is essential for integrated data analysis.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Given two tables, Customers and Orders, the resulting joined table has NULLs for customers without orders. What type of JOIN was used?",
    "options": [
      "LEFT JOIN",
      "INNER JOIN",
      "ANTI JOIN",
      "CROSS JOIN",
      "FULL JOIN"
    ],
    "answer": 0,
    "category": "SQL JOIN",
    "explanation": "A LEFT JOIN includes all records from the left table (Customers) and matches them with the right table (Orders), filling in NULLs where there are no matches.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When integrating Databricks SQL with popular visualization tools such as Tableau, Power BI, and Looker, which of the following steps is commonly involved in the connection process?",
    "options": [
      "Setting up a direct JDBC/ODBC connection between Databricks SQL and the visualization tool.",
      "Implementing a custom API for each visualization tool to query data from Databricks SQL.",
      "Using email to transfer data snapshots from Databricks SQL to the visualization tools.",
      "Requiring a third-party data integration tool to mediate the connection between Databricks SQL and the visualization tools.",
      "Manually exporting data from Databricks SQL to CSV files and importing them into the visualization tool."
    ],
    "answer": 0,
    "category": "Data Integration",
    "explanation": "A direct JDBC/ODBC connection is the standard method to connect Databricks SQL with visualization tools, enabling real-time data access for visualization.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, a data analyst is working on a dashboard and wants to ensure a consistent color scheme across all visualizations. Which approach should the analyst take?",
    "options": [
      "Export the dashboard data to a third-party tool for color adjustments, then re-import it.",
      "Use a dashboard-wide setting that allows the analyst to apply a uniform color scheme to all visualizations simultaneously.",
      "Implement a script in the dashboard code to automatically adjust the colors of all visualizations.",
      "Manually adjust the color settings in each individual visualization to match the desired scheme.",
      "Change the default color settings in the Databricks user preferences to automatically apply to all dashboards and visualizations."
    ],
    "answer": 1,
    "category": "Databricks Dashboard",
    "explanation": "A dashboard-wide setting is the most efficient way to ensure a uniform color scheme without needing to adjust each visualization manually.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the key audience and side audiences for Databricks SQL?",
    "options": [
      "The key audience is data scientists, with both data engineers and software developers as side audiences.",
      "The key audience includes both data analysts and data scientists, with no significant side audiences.",
      "The primary audience consists of data analysts, with data scientists and data engineers forming the side audiences.",
      "The key audience is exclusively data engineers, with software developers as a side audience.",
      "The main audience comprises business intelligence professionals, with data analysts and data engineers as side audiences."
    ],
    "answer": 2,
    "category": "Audience",
    "explanation": "Databricks SQL primarily targets data analysts, but data scientists and data engineers also use it as side audiences for data processing and insights.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In a Databricks dashboard designed to track regional sales data, an analyst introduces a parameter to select a specific region. This is an example of which behavior in a Databricks dashboard?",
    "options": [
      "The parameter serves as an input field for users to add new data.",
      "The parameter solely adjusts the layout without changing the data displayed.",
      "The parameter automatically recalculates the entire dataset for the new region, affecting the data source.",
      "The parameter behaves as a dynamic filter, altering the scope of the data presented based on user selection.",
      "The parameter functions as a decorative element, enhancing the visual appeal but not the data content."
    ],
    "answer": 3,
    "category": "Dashboard Parameters",
    "explanation": "In this context, the parameter acts as a dynamic filter, enabling the dashboard to display data specific to the selected region, without affecting the underlying dataset.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "How would you write a SQL query to list each product_id and its corresponding quantity for every order from a nested JSON array in Databricks?",
    "options": [
      "SELECT EXPLODE(order_info:items) AS (product_id, quantity) FROM customer_orders;",
      "SELECT order_info:items[*].product_id, order_info:items[*].quantity FROM customer_orders UNNEST items;",
      "SELECT FLATTEN(order_info:items.product_id, order_info:items.quantity) FROM customer_orders;",
      "SELECT order_info:items.product_id, order_info:items.quantity FROM customer_orders;",
      "SELECT order_info:items[*].product_id, order_info:items[*].quantity FROM customer_orders;"
    ],
    "answer": 0,
    "category": "SQL JSON",
    "explanation": "Using EXPLODE is the correct approach to handle nested JSON arrays and extract the product_id and quantity for each order in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In a Databricks environment, you are optimizing the performance of a data processing task that involves complex operations on arrays within a Spark SQL dataset. Which of the following higher-order functions in Spark SQL would be most suitable for efficiently transforming elements within an array column scores?",
    "options": [
      "SELECT ARRAY_CONTAINS(scores, 10) FROM dataset;",
      "SELECT COLLECT_LIST(scores) FROM dataset GROUP BY scores;",
      "SELECT ARRAY_SORT(scores) FROM dataset;",
      "SELECT EXPLODE(scores) FROM dataset;",
      "SELECT TRANSFORM(scores, score -> score * 2) FROM dataset;"
    ],
    "answer": 4,
    "category": "Spark SQL",
    "explanation": "The TRANSFORM function allows transformation of each element within the array, which is ideal for modifying array data as required in this context.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of Databricks SQL, which of the following statements accurately describes both a caution and a benefit of working with streaming data?",
    "options": [
      "Benefit: Streaming data allows for real-time analytics and decision-making. Caution: There is a higher risk of data inconsistency due to the continuous flow of data.",
      "Benefit: Streaming data simplifies data transformation. Caution: It can lead to increased costs due to the need for more computing resources.",
      "Benefit: Streaming data can handle large volumes of data efficiently. Caution: Real-time data processing may lead to higher error rates if not managed correctly.",
      "Benefit: Streaming data reduces the need for storage. Caution: It requires more complex SQL queries.",
      "Benefit: Streaming data ensures complete data privacy. Caution: It can lead to delayed data processing."
    ],
    "answer": 0,
    "category": "Streaming Data",
    "explanation": "Streaming data enables real-time analytics, but it poses risks of inconsistency as data flows continuously, which must be managed carefully.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst is creating a dashboard to present monthly sales data to company executives. After changes, the executives find the dashboard more informative. This scenario illustrates which of the following points about visualization formatting?",
    "options": [
      "Formatting is primarily used to reduce the size of the data set visually displayed.",
      "Formatting changes the underlying data, thus altering the data's interpretation.",
      "Over-formatting can lead to data misinterpretation by introducing visual biases.",
      "Proper formatting can enhance readability and comprehension, leading to a more accurate interpretation of the data.",
      "Formatting only affects the aesthetic aspect of the visualization and has no impact on its reception."
    ],
    "answer": 3,
    "category": "Data Visualization",
    "explanation": "Proper formatting improves readability and comprehension, enabling stakeholders to interpret the data more accurately.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, when creating a basic, schema-specific visualization, what is the first step you should take?",
    "options": [
      "Configure the dashboard settings to match the schema requirements.",
      "Write a SQL query to retrieve data from the specific schema.",
      "Adjust the data refresh rate to ensure real-time visualization.",
      "Import external visualization libraries for advanced charting.",
      "Select the visualization type from the visualization menu."
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Retrieving data with a SQL query from the specific schema is the first step in ensuring the visualization is based on accurate data.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "When importing data from an Amazon S3 bucket into a Databricks environment using Databricks SQL, which SQL command is typically used to perform this operation?",
    "options": [
      "CREATE TABLE my_table USING CSV LOCATION 's3://mybucket/mydata.csv';",
      "INSERT INTO my_table SELECT * FROM s3a://mybucket/mydata.csv;",
      "SELECT * INTO my_table FROM OPENROWSET(BULK 's3://mybucket/mydata.csv', SINGLE_CLOB) AS mydata;",
      "COPY INTO my_table FROM 's3://mybucket/mydata.csv' FILEFORMAT = CSV;",
      "LOAD DATA INPATH 's3://mybucket/mydata.csv' INTO TABLE my_table;"
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "The correct command uses CREATE TABLE with the USING clause to specify the S3 location for the CSV file, enabling data import directly from S3.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "As a data analyst, you are preparing to present your findings from a recent study. Which of the following steps aligns most closely with the initial phase of preparing your presentation in Databricks SQL?",
    "options": [
      "Drafting a preliminary report in a word processing software.",
      "Designing an interactive frontend interface using HTML and CSS.",
      "Developing a Python script to perform advanced statistical analysis.",
      "Crafting a SQL query to gather and summarize the relevant data.",
      "Scheduling meetings with stakeholders to discuss data interpretations."
    ],
    "answer": 3,
    "category": "Data Presentation",
    "explanation": "The initial phase involves crafting SQL queries to gather and summarize relevant data, which will serve as the basis for the presentation.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What are the primary benefits of implementing Delta Lake within the Databricks Lakehouse architecture?",
    "options": [
      "Delta Lake primarily enhances data security and compliance features.",
      "Delta Lake is beneficial only for handling unstructured data types.",
      "Delta Lake provides ACID transactions, scalable metadata handling, and time-travel features.",
      "It mainly improves the graphical user interface for data exploration.",
      "It offers high-speed streaming data ingestion and real-time analytics capabilities."
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Delta Lake's primary benefits include support for ACID transactions, metadata scalability, and time-travel features, enhancing data reliability and manageability.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following statements accurately distinguishes between discrete and continuous statistics in the context of data analysis?",
    "options": [
      "Continuous data can only take on integer values, whereas discrete data can take on any value, including decimals.",
      "Discrete data is used for time series analysis, while continuous data is not suitable for this purpose.",
      "Continuous data is always non-numeric, whereas discrete data is numeric.",
      "Both discrete and continuous data are types of categorical data used in qualitative analysis.",
      "Discrete data can only take on a finite number of values, while continuous data can take on any value within a given range."
    ],
    "answer": 4,
    "category": "Statistics",
    "explanation": "Discrete data takes on specific, countable values, while continuous data can take on any value within a range, suitable for measurements.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the correct method to rename a table in Databricks?",
    "options": [
      "Use the ALTER TABLE RENAME TO command.",
      "Update the table name through the Databricks UI.",
      "Delete the old table and create a new one with the desired name.",
      "Modify the table name in the metadata file.",
      "Use the RENAME TABLE command."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The correct way to rename a table in Databricks is using the ALTER TABLE RENAME TO command, which changes the table's name directly.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "How can you determine if a table in Databricks is managed or unmanaged?",
    "options": [
      "By the size of the table data.",
      "By the location of the data files specified in the table definition.",
      "By the type of data stored in the table.",
      "By the speed of query execution on the table.",
      "By the number of columns in the table."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "In Databricks, the location of data files helps determine if a table is managed (stored in Databricks-managed storage) or unmanaged (external storage).",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In a Databricks SQL context, consider a dataset with columns 'Department', 'Employee', and 'Sales'. You are required to analyze the data using the ROLLUP and CUBE functions. Given this scenario, select the correct statement regarding the type of aggregations ROLLUP and CUBE would generate when applied to the 'Department' and 'Employee' columns.",
    "options": [
      "ROLLUP provides aggregations only for each combination of 'Department' and 'Employee', while CUBE gives a detailed breakdown including each 'Department', each 'Employee', and a grand total.",
      "ROLLUP generates hierarchical aggregations starting from the leftmost column in the GROUP BY clause. It would produce subtotals for each 'Department', subtotals for each combination of 'Department' and 'Employee', and a grand total.",
      "CUBE creates aggregations for all possible combinations of the columns in the GROUP BY clause. It would generate subtotals for each 'Department', each 'Employee', each combination of 'Department' and 'Employee', and a grand total.",
      "Neither ROLLUP nor CUBE will generate subtotals for individual 'Departments' or 'Employees'; they only provide a grand total.",
      "Both ROLLUP and CUBE produce identical aggregations, including subtotals for each 'Department', each 'Employee', each combination of 'Department' and 'Employee', and a grand total."
    ],
    "answer": 1,
    "category": "SQL Aggregation",
    "explanation": "ROLLUP generates hierarchical subtotals, providing aggregated values at multiple levels, making it suitable for creating comprehensive summaries by department and employee.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks SQL, what is the primary purpose of using the ANALYZE TABLE command?",
    "options": [
      "To synchronize the table data with an external data source.",
      "To change the data storage format of the table.",
      "To modify the structure of a table by adding or removing columns.",
      "To collect statistics about the table for optimizing query performance.",
      "To back up the table data to a specified location."
    ],
    "answer": 3,
    "category": "SQL Optimization",
    "explanation": "ANALYZE TABLE collects table statistics, which are essential for the query optimizer to improve performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What is the primary purpose of Databricks SQL warehouses/warehouses in a data analytics environment?",
    "options": [
      "To offer a centralized platform for advanced machine learning model development and deployment.",
      "To provide a secure environment for data encryption and compliance management.",
      "To serve as the primary storage location for large-scale data sets, replacing traditional data warehouses.",
      "To act as the primary interface for application development and deployment within the Databricks ecosystem.",
      "To facilitate SQL-based data querying and analysis, offering a managed environment for running SQL queries on large datasets."
    ],
    "answer": 4,
    "category": "Databricks SQL warehouses",
    "explanation": "Databricks SQL warehouses/warehouses are primarily designed to handle SQL queries and analysis on large datasets efficiently, providing a robust environment for analytics.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Consider the following SalesData table, and an SQL query is executed to generate a specific output. Which query most likely used to generate this output?",
    "options": [
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY Region, Product, ROLLUP(Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY CUBE(Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY Region, Product;",
      "SELECT Region, Product, COUNT(*) AS TotalSales FROM SalesData GROUP BY Region, Product, CUBE(Region, Product);"
    ],
    "answer": 1,
    "category": "SQL Aggregation",
    "explanation": "Using GROUP BY CUBE generates all combinations of the grouped columns with subtotals and a grand total, matching the example output.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a critical organization-specific consideration when handling PII data?",
    "options": [
      "Adapting PII data handling protocols to comply with regional and sector-specific privacy laws.",
      "Developing a uniform public access policy for all PII data.",
      "Implementing a one-size-fits-all approach to PII data storage and processing.",
      "Prioritizing cost-saving measures over data security for PII data.",
      "Always using the same encryption method for PII data across all departments."
    ],
    "answer": 0,
    "category": "Data Privacy",
    "explanation": "Complying with regional and sector-specific privacy laws is crucial to handle PII data appropriately, ensuring adherence to varying legal requirements.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of Delta Lake tables in Databricks, how is historical data maintained, and what command is used to access this history?",
    "options": [
      "Delta Lake tables do not maintain historical data.",
      "Historical data is maintained through versioned table updates, accessed with the DESCRIBE HISTORY command.",
      "Historical data is maintained in separate snapshot tables, accessed with the SHOW SNAPSHOT command.",
      "Historical data is stored in a dedicated audit log, accessed with the VIEW AUDIT LOG command.",
      "Historical data is maintained through periodic backups, accessed with the SHOW BACKUP command."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake maintains historical data through versioned table updates, accessible via the DESCRIBE HISTORY command, which provides a record of changes.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Given a database with a table Orders, you want to retrieve the list of orders placed by a particular customer where the order amount is greater than $500. Which SQL query will correctly retrieve this data?",
    "options": [
      "DELETE FROM Orders WHERE CustomerId = 123 AND Amount > 500;",
      "UPDATE Orders SET Amount = 500 WHERE CustomerId = 123;",
      "SELECT OrderId FROM Orders WHERE CustomerId = 123 OR Amount > 500;",
      "SELECT * FROM Orders WHERE CustomerId = 123 AND Amount > 500;",
      "SELECT CustomerId, Amount FROM Orders WHERE OrderId = 123 AND Amount > 500;"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "The query needs to select all fields where CustomerId is 123 and Amount is greater than 500, which is done with a SELECT * statement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When sharing Databricks SQL dashboards, there are two primary settings: 'Run as viewer' and 'Run as owner'. What are the pros and cons of each setting in the context of sharing dashboards?",
    "options": [
      "'Run as viewer' allows full customization of queries for each viewer (pro), but can be more resource-intensive (con). 'Run as owner' simplifies query management (pro), but doesn't account for individual user access levels (con).",
      "'Run as viewer' and 'Run as owner' both provide the same level of data visibility (pro), but may have limitations in customizing query execution (con).",
      "'Run as owner' offers greater customization of dashboard settings (pro), but requires viewers to have advanced knowledge of SQL (con). 'Run as viewer' simplifies the user experience (pro), but may lead to inconsistent data reporting (con).",
      "'Run as viewer' enhances data security by adhering to individual viewer's permissions (pro), but may limit data visibility (con). 'Run as owner' ensures consistent data visibility across users (pro), but might pose security risks if the owner has broader data access (con).",
      "Both settings allow for easy sharing of dashboards (pro), but can lead to complexities in managing user permissions and data access (con)."
    ],
    "answer": 3,
    "category": "Dashboard Sharing",
    "explanation": "'Run as viewer' allows viewers to access data based on their permissions, enhancing security, while 'Run as owner' ensures data consistency but may pose security risks.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the context of statistics, what are key moments of a statistical distribution?",
    "options": [
      "The mean, variance, skewness, and kurtosis, which are the first four moments of a distribution.",
      "The mean, median, and mode, which define the central tendency of the distribution.",
      "The skewness and kurtosis, which describe the shape and tail behavior of the distribution.",
      "The range, interquartile range, and standard deviation, which describe the variability of the distribution.",
      "The maximum and minimum values, which set the boundaries of the distribution."
    ],
    "answer": 0,
    "category": "Statistics",
    "explanation": "The first four moments of a distribution are mean, variance, skewness, and kurtosis, describing central tendency, variability, asymmetry, and peakedness.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What are the primary responsibilities of a table owner in Databricks?",
    "options": [
      "Designing the visual representation of the table data.",
      "Regularly updating the table data to keep it current.",
      "Ensuring the table is always available for querying and analysis.",
      "Optimizing the table for faster query performance.",
      "Managing user access and permissions for the table."
    ],
    "answer": 4,
    "category": "Data Management",
    "explanation": "Table owners in Databricks are responsible for managing user access and permissions, ensuring proper security and accessibility of the table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "You are using Databricks to load data from a CSV file into a Delta table named SalesData. Which Databricks SQL COPY INTO command correctly imports this data?",
    "options": [
      "COPY INTO SalesData FROM CSV 'DBFS:/data/sales.csv' WITH HEADER;",
      "COPY INTO SalesData FROM 'DBFS:/data/sales.csv' USING FORMAT AS CSV HEADER = TRUE;",
      "COPY INTO SalesData FROM 'DBFS:/data/sales.csv' FORMAT = CSV FORMAT_OPTIONS ('header' = 'true');",
      "COPY INTO SalesData FROM 'DBFS:/data/sales.csv' FILEFORMAT = 'CSV';",
      "COPY INTO SalesData FROM 'DBFS:/data/sales.csv' USING (FILEFORMAT = CSV, HEADER = 'true');"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "The correct syntax for importing with a header row in Databricks SQL is to use FILEFORMAT and HEADER options in the COPY INTO command.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst is using Databricks SQL to visualize data showing the monthly sales trends across different regions. Based on the data and the goal of showing trends over time, which visualization type should the analyst select in Databricks SQL?",
    "options": [
      "Heatmap, for representing the intensity of sales in different regions.",
      "Bar chart, as it is best for comparing categories of data at a single point in time.",
      "Pie chart, to show the proportion of sales in each region compared to the whole.",
      "Line chart, as it effectively displays trends and changes over time.",
      "Scatter plot, to show the relationship between sales volume and time."
    ],
    "answer": 3,
    "category": "Data Visualization",
    "explanation": "A line chart is ideal for visualizing trends over time, as it highlights changes and patterns across time intervals.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the context of Databricks, how is Personally Identifiable Information (PII) typically handled to ensure data privacy and compliance?",
    "options": [
      "By creating a separate database for PII.",
      "Through the use of Delta Lake features for fine-grained access control.",
      "By automatically encrypting all data fields that contain PII.",
      "PII is not specifically handled in Databricks; it relies on external tools.",
      "By anonymizing PII data through built-in Databricks functions."
    ],
    "answer": 1,
    "category": "Data Security",
    "explanation": "Delta Lake's fine-grained access control enables secure handling of PII by restricting access at a detailed level within Databricks.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "You are analyzing a dataset OrderDetails in a Databricks SQL environment, which includes OrderID, ProductID, Quantity, and UnitPrice. Your objective is to find the total revenue generated by each product. Which SQL query effectively aggregates this data to provide the desired output?",
    "options": [
      "SELECT ProductID, SUM(Quantity) FROM OrderDetails GROUP BY ProductID;",
      "SELECT ProductID, AVG(UnitPrice * Quantity) FROM OrderDetails GROUP BY ProductID;",
      "SELECT ProductID, SUM(UnitPrice) FROM OrderDetails GROUP BY ProductID;",
      "SELECT ProductID, SUM(UnitPrice * Quantity) AS TotalRevenue FROM OrderDetails GROUP BY ProductID;",
      "SELECT OrderID, SUM(UnitPrice * Quantity) AS TotalRevenue FROM OrderDetails GROUP BY OrderID;"
    ],
    "answer": 3,
    "category": "SQL Aggregation",
    "explanation": "To calculate the total revenue, we multiply UnitPrice by Quantity for each row, then sum these values for each ProductID, as shown in the correct query.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Regarding the accessibility and functionality of Databricks SQL dashboards, which statement best reflects their use in a business environment by various stakeholders?",
    "options": [
      "Only the original creator can view and run it.",
      "Only data engineers and IT professionals can view and run it due to technical nature.",
      "Dashboards are exclusively for top-level executives.",
      "A variety of users, including analysts, managers, and stakeholders, can view and run for collaborative analysis.",
      "Dashboards are publicly accessible with internet access, without any restrictions."
    ],
    "answer": 3,
    "category": "Dashboard Accessibility",
    "explanation": "Databricks SQL dashboards are designed for collaborative analysis, allowing a range of users to interact with the data, making it ideal for team-based environments.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "How is data enhancement commonly applied in analytics?",
    "options": [
      "By converting unstructured data into structured format for easier database storage.",
      "Through cleaning and normalizing data to maintain consistency in databases.",
      "By anonymizing sensitive information in datasets for privacy compliance.",
      "By reducing the size of the dataset to improve processing speed.",
      "By incorporating external data sources to enrich existing datasets for deeper insights."
    ],
    "answer": 4,
    "category": "Data Enhancement",
    "explanation": "Data enhancement often involves incorporating external data to provide deeper insights, enriching the original dataset with additional information.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the queries below could have been used to generate the output?",
    "options": [
      "SELECT Name, Department FROM Employee WHERE Salary OVER 50000;",
      "SELECT Name, Department FROM Employee WHERE Salary >= 50000;",
      "SELECT Department, Name FROM Employee WHERE Salary > 50000;",
      "SELECT * FROM Employee WHERE Salary > 50000;",
      "SELECT Name, Department FROM Employee WHERE Salary >= 50000;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The correct query is 'SELECT Name, Department FROM Employee WHERE Salary >= 50000;', as it selects only the Name and Department columns for employees with a salary above 50,000.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, setting up a refresh schedule for dashboards is essential for ensuring that the displayed data is current. How does one configure a refresh schedule for a Databricks SQL dashboard?",
    "options": [
      "Sending periodic requests to the Databricks support team to refresh the dashboard.",
      "Writing a custom script in the dashboard's SQL queries to automate refreshes.",
      "Utilizing an external tool to trigger refreshes in the Databricks dashboard.",
      "Manually updating the dashboard at regular intervals without any automated scheduling.",
      "Click Schedule at the upper-right of the dashboard. Then, click Add schedule."
    ],
    "answer": 4,
    "category": "Dashboard Configuration",
    "explanation": "To set a refresh schedule, users can access the scheduling options within the dashboard interface, allowing automated updates.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which query most likely used to generate this output?",
    "options": [
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY ROLLUP (Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY CUBE (Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY ROLLUP (Product, Region);",
      "SELECT Region, Product, COUNT(*) AS TotalSales FROM SalesData GROUP BY Region, Product WITH ROLLUP;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The 'CUBE' function generates a result that includes all possible combinations of aggregations for the specified columns, including NULL values for grand totals. This matches the provided output.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In a Databricks SQL environment, you are tasked with analyzing sales data. You need to find all products that had their first sale amounting to more than $500. Which SQL query using a subquery appropriately retrieves this information?",
    "options": [
      "SELECT ProductID FROM SalesRecords WHERE SaleAmount > 500 AND SaleDate = (SELECT MIN(SaleDate) FROM SalesRecords GROUP BY ProductID);",
      "SELECT ProductID FROM SalesRecords WHERE SaleAmount > 500 AND SaleID IN (SELECT MIN(SaleID) FROM SalesRecords GROUP BY ProductID);",
      "SELECT DISTINCT ProductID FROM SalesRecords WHERE SaleAmount > 500 GROUP BY ProductID HAVING SaleDate = MIN(SaleDate);",
      "SELECT ProductID FROM SalesRecords WHERE SaleAmount > 500 AND SaleDate = (SELECT MIN(SaleDate) FROM SalesRecords WHERE ProductID = SalesRecords.ProductID);",
      "SELECT ProductID FROM (SELECT ProductID, MIN(SaleDate) AS FirstSaleDate FROM SalesRecords GROUP BY ProductID) AS FirstSales WHERE FirstSaleDate > 500;"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "The correct answer uses a subquery to find the minimum sale date for each ProductID where SaleAmount is over 500, filtering correctly based on the specific product's earliest sale.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the context of Databricks, what is the primary advantage of using a Serverless Databricks SQL warehouses/warehouse?",
    "options": [
      "It specializes in real-time data streaming and complex event processing for IoT applications.",
      "It is primarily designed for large-scale machine learning workloads requiring extensive computational resources.",
      "It provides a dedicated environment for developing complex data pipelines and ETL processes.",
      "It enables quick-start capabilities, significantly reducing the time to initiate SQL queries and data analysis tasks.",
      "It offers enhanced data security and privacy controls suitable for sensitive data processing."
    ],
    "answer": 3,
    "category": "Databricks SQL",
    "explanation": "Serverless SQL warehouses in Databricks are designed to quickly start SQL queries and data tasks without infrastructure management.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, how do you identify the owner of a table using Catalog Explorer?",
    "options": [
      "The owner is displayed when clicking on a table in the Catalog Explorer.",
      "By inspecting the table's creation script in Catalog Explorer.",
      "Through the 'Owner' column in the Catalog Explorer’s table list.",
      "Table ownership is not visible in Catalog Explorer.",
      "By executing a SQL query in Catalog Explorer to retrieve the table owner."
    ],
    "answer": 2,
    "category": "Databricks Catalog",
    "explanation": "The 'Owner' column in the table list of Catalog Explorer directly shows the ownership information.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "You are analyzing a dataset in Databricks SQL named WeatherReadings which includes the columns StationID (integer), ReadingTimestamp (timestamp), and Temperature (float). You need to calculate the average temperature for each station in 1-hour windows, sliding every 30 minutes. Which SQL query correctly uses the windowing function to achieve this?",
    "options": [
      "SELECT StationID, window(ReadingTimestamp, '1 hour', '30 minutes'), AVG(Temperature) FROM WeatherReadings GROUP BY StationID, window(ReadingTimestamp, '1 hour', '30 minutes');",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp RANGE BETWEEN INTERVAL '30 minutes' PRECEDING AND INTERVAL '30 minutes' FOLLOWING) FROM WeatherReadings;",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp ROWS BETWEEN INTERVAL '30 minutes' PRECEDING AND CURRENT ROW) FROM WeatherReadings;",
      "SELECT StationID, window(ReadingTimestamp, '1 hour'), AVG(Temperature) FROM WeatherReadings GROUP BY StationID, window(ReadingTimestamp, '1 hour');",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp) FROM WeatherReadings;"
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "The window function allows calculating the average temperature within defined time intervals, in this case, 1-hour windows with a 30-minute sliding interval.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what distinguishes a managed table from an unmanaged (external) table in terms of data and metadata management?",
    "options": [
      "Managed tables allow for external file storage, whereas unmanaged tables store data within the Databricks environment.",
      "Managed tables store both data and metadata in the cloud, while unmanaged tables store only metadata.",
      "Unmanaged tables enable ACID transactions, unlike managed tables.",
      "Managed tables require manual data deletion after dropping the table, whereas unmanaged tables automatically delete data.",
      "Managed tables have their data and metadata managed by Databricks, while unmanaged tables have only metadata managed."
    ],
    "answer": 4,
    "category": "Table Management",
    "explanation": "Databricks fully manages both data and metadata for managed tables, whereas for unmanaged tables only the metadata is managed.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In the context of Databricks dashboards, how do query parameters influence the output of underlying SQL queries within a dashboard?",
    "options": [
      "Query parameters serve as placeholders in SQL queries, allowing for dynamic data filtering based on user input, thus altering the output of the query according to the specified parameter values.",
      "Query parameters are used to format the visual aspects of the output, such as color and font, without changing the actual data returned by the query.",
      "They modify the layout of the dashboard, rearranging the visualizations based on user preferences, but do not change the data output of the queries.",
      "Query parameters act as static reference points in SQL queries, ensuring that the output remains constant irrespective of user interactions.",
      "They automatically update the SQL queries on a set schedule, such as daily or weekly, to change the output data based on temporal parameters."
    ],
    "answer": 0,
    "category": "Dashboard Parameters",
    "explanation": "Query parameters in Databricks dashboards allow for dynamic filtering, adjusting the data returned based on user input and thus making dashboards more interactive and customizable.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "How does Delta Lake manage table metadata in Databricks?",
    "options": [
      "By storing metadata in a separate, dedicated cloud storage.",
      "By automatically synchronizing metadata with the primary data storage.",
      "Delta Lake does not manage table metadata.",
      "Through manual updates by the database administrator.",
      "By maintaining a transaction log that records metadata changes."
    ],
    "answer": 4,
    "category": "Delta Lake",
    "explanation": "Delta Lake uses a transaction log to track metadata changes, ensuring data consistency and enabling time travel queries.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Consider the following two tables: Colors and Shapes. After executing an SQL JOIN query, you receive the following result. What type of JOIN was used to produce this result?",
    "options": [
      "FULL JOIN",
      "LEFT JOIN",
      "INNER JOIN",
      "RIGHT JOIN",
      "CROSS JOIN"
    ],
    "answer": 4,
    "category": "SQL Joins",
    "explanation": "The result shows all possible combinations between the two tables, indicating a CROSS JOIN, which returns the Cartesian product of the tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how is a 'Query Based Dropdown List' used to enhance the functionality of a dashboard with query parameters?",
    "options": [
      "It dynamically generates a dropdown list based on the distinct output of a separate query, allowing users to select values as query parameters.",
      "It creates a fixed list of predefined options that users can choose from to filter dashboard data.",
      "It automatically updates query parameters based on external data sources, without user interaction.",
      "It's used to manually input query parameters, unrelated to the output of any other query.",
      "The dropdown list is purely aesthetic, with no impact on the actual queries or data displayed."
    ],
    "answer": 0,
    "category": "Dashboard Enhancement",
    "explanation": "A 'Query Based Dropdown List' is used to dynamically populate options based on the result of a query, enhancing user interaction and filtering on the dashboard.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Assuming you have a table TrafficData with columns Timestamp (timestamp) and VehicleCount (integer), and you need to calculate the sum of VehicleCount for every 10-minute window. Which SQL query using the window_time function correctly achieves this in a Databricks environment?",
    "options": [
      "SELECT window_time(Timestamp, '10 minutes'), COUNT(VehicleCount) FROM TrafficData GROUP BY Timestamp;",
      "SELECT window_time(Timestamp, '10 minutes'), SUM(VehicleCount) FROM TrafficData GROUP BY window_time(Timestamp, '10 minutes');",
      "SELECT window_time(Timestamp, '10 minutes'), AVG(VehicleCount) FROM TrafficData GROUP BY window_time(Timestamp, '10 minutes');",
      "SELECT Timestamp, SUM(VehicleCount) OVER (ORDER BY Timestamp RANGE BETWEEN INTERVAL 10 MINUTES PRECEDING AND CURRENT ROW) FROM TrafficData;",
      "SELECT Timestamp, SUM(VehicleCount) OVER (PARTITION BY window_time(Timestamp, '10 minutes')) FROM TrafficData;"
    ],
    "answer": 1,
    "category": "Window Functions",
    "explanation": "The query groups data by 10-minute intervals using the window_time function and calculates the sum of VehicleCount for each interval.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When using the schema browser in the Query Editor of Databricks, what type of information can you expect to find displayed? Choose the most appropriate option.?",
    "options": [
      "List of all users and groups who have access to the Databricks workspace.",
      "Only the SQL queries that have been executed in the past sessions.",
      "Real-time performance metrics and logs of the Databricks cluster.",
      "Detailed documentation and syntax for all SQL functions and commands.",
      "Available databases, tables, and columns, along with their data types."
    ],
    "answer": 4,
    "category": "Schema Browser",
    "explanation": "The schema browser displays available databases, tables, and columns with data types, aiding users in query construction.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of analytics, what is an example of effectively enhancing data in a common application?",
    "options": [
      "Integrating weather data into a retail sales analysis to understand the impact of weather on sales trends.",
      "Restricting data access to a limited number of users to ensure data security.",
      "Strictly categorizing data based on its source without additional processing.",
      "Keeping data in its original, raw format for archival purposes.",
      "Performing routine software updates on data analysis tools without modifying data."
    ],
    "answer": 0,
    "category": "Data Enhancement",
    "explanation": "Integrating external data, such as weather, enhances analytics by providing additional context that may affect sales trends.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Consider two tables in a database: Table 1: Employees | EmployeeID | Name | Department | |------------|-------------|------------| | 101 | Alan Smith | HR | | 102 | Betty Jones | Marketing | | 103 | Carl Brown | IT | | 104 | Donna Ray | Sales | Table 2: Salaries | EmployeeID | Salary | |------------|--------| | 101 | 70000 | | 102 | 60000 | | 103 | 50000 | | 104 | 80000 | Given the output of a join between the two tables: | Name | Salary | |------------|--------| | Donna Ray | 80000 | | Alan Smith | 70000 | Which of the queries below could have been used to generate the output?",
    "options": [
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary >= (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary ASC;",
      "SELECT * FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary >= (SELECT MAX(Salary) FROM Salaries) ORDER BY S.Salary DESC;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The correct query joins the Employees and Salaries tables on EmployeeID, filters salaries greater than the average salary, and orders the result by salary in descending order to match the output requirement. This produces only the rows for Donna Ray and Alan Smith, with their respective salaries of 80000 and 70000.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the correct sequence of steps to execute a SQL query in Databricks?",
    "options": [
      "Create a query using Terraform, execute the query in a Databricks job, and use COPY INTO to load data.",
      "Manually input data, write a query in Databricks notebook, execute the query, and export the results.",
      "Choose a SQL warehouse, construct and edit the query, execute the query, and visualize results.",
      "Write the query in an external tool, import it into Databricks, select a data source, and execute the query.",
      "Open SQL Editor, select a SQL warehouse, construct and edit the query, execute the query."
    ],
    "answer": 4,
    "category": "SQL Execution",
    "explanation": "The standard approach in Databricks for executing a SQL query involves using the SQL Editor, selecting the appropriate warehouse, constructing the query, and executing it.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is tasked with presenting yearly revenue data to stakeholders in a way that is both informative and visually appealing. The analyst decides to use a line graph to show the revenue trends over the year. What is the best approach the analyst should take in terms of formatting the graph?",
    "options": [
      "Add multiple background images related to the company’s business to make the graph more engaging.",
      "Use 3D effects on the graph lines to give a more modern and advanced look.",
      "Use a wide range of vibrant colors for different data points to make the graph more colorful.",
      "Apply a minimalistic design with a consistent color scheme and clear labeling to enhance focus on the data.",
      "Incorporate various font styles and sizes for each data point to make the graph more dynamic."
    ],
    "answer": 3,
    "category": "Data Visualization",
    "explanation": "A minimalistic design with clear labeling and a consistent color scheme enhances readability and maintains focus on the data trends rather than unnecessary visual elements.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "You need to apply a custom scaling function to normalize transaction amounts for analysis. You decide to create a UDF (User-Defined Function) in Python. Which of the following approaches correctly illustrates the creation and application of a UDF for this purpose?",
    "options": [
      "Use a built-in SQL function to normalize TransactionAmount directly within a SQL query without defining a UDF.",
      "Create a Python script outside Databricks to preprocess the data, then import the normalized dataset into Databricks.",
      "Define a Python function normalize(amount) and register it as a UDF, then use SELECT normalize(TransactionAmount) FROM Transactions;",
      "Implement a machine learning model within Databricks to automatically normalize TransactionAmount.",
      "Manually apply the normalization function to each row of the Transactions dataset using a Spark DataFrame loop."
    ],
    "answer": 2,
    "category": "Python UDF",
    "explanation": "Defining and registering a Python function as a UDF allows for the normalization of TransactionAmount directly within SQL queries in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Given a sales database with a table SalesData containing columns Region, ProductType, and SalesAmount, you are tasked with creating a report that includes the total sales amount for each combination of Region and ProductType, as well as totals for each Region alone and the overall total. Which SQL query correctly generates this report?",
    "options": [
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY CUBE(Region, ProductType);",
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY Region, ProductType WITH CUBE;",
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY ROLLUP(Region, ProductType);",
      "SELECT Region, SUM(SalesAmount) FROM SalesData GROUP BY CUBE(Region);",
      "SELECT Region, ProductType, COUNT(SalesAmount) FROM SalesData GROUP BY CUBE(Region, ProductType);"
    ],
    "answer": 0,
    "category": "SQL Aggregation",
    "explanation": "The CUBE function generates all combinations of specified columns, which is needed to calculate totals at multiple hierarchical levels (e.g., by Region and ProductType, by Region only, and overall total).",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, when dealing with the ingestion of data, how does the platform handle directories containing multiple files?",
    "options": [
      "Databricks SQL requires manual conversion of all files in a directory to a uniform format before ingestion.",
      "Databricks SQL automatically converts and ingests files of different types from a directory into a standard format.",
      "Databricks SQL can ingest directories of files, provided all files in the directory are of the same type, such as all CSV or all JSON.",
      "Databricks SQL can only ingest a single file at a time, regardless of file type.",
      "Databricks SQL can ingest directories containing files of mixed types, such as CSV and JSON, simultaneously."
    ],
    "answer": 2,
    "category": "Data Ingestion",
    "explanation": "Databricks SQL can ingest directories containing multiple files as long as all files in the directory are of the same type, such as CSV or JSON.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the context of Databricks, when dealing with the import of small text files, such as lookup tables or for quick data integrations, which approach is recommended to OPTIMIZE performance and ease of use?",
    "options": [
      "Employ small-file upload techniques specifically designed for handling small text files efficiently.",
      "Upload small text files to a temporary storage service before importing them into Databricks.",
      "Always compress text files into larger archives before uploading, regardless of the original file size.",
      "Utilize large-file upload methods for all types of data, regardless of file size.",
      "Convert small text files to a binary format to increase upload speed and efficiency."
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Optimizing for small text files involves specific techniques that efficiently handle smaller data volumes without additional steps.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "In managing Databricks SQL warehouses or warehouses, a key consideration is the balance between computational power and cost. Which statement best encapsulates this trade-off?",
    "options": [
      "Smaller clusters are more expensive and offer high performance, suitable for complex analytics.",
      "Larger clusters enhance performance but increase costs, suitable for demanding data tasks, whereas smaller clusters reduce costs but may limit performance.",
      "Cluster size impacts only the storage capacity, not the cost or performance.",
      "Larger clusters offer lower performance but are more cost-effective, ideal for high-volume data processing.",
      "Cluster size has no impact on cost or performance in Databricks SQL."
    ],
    "answer": 1,
    "category": "Databricks SQL Management",
    "explanation": "Larger clusters improve performance but come with higher costs, making them suitable for demanding tasks, while smaller clusters are more cost-effective but may not perform as well.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the Databricks Unity Catalog, which SQL command correctly creates a new table named customer_data in a database sales under the catalog us_catalog, based on a SELECT query from an existing table transactions in the same database and catalog?",
    "options": [
      "NEW TABLE us_catalog.sales.customer_data FROM SELECT * IN transactions;",
      "TABLE CREATE us_catalog.sales.customer_data AS (SELECT * FROM transactions);",
      "CREATE TABLE customer_data IN us_catalog.sales AS SELECT * FROM transactions;",
      "CREATE TABLE us_catalog.sales.customer_data AS SELECT * FROM us_catalog.sales.transactions;",
      "us_catalog.sales: CREATE TABLE customer_data AS SELECT * FROM transactions;"
    ],
    "answer": 3,
    "category": "Unity Catalog",
    "explanation": "The correct syntax specifies the catalog and schema, followed by the CREATE TABLE command and the SELECT query.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "The Medallion Architecture in Databricks is a conceptual framework for data organization and pipeline management. How does it structure the data processing pipeline?",
    "options": [
      "It involves a single stage of data processing where raw, refined, and curated data are merged into a unified table known as the Platinum table.",
      "It uses a reverse-pyramid structure starting with the most refined data in the base layer, moving to semi-processed data, and ending with raw data at the top.",
      "None of the above",
      "It starts with unstructured data in Gold tables, then structures the data in Silver tables, and finally stores the raw data in Bronze tables.",
      "It begins with raw data in Bronze tables, moves to refined data in Silver tables, and culminates in curated data in Gold tables."
    ],
    "answer": 4,
    "category": "Medallion Architecture",
    "explanation": "The Medallion Architecture in Databricks organizes data through a Bronze-Silver-Gold structure, progressing from raw data to curated data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of a Databricks Lakehouse architecture, you are working with silver-level data that has been aggregated from various bronze tables. You notice inconsistencies in customer names due to variations in casing and spacing (e.g., 'John Doe', 'john doe', 'John Doe'). What would be an appropriate SQL query to standardize these customer names in the silver table CustomerData?",
    "options": [
      "CREATE VIEW CleanCustomerData AS SELECT DISTINCT TRIM(LOWER(customer_name)) FROM CustomerData;",
      "SELECT customer_name FROM CustomerData GROUP BY customer_name;",
      "UPDATE CustomerData SET customer_name = TRIM(UPPER(customer_name));",
      "SELECT DISTINCT TRIM(UPPER(customer_name)) FROM CustomerData;",
      "ALTER TABLE CustomerData MODIFY COLUMN customer_name SET DATA TYPE VARCHAR(255) NOT NULL;"
    ],
    "answer": 0,
    "category": "Data Standardization",
    "explanation": "Creating a view with TRIM and LOWER functions standardizes names by removing extra spaces and making all letters lowercase.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In a data analytics environment, how can dashboards be configured to automatically refresh and display the most current data?",
    "options": [
      "Dashboards automatically refresh only when the underlying data source is replaced with a new one.",
      "Dashboards can be set up to refresh automatically at specified intervals using built-in scheduling features.",
      "Automatic refreshes are achieved by scripting a periodic page reload in the web browser displaying the dashboard.",
      "Automatic dashboard refreshes require a complete system reboot at regular intervals.",
      "Dashboards must be manually refreshed by the user to display the latest data."
    ],
    "answer": 1,
    "category": "Dashboard Configuration",
    "explanation": "Databricks SQL allows setting up automatic refresh intervals for dashboards, enabling real-time data monitoring and updates.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In data analytics, identify a scenario where data enhancement would significantly improve the outcome.",
    "options": [
      "In a marketing campaign, to enrich customer data with additional demographic and psychographic information for targeted advertising.",
      "When migrating data from one storage system to another without changing its format or content.?",
      "When performing routine data backup and recovery processes.?",
      "When aggregating large volumes of data for storage efficiency without analysis.?",
      "During the initial stages of data collection where data volume is more critical than data quality."
    ],
    "answer": 0,
    "category": "Data Enhancement",
    "explanation": "Data enhancement is valuable in targeted advertising because it enables more precise targeting by enriching customer data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst at an e-commerce company is using Databricks SQL to visualize customer feedback scores (ranging from 1 to 5) against product categories. The analyst wants to identify patterns and outliers in the feedback scores across different categories. Which visualization type should the analyst select in Databricks SQL to effectively communicate these insights?",
    "options": [
      "Stacked bar chart, for showing the total feedback scores by category.",
      "Bubble chart, for showing the volume of feedback in relation to scores across categories.",
      "Radar chart, to compare the feedback scores of different categories in a circular format.",
      "Treemap, to represent feedback scores as proportions within each category.",
      "Box chart, to display the distribution of feedback scores within each category, highlighting the median, quartiles, and outliers."
    ],
    "answer": 4,
    "category": "Data Visualization",
    "explanation": "A box chart effectively shows the distribution of feedback scores by category, with additional insight into median, quartiles, and outliers.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the realm of data visualization and analysis, Databricks SQL dashboards serve a specific purpose. What is the primary function of Databricks SQL dashboards in the context of data query results?",
    "options": [
      "To display the output of a single, complex SQL query.",
      "To serve as an interactive platform for writing and testing new SQL queries.",
      "To showcase the results of multiple SQL queries simultaneously in a unified view.",
      "To execute real-time data transformations without displaying results.",
      "To store raw data for long-term archival purposes."
    ],
    "answer": 2,
    "category": "Databricks SQL Dashboards",
    "explanation": "Databricks SQL dashboards are designed to present results from multiple SQL queries in a single, unified view, allowing for comprehensive data visualization.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "What is a key benefit of using ANSI SQL as the standard query language in the Lakehouse architecture?",
    "options": [
      "It allows for real-time data streaming and complex event processing.",
      "It supports native machine learning algorithms.",
      "It ensures compatibility and interoperability across different database systems.",
      "It enables automatic data encryption and security.",
      "It provides enhanced graphical data visualization tools."
    ],
    "answer": 2,
    "category": "SQL Standards",
    "explanation": "ANSI SQL enables compatibility and interoperability, making it easier to integrate with various database systems within the Lakehouse architecture.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Consider two tables in a database: Employees and Salaries. Given the output of a join between these tables, which of the queries below could have been used to generate the output?",
    "options": [
      "SELECT e.Name, s.Salary FROM Employees e JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e INNER JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e LEFT JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e RIGHT JOIN Salaries s ON e.EmployeeID = s.EmployeeID AND s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e LEFT JOIN Salaries s ON e.EmployeeID = s.EmployeeID AND s.Salary > 60000;"
    ],
    "answer": 1,
    "category": "SQL Joins",
    "explanation": "The INNER JOIN is used to match rows where EmployeeID is present in both tables, filtering with the condition WHERE s.Salary > 60000 to display employees with salaries over 60,000.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "You are working with a sales data table in Databricks SQL that contains columns for Region, Product, and SalesAmount. You want to generate a report that includes the total sales amount for each combination of Region and Product, as well as the total for each Region and the overall total. Which SQL query would you use to achieve this?",
    "options": [
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY Region, Product WITH CUBE;",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY CUBE(Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY Region, Product WITH ROLLUP;",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY GROUPING SETS ((Region, Product), (Region), ());",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY ROLLUP(Region, Product);"
    ],
    "answer": 4,
    "category": "SQL Aggregation",
    "explanation": "ROLLUP provides subtotals for each level of grouping, as well as the overall total, making it ideal for hierarchical summaries.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following best describes the purpose of descriptive statistics in data analysis?",
    "options": [
      "To summarize and describe the main features of a dataset.",
      "To establish causal relationships between different variables.",
      "To categorize data into distinct groups based on algorithmic models.",
      "To test hypotheses about the relationship between variables.",
      "To make predictions about future trends based on historical data."
    ],
    "answer": 0,
    "category": "Statistics",
    "explanation": "Descriptive statistics are used to summarize and describe the main features of a dataset, providing an overview of its characteristics.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What are the key differences in behavior between managed and unmanaged tables in Databricks?",
    "options": [
      "Managed tables support ACID transactions, while unmanaged tables do not.",
      "Managed tables automatically back up data, whereas unmanaged tables require manual backups.",
      "Managed tables can only store structured data, while unmanaged tables can store both structured and unstructured data.",
      "Managed tables store their data in a default location managed by Databricks, while unmanaged tables allow specifying a storage location.",
      "Unmanaged tables are optimized for streaming data, whereas managed tables are not."
    ],
    "answer": 3,
    "category": "Table Management",
    "explanation": "Managed tables are fully controlled by Databricks and stored in a default location, whereas unmanaged tables require a specified storage path.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "How do you set the location of a table in Databricks when creating or altering it?",
    "options": [
      "By using the CREATE TABLE... LOCATION or ALTER TABLE... SET LOCATION command.",
      "By using the SET LOCATION command in the table creation query.",
      "By moving the data files to the desired location manually.",
      "Location setting is not supported in Databricks.",
      "By specifying the location in the Databricks UI during table creation."
    ],
    "answer": 0,
    "category": "Table Management",
    "explanation": "The CREATE TABLE... LOCATION and ALTER TABLE... SET LOCATION commands allow users to specify or change the location of a table's data files.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In Databricks, a data analyst needs to share a dashboard with colleagues (with relevant access to the workspace), ensuring that the dashboard displays up-to-date results whenever it is accessed. Which method should the analyst use to share the dashboard for this purpose?",
    "options": [
      "Export the dashboard as a static PDF and email it to colleagues.",
      "Share a direct link to the dashboard that is hosted on the Databricks platform, allowing colleagues to view the latest results in real-time.",
      "Take screenshots of the dashboard and share them via a messaging platform.",
      "Print out the dashboard and distribute physical copies to colleagues.",
      "Save the dashboard as a static HTML file and share it via a file-sharing service."
    ],
    "answer": 1,
    "category": "Data Sharing",
    "explanation": "Sharing a direct link within Databricks allows colleagues to access the dashboard with real-time updates, ensuring data is current.",
    "sources": [
      "https://docs.databricks.com/en/sharing/index.html"
    ]
  },
  {
    "question": "Which of the following features is a primary function of the Catalog Explorer in Databricks?",
    "options": [
      "Previewing and exploring data, along with configuring security and access controls.",
      "Scheduling and automating data pipeline workflows.",
      "Generating real-time analytics and visualizations without the need for coding.",
      "Automatically transforming data into normalized forms for analysis.",
      "Executing complex machine learning algorithms on large datasets."
    ],
    "answer": 0,
    "category": "Catalog Explorer",
    "explanation": "The Catalog Explorer in Databricks primarily supports data preview and exploration, while also allowing for security and access configuration.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Consider the following two tables: Employees and Departments. Given the output from joining both tables, which of the queries below could have been used to generate the output?",
    "options": [
      "SELECT Employees.Name, Departments.Department FROM Employees FULL JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT Employees.Name, Departments.Department FROM Employees LEFT JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT * FROM Employees INNER JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT * FROM Employees LEFT JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;"
    ],
    "answer": 1,
    "category": "SQL Joins",
    "explanation": "A LEFT JOIN returns all records from the left table (Employees) and the matched records from the right table (Departments). Where there is no match, NULL values are returned for columns from the right table, which matches the output shown.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data engineering, how is 'performing last-mile ETL as project-specific data enhancement' best described?",
    "options": [
      "Conducting final data transformations and enrichments specific to the needs of a particular project.",
      "Performing initial data extraction from various source systems into a staging area.",
      "Implementing general ETL processes applicable to all datasets across the organization.",
      "Migrating all data to a centralized data warehouse for unified access.",
      "Utilizing advanced data analytics techniques to generate predictive insights."
    ],
    "answer": 0,
    "category": "ETL Processes",
    "explanation": "Last-mile ETL focuses on final, specific transformations or enhancements tailored to individual project requirements, adding value to data at the end of the pipeline.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In a Spark SQL dataset EmployeeData, you have a column monthlyPerformanceRatings which is an array of integers representing monthly performance ratings of employees. You are tasked with identifying employees whose performance has consistently improved over the last three months. Which Spark SQL query utilizing a higher-order function is best suited for this task?",
    "options": [
      "SELECT employeeId FROM EmployeeData WHERE ZIP_WITH(monthlyPerformanceRatings, monthlyPerformanceRatings, (current, next) -> next > current);",
      "SELECT employeeId FROM EmployeeData WHERE REDUCE(monthlyPerformanceRatings, 0, (acc, rating) -> acc + rating, acc -> acc) > 3;",
      "SELECT employeeId FROM EmployeeData WHERE ARRAY_SORT(monthlyPerformanceRatings) = monthlyPerformanceRatings;",
      "SELECT employeeId FROM EmployeeData WHERE EXISTS(monthlyPerformanceRatings, rating -> rating > 3);",
      "SELECT employeeId FROM EmployeeData WHERE SLICE(monthlyPerformanceRatings, -3, 3) = ARRAY_SORT(SLICE(monthlyPerformanceRatings, -3, 3));"
    ],
    "answer": 4,
    "category": "Spark SQL",
    "explanation": "The SLICE and ARRAY_SORT functions are used to check if the last three ratings are in ascending order, indicating improvement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-constraints.html"
    ]
  },
  {
    "question": "In Databricks SQL, which types of visualizations can be developed to represent data?",
    "options": [
      "Bar Chart, Line Graph, Heatmap, Gauge.",
      "Table, Chart, Sankey Diagram, Scatter Plot.",
      "Line Chart, Bubble Chart, Word Cloud, Counter.",
      "Table, Details, Counter, Pivot.",
      "Histogram, Radar Chart, Tree Map, Choropleth."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Common visualization options in Databricks SQL include bar charts, line graphs, heatmaps, and gauges for representing data insights.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In the landscape of Business Intelligence (BI) and data analytics, what role does Databricks SQL play when integrated with other BI tools?",
    "options": [
      "Databricks SQL primarily enhances data visualization capabilities.",
      "Databricks SQL and BI tools cannot be used together.",
      "Databricks SQL is used only for data storage, with BI tools handling all data processing and analysis.",
      "Databricks SQL acts as a data processing and query engine, complementing BI tools for enhanced data analysis and reporting.",
      "Databricks SQL replaces traditional BI tools for data analysis and reporting."
    ],
    "answer": 3,
    "category": "BI Integration",
    "explanation": "Databricks SQL serves as a processing engine that complements BI tools by handling data queries and processing, which BI tools can then use for visualization and reporting.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What is the minimum permission a user needs to configure a refresh schedule on a Databricks SQL Dashboard?",
    "options": [
      "Modify Permissions.",
      "Can Edit.",
      "Can View.",
      "No permissions.",
      "Owner."
    ],
    "answer": 1,
    "category": "Permissions",
    "explanation": "A user with 'Can Edit' permissions has sufficient access to configure a refresh schedule on a Databricks SQL Dashboard.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks SQL, when alerts are configured based on specific criteria, how are notifications typically sent to inform users or administrators of the triggered alerts?",
    "options": [
      "Notifications are not supported for alerts in Databricks SQL Analytics.",
      "Notifications are sent through SMS messages to designated phone numbers when alerts are triggered.",
      "Notifications are automatically sent to the dashboard’s viewers via email when alerts are triggered.",
      "Alerts generate a pop-up notification within the Databricks SQL Analytics interface, visible to all users.",
      "Alerts trigger notifications via a variety of channels, such as email, Slack, or webhook integrations, based on the defined configuration."
    ],
    "answer": 4,
    "category": "Databricks Alerts",
    "explanation": "Databricks SQL Analytics allows notifications to be configured across multiple channels, including email and webhook integrations.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In a Databricks environment, you're analyzing query performance improvements. After several runs of a complex query on a large dataset, you notice a significant reduction in latency. What feature of Databricks is most likely contributing to this decrease in query execution time?",
    "options": [
      "Use of persistent tables instead of temporary views.",
      "Increased hardware resources allocation.",
      "Automatic query rewriting for optimization.",
      "Caching of intermediate data and results from previous query executions.",
      "Improved data indexing mechanisms."
    ],
    "answer": 3,
    "category": "Query Optimization",
    "explanation": "Databricks caches intermediate data, reducing latency for repeated query executions on large datasets.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of Databricks, there are distinct types of parameters used in dashboards and visualizations. Based on the descriptions provided, how do Widget Parameters, Dashboard Parameters, and Static Values differ in their application and impact?",
    "options": [
      "Widget Parameters are tied to a single visualization and affect only the query underlying that specific visualization. Dashboard Parameters can influence multiple visualizations within a dashboard and are configured at the dashboard level. Static Values are used to replace parameters, making them 'disappear' and setting a fixed value in place.",
      "Dashboard Parameters are specific to individual visualizations and cannot be shared across multiple visualizations within a dashboard. Widget Parameters are used at the dashboard level to influence all visualizations. Static Values change dynamically in response to user interactions.",
      "Both Widget Parameters and Dashboard Parameters have the same functionality and impact, allowing for dynamic changes across all visualizations in a dashboard. Static Values provide temporary placeholders for these parameters.",
      "Static Values are used to create interactive elements in dashboards, while Widget and Dashboard Parameters are used for aesthetic modifications only, without impacting the data or queries.",
      "Widget Parameters apply to the entire dashboard and can change the layout, whereas Dashboard Parameters are fixed and do not allow for interactive changes. Static Values are dynamic and change frequently based on user input."
    ],
    "answer": 0,
    "category": "Dashboard Parameters",
    "explanation": "Widget Parameters affect single visualizations, Dashboard Parameters can impact multiple visualizations at once, and Static Values lock in specific parameter values without user input.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "How can you change access rights to a table in Databricks using Catalog Explorer?",
    "options": [
      "By editing the table schema in the 'Schema' section of Data Explorer.",
      "Access rights can only be changed via the Databricks CLI, not in Data Explorer.",
      "By executing a SQL command in Data Explorer for access modification.",
      "By selecting the table, navigating to the 'Permissions' tab, and modifying permissions.",
      "Through the 'Access Rights' menu in the table's context menu in Data Explorer."
    ],
    "answer": 3,
    "category": "Access Management",
    "explanation": "In Databricks, access rights can be modified by navigating to the 'Permissions' tab in the table settings within Catalog Explorer.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "How do Delta Lake tables in Databricks maintain their historical data?",
    "options": [
      "By storing historical data in a separate cloud storage.",
      "By maintaining a versioned history of data changes for a configurable period of time.",
      "Delta Lake tables do not maintain historical data.",
      "By creating a new table for each update.",
      "Through automatic backups at regular intervals."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake uses versioning to track historical changes, allowing data retrieval from different points in time.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Delta Lake stores table data as a series of data files, but it also stores additional information that is crucial for data management. Which of the following types of information is stored alongside data files when using Delta Lake?",
    "options": [
      "Schema history, performance statistics, and user activity logs",
      "Data backup files, table metadata, and user access permissions",
      "Table metadata, transaction logs, and schema history",
      "Visualization summaries, query execution plans, and performance statistics",
      "None of the above"
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Delta Lake retains table metadata, transaction logs, and schema history to support data versioning and ACID transactions, enabling data consistency and reliability.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Consider the following two tables, employees and departments. An analyst performed a SQL JOIN operation to create the following result table. Which type of JOIN was used to create this result table?",
    "options": [
      "LEFT",
      "CROSS",
      "INNER",
      "FULL OUTER",
      "RIGHT"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The LEFT JOIN operation was used because it includes all rows from the 'employees' table, even those without a matching department in the 'departments' table (e.g., 'David' with a NULL department). This behavior is characteristic of a LEFT JOIN.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary method for importing data from object storage into Databricks SQL?",
    "options": [
      "Utilizing the COPY INTO command to load data from object storage into a Databricks SQL table.",
      "Writing a custom Python script to handle the import of data from object storage into Databricks SQL.",
      "Using Databricks SQL to create a direct connection to the object storage without any intermediate steps.",
      "Manually downloading the data from object storage and uploading it into Databricks SQL.",
      "Configuring an external API to automatically sync data from object storage to Databricks SQL."
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "The COPY INTO command is specifically designed for efficiently importing data from object storage directly into Databricks SQL tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst is setting up a new Databricks SQL environment and wants to minimize the startup time for executing SQL queries. Which of the following options should the analyst choose to ensure quick and efficient query execution with minimal startup delays?",
    "options": [
      "Set up a single-node cluster with autoscaling.",
      "Use a Serverless Databricks SQL warehouses.",
      "Configure a Databricks job cluster.",
      "Deploy a standard Databricks SQL warehouses.",
      "Create a high-concurrency cluster."
    ],
    "answer": 1,
    "category": "Cluster Configuration",
    "explanation": "A Serverless Databricks SQL warehouses minimizes startup time by automatically managing resources and eliminating the need for manual cluster setup, ideal for SQL queries.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following best describes the benefits of using Delta Lake within a Lakehouse architecture?",
    "options": [
      "Delta Lake enhances the Lakehouse architecture by providing ACID transactions, scalable metadata handling, and unified batch and streaming data processing, thereby ensuring data consistency and reliability.",
      "Delta Lake enables the creation of highly interactive data visualizations and dashboards directly on top of raw data in the Lakehouse.",
      "Delta Lake facilitates real-time data analysis by automatically converting batch data into streaming data pipelines.",
      "Delta Lake reduces storage costs in a Lakehouse by compressing data files and eliminating the need for data redundancy.",
      "Delta Lake simplifies the integration of multiple data sources by providing built-in connectors for various databases and APIs."
    ],
    "answer": 0,
    "category": "Lakehouse Architecture",
    "explanation": "Delta Lake adds reliability and consistency to the Lakehouse by supporting ACID transactions and unified batch/streaming processing, essential for managing large-scale data environments.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "An analyst has a complex query that retrieves the top 3 products by sales from each region. Instead of writing a long, complicated query, the analyst decides to simplify the process using subqueries. Which of the following SQL queries correctly uses a subquery to achieve this?",
    "options": [
      "SELECT product_id, region, sales_amount FROM (SELECT product_id, region, sales_amount, RANK() OVER (PARTITION BY region ORDER BY sales_amount DESC) as sales_rank FROM sales) subquery WHERE subquery.sales_rank <= 3;",
      "SELECT product_id, region, sales_amount FROM sales WHERE sales_amount > (SELECT MAX(sales_amount) FROM sales WHERE region = 'West');",
      "SELECT product_id, region, sales_amount FROM sales WHERE region IN (SELECT DISTINCT region FROM sales);",
      "SELECT product_id, region, sales_amount FROM sales WHERE product_id = (SELECT product_id FROM sales WHERE region = 'North');",
      "SELECT product_id, region, sales_amount FROM sales WHERE sales_amount > ALL (SELECT sales_amount FROM sales WHERE region = 'East');"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The correct answer is the first option, which uses a subquery with the RANK() window function to partition the data by region and rank products by sales_amount in descending order. The main query then filters this subquery to retrieve only the top 3 ranked products for each region, fulfilling the requirement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is analyzing a dataset and wants to summarize the central tendency and spread of the data. They decide to calculate the mean, median, standard deviation, and range. Which of the following statements best compares and contrasts these key statistical measures?",
    "options": [
      "The mean is used to calculate the average distance between data points, while the range measures the spread of the data based on quartiles.",
      "The standard deviation measures the spread of the data, while the mean and median measure the central tendency, and the range provides the difference between the highest and lowest values.",
      "The range and standard deviation both measure the central tendency, and the mean and median measure how spread out the data is.",
      "The median and mean both measure the spread of the data, while the standard deviation gives the average value in the dataset.",
      "The mean is always the best measure of central tendency, regardless of outliers, while the median is used only when all data points are identical."
    ],
    "answer": 1,
    "category": "Statistics",
    "explanation": "The standard deviation quantifies data spread, mean and median indicate central tendency, and range shows the extent of values from highest to lowest.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is working within Databricks SQL and is exploring the Schema Browser from the Query Editor page. The analyst wants to understand what kind of information is displayed in the Schema Browser. Which of the following types of information can the analyst view in the Schema Browser on the Query Editor page?",
    "options": [
      "Information about running queries",
      "Details of SQL alerts",
      "Available schemas and their tables",
      "Active dashboard visualizations",
      "List of available catalogs"
    ],
    "answer": [
      2,
      4
    ],
    "category": "Schema Browser",
    "explanation": "The Schema Browser in Databricks SQL provides information on available schemas and their tables, as well as a list of catalogs, assisting users in database navigation.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "An administrator needs to change access rights to a table within Databricks using the Catalog Explorer interface. Which of the following steps should they follow to modify the permissions for a specific user or group?",
    "options": [
      "Open the Catalog Explorer, locate the table, click on the 'Permissions' tab, and adjust the access rights by adding or modifying roles for specific users or groups.",
      "Use a SQL command in a notebook to alter the permissions, as Catalog Explorer does not support changing access rights.",
      "Access the table’s storage location in DBFS and manually update the access control list (ACL) for the data files.",
      "Navigate to the Catalog tab, select the table, and directly edit the table’s metadata to change access rights.",
      "Create a new version of the table with different access rights and replace the existing table with this new version."
    ],
    "answer": 0,
    "category": "Access Control",
    "explanation": "The 'Permissions' tab within Catalog Explorer allows administrators to manage user and group access to specific tables directly.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst has set up a Databricks dashboard to auto-refresh periodically throughout the day. However, they have noticed that the associated SQL warehouse is running constantly, leading to unexpectedly high costs, even during periods of inactivity when the dashboard is not being actively viewed. What should the analyst check to reduce these costs?",
    "options": [
      "Reduce the dashboard refresh interval to lower the frequency of query execution.",
      "Check the SQL warehouse auto stop setting to ensure it shuts down after periods of inactivity when the dashboard is not being actively refreshed.",
      "Manually stop the SQL warehouse after each dashboard refresh.",
      "Disable the auto-refresh feature for the dashboard to prevent the SQL warehouse from running constantly.",
      "Increase the compute capacity of the SQL warehouse to handle periods of inactivity more efficiently."
    ],
    "answer": 1,
    "category": "SQL warehouse",
    "explanation": "Enabling the auto stop setting ensures that the SQL warehouse automatically shuts down during periods of inactivity, reducing costs by avoiding unnecessary resource usage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-constraints.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "A data analyst has created a dashboard in Databricks and needs to share it with stakeholders who do not have access to the workspace or the underlying compute resources. The analyst is considering whether to embed their credentials when sharing the dashboard. What is a key consideration when deciding whether or not to embed credentials?",
    "options": [
      "Embedding credentials disables real-time updates to the dashboard.",
      "Embedding credentials forces viewers to have individual compute resources for dashboard refreshes.",
      "Not embedding credentials will allow users without workspace access to still view the dashboard, as it relies on the analyst’s data access.",
      "Embedding credentials allows all viewers to use the same shared cache for maximum efficiency and access the dashboard, even if they don’t have access to the workspace.",
      "Not embedding credentials ensures the dashboard is always accessible, regardless of compute resource availability."
    ],
    "answer": 2,
    "category": "Access Control",
    "explanation": "Embedding credentials ensures all users have access to the dashboard efficiently, even without access to the underlying workspace or resources.",
    "sources": [
      "https://docs.databricks.com/en/sharing/index.html"
    ]
  },
  {
    "question": "A data analyst is working with two different datasets. One dataset contains the number of customers visiting a store each day, while the other contains the daily revenue generated by the store. What is the key difference between the types of statistics the analyst will use to analyze these datasets?",
    "options": [
      "Discrete data can only be visualized in bar charts, while continuous data can only be visualized in line charts.",
      "Discrete data involves percentages, while continuous data always involves ratios.",
      "Discrete data deals with whole numbers and categories, while continuous data involves data that can take any value within a range.",
      "Discrete data is used to measure time-related metrics, while continuous data measures frequency.",
      "Discrete data represents trends, while continuous data represents absolute values."
    ],
    "answer": 2,
    "category": "Data Types",
    "explanation": "Discrete data consists of countable values, such as customer counts, while continuous data can take any value within a range, like revenue.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When handling Personally Identifiable Information (PII) data within an organization, which of the following considerations is most crucial to ensure compliance and data protection?",
    "options": [
      "PII data should be stored in a separate database with a lower backup frequency to reduce storage costs.",
      "Access to PII data should be restricted based on user roles, with regular audits to ensure compliance with data protection regulations.",
      "PII data should be freely shared across departments to ensure that all business units have access to the same information for decision-making.",
      "PII data should only be stored on local servers to prevent exposure in cloud environments.",
      "PII data should be anonymized only when shared outside the organization, but not when used internally."
    ],
    "answer": 1,
    "category": "Data Compliance",
    "explanation": "Restricting access to PII data based on user roles and conducting regular audits is essential for compliance with data protection regulations and to ensure security.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "An analyst needs to apply a custom scaling transformation to the 'revenue' column in a SQL-based Spark environment. The scaling factor is 2.5. The analyst decides to create a User Defined Function (UDF) to accomplish this task. Given the following table 'sales_data':",
    "options": [
      "-- Step 1: Define the UDF CREATE FUNCTION scale_revenue(revenue FLOAT) RETURNS FLOAT RETURN revenue * 2.5; -- Step 2: Apply the UDF SELECT product_id, scale_revenue(revenue) AS scaled_revenue FROM sales_data;",
      "-- Step 1: Define the UDF CREATE FUNCTION scale_revenue(revenue DOUBLE) RETURNS DOUBLE RETURN revenue * 2.5; -- Step 2: Apply the UDF SELECT product_id, scale_revenue(revenue) AS scaled_revenue FROM sales_data;",
      "-- Step 1: Define the UDF CREATE FUNCTION scale_revenue(revenue INT) RETURNS INT RETURN revenue * 2.5; -- Step 2: Apply the UDF SELECT product_id, scale_revenue(revenue) AS scaled_revenue FROM sales_data;",
      "-- Step 1: Define the UDF CREATE FUNCTION scale_revenue(revenue DOUBLE) RETURNS DOUBLE RETURN revenue * 2.5; -- Step 2: Apply the UDF SELECT product_id, scale_revenue(revenue) AS scaled_revenue FROM sales_data WHERE revenue > 1000;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "The correct data type for the UDF's return value is DOUBLE because it can handle decimal values accurately, which is important for applying a scaling factor like 2.5 to the revenue. Additionally, the function is correctly applied to the 'revenue' column in the SELECT statement without any unnecessary filters.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is publishing a dashboard in Databricks and is deciding whether to embed their credentials or not. If the analyst chooses to embed their credentials, which of the following is a consequence?",
    "options": [
      "Users will need to have direct access to the workspace, underlying data, and SQL warehouse to view the dashboard.",
      "All viewers of the dashboard can run queries using the analyst’s credentials, even if they don’t have access to the underlying data or SQL warehouse.",
      "The dashboard will automatically be versioned, and users can revert to previous published versions.",
      "Viewers will need to run queries using their own credentials to access the dashboard data and compute resources.",
      "The published dashboard will be emailed to subscribers with limited access to the underlying data."
    ],
    "answer": 1,
    "category": "Access Control",
    "explanation": "Embedding credentials allows all viewers to access data and run queries using the analyst's permissions, simplifying access but raising security considerations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, the persistence and scope of a table depend on how it is created and managed. Which of the following statements accurately describe the persistence and scope of different types of tables?",
    "options": [
      "Unmanaged tables are automatically deleted when the session ends, while managed tables and temporary tables persist across sessions.",
      "Managed tables persist only within the session and store both data and metadata in the DBFS, while unmanaged tables store both data and metadata permanently in DBFS.",
      "Temporary tables store data in the cloud permanently but restrict access to the current session.",
      "Managed tables are stored temporarily in memory and are deleted when the session ends, while unmanaged tables store data permanently in DBFS.",
      "Temporary tables are session-scoped and are automatically deleted when the session ends, while managed and unmanaged tables persist across sessions."
    ],
    "answer": 4,
    "category": "Databricks Tables",
    "explanation": "Temporary tables are session-scoped and only persist during the session, while managed and unmanaged tables can persist beyond the session.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "An analyst has a table 'employee_salaries' with the following structure: | department | salary | |------------|--------| | HR | 50000 | | IT | 75000 | | HR | 60000 | | IT | 70000 | | Finance | 80000 | | Finance | 85000 | The analyst wants to create a new table 'salary_ranks' that ranks each 'salary' within its 'department' using the PERCENT_RANK window function. The result should look like this: | department | salary | percent_rank | |------------|--------|--------------| | Finance | 80000 | 0.0000 | | Finance | 85000 | 1.0000 | | HR | 50000 | 0.0000 | | HR | 60000 | 1.0000 | | IT | 70000 | 0.0000 | | IT | 75000 | 1.0000 | Which of the following SQL queries will produce the desired output?",
    "options": [
      "SELECT department, salary, PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary ASC) AS percent_rank FROM employee_salaries;",
      "SELECT department, salary, PERCENT_RANK() OVER (PARTITION BY salary ORDER BY department) AS percent_rank FROM employee_salaries;",
      "SELECT department, salary, PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS percent_rank FROM employee_salaries;",
      "SELECT department, salary, PERCENT_RANK() OVER (ORDER BY salary) AS percent_rank FROM employee_salaries;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The correct option uses PERCENT_RANK with PARTITION BY department and ORDER BY salary ASC, which ranks salaries within each department as required. This ensures each department's salaries are ranked independently.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is required to set up a partner for use with Partner Connect in Databricks?",
    "options": [
      "An active Databricks workspace with admin-level permissions.",
      "A configured SQL warehouse with the CAN CREATE permission.",
      "A direct API connection to the partner’s platform.",
      "A pre-configured cluster specifically set up for partner integration.",
      "The partner application must be available in the Databricks Partner Connect interface."
    ],
    "answer": 4,
    "category": "Partner Integration",
    "explanation": "For Partner Connect integration, the partner application must be accessible via Databricks' Partner Connect interface, facilitating setup and connection.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "An analyst is tasked with loading and updating data in a Delta Lake table called 'customer_data'. They are considering using MERGE INTO, INSERT INTO, or COPY INTO based on different requirements. Given the scenarios below, which option correctly matches the statement with its most appropriate use case? Scenarios: 1. Scenario 1: The analyst needs to update existing records and insert new records into 'customer_data' from a staging table, ensuring that duplicates are handled based on a matching condition. 2. Scenario 2: The analyst has a set of new data files that need to be bulk-loaded into 'customer_data', appending the data without modifying any existing records. 3. Scenario 3: The analyst needs to append new records from another table into 'customer_data', with the source table already matching the structure of the target table.",
    "options": [
      "INSERT INTO for Scenario 1, MERGE INTO for Scenario 2, COPY INTO for Scenario 3",
      "INSERT INTO for Scenario 1, COPY INTO for Scenario 2, MERGE INTO for Scenario 3",
      "COPY INTO for Scenario 1, INSERT INTO for Scenario 2, MERGE INTO for Scenario 3",
      "MERGE INTO for Scenario 1, INSERT INTO for Scenario 2, COPY INTO for Scenario 3"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "MERGE INTO is appropriate for Scenario 1 as it allows for upsert operations where existing records can be updated, and new records inserted based on conditions. INSERT INTO is used in Scenario 2 for simply appending data without modifying existing records. COPY INTO is suitable for Scenario 3 when appending new data that already matches the table schema.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "A data analyst has been asked to visualize the energy flow between different sources and consumers in a power grid. Which of the following visualization types is most appropriate for representing this type of flow?",
    "options": [
      "Heatmap",
      "Sankey",
      "Pivot Table",
      "Choropleth",
      "Word Cloud"
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "A Sankey diagram is ideal for visualizing flow between sources and destinations, showing the magnitude of flow between them, which is suitable for energy flow representation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A new data analyst has joined your team that uses Databricks SQL, but they are unfamiliar with the platform. The analyst wants to know where they can write and execute SQL queries within Databricks SQL. On which of the following pages can the analyst write and execute SQL queries?",
    "options": [
      "Data page",
      "Alerts page",
      "Dashboards page",
      "SQL Editor page",
      "Queries page"
    ],
    "answer": 3,
    "category": "Databricks SQL Interface",
    "explanation": "The SQL Editor page in Databricks SQL is specifically designed for writing and executing SQL queries, providing the necessary interface for query development.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following statements correctly identifies a key feature of Delta Lake tables regarding data history?",
    "options": [
      "Delta Lake tables only maintain history for the last transaction, discarding any earlier versions of the data.",
      "Delta Lake tables automatically delete all historical data once a new version of the table is created.",
      "Delta Lake tables maintain a history of changes, allowing users to access and query previous versions of the data for a configurable period of time.",
      "Delta Lake tables store historical data indefinitely, without any option to configure the retention period.",
      "Delta Lake tables do not maintain any historical versions of the data once the table is updated."
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Delta Lake provides time travel functionality, enabling users to query historical versions of data within a configurable retention period.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the capability of a Lakehouse architecture, particularly within Databricks, regarding data processing workloads?",
    "options": [
      "It allows for the seamless integration of batch and streaming workloads within a unified platform.",
      "It exclusively supports real-time streaming workloads, designed for continuous data ingestion and analysis.",
      "It requires manual synchronization between batch and streaming processes, with limited automation.",
      "It separates batch and streaming workloads, requiring dedicated environments for each type of processing.",
      "It restricts data processing to batch workloads only, optimizing for large-scale batch processing."
    ],
    "answer": 0,
    "category": "Lakehouse Architecture",
    "explanation": "Lakehouse architecture unifies batch and streaming data processing, enabling seamless integration and efficient handling of various data workloads.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst has created a dashboard in Databricks to showcase key business metrics, but the stakeholders find it difficult to navigate because the dashboard lacks structure and visual clarity. What is the best approach to enhance the dashboard's formatting?",
    "options": [
      "Embed the explanations and instructions within the axis labels of the charts.",
      "Add markdown text to create well-structured headings, titles, and explanatory notes for the dashboard.",
      "Modify the SQL queries to include formatted text outputs for better presentation.",
      "Change the font size of the visualization labels to increase readability.",
      "Use only color schemes in visualizations to differentiate sections of the dashboard."
    ],
    "answer": 1,
    "category": "Dashboard Design",
    "explanation": "Adding markdown text allows for clear structure through headings, titles, and explanatory notes, improving user navigation and understanding.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the purpose of the gold layer in the Databricks Lakehouse architecture?",
    "options": [
      "It is used exclusively for storing metadata related to datasets and table structures.",
      "It is used for storing raw, unprocessed data as it is ingested from various sources.",
      "It serves as a backup storage layer for historical data, ensuring data integrity and availability.",
      "It is the layer where data is prepared and transformed, typically used for staging before further processing.",
      "It is optimized for quick, scalable querying and analytics, where data is cleaned, aggregated, and ready for reporting."
    ],
    "answer": 4,
    "category": "Lakehouse Architecture",
    "explanation": "The gold layer in a Lakehouse architecture contains highly curated, processed data, ready for analytics and reporting, focusing on efficiency and accessibility.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst wants to visualize the results of several SQL queries simultaneously in Databricks SQL. Which feature should the analyst use to achieve this?",
    "options": [
      "Query History",
      "Alerts",
      "Data Explorer",
      "Dashboards",
      "SQL Editor"
    ],
    "answer": 3,
    "category": "Data Visualization",
    "explanation": "Dashboards enable the simultaneous display of multiple query results, allowing analysts to view and compare data insights side-by-side.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "A data analytics team is utilizing gold-level tables from a Delta Live Tables pipeline built on the medallion architecture. Before completing their analysis, the team needs to perform final transformations and data processing on the gold tables. What is the term used to describe this type of work?",
    "options": [
      "Data enhancement",
      "Data testing",
      "Data blending",
      "Last-mile ETL",
      "Last-mile dashboarding"
    ],
    "answer": 3,
    "category": "Data Processing",
    "explanation": "Last-mile ETL refers to final processing steps that prepare data for reporting or analysis, ensuring it is fully refined and ready for end-use.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "A data analyst has been asked to calculate the total sales amount for each product category and has written the following query: SELECT category, SUM(sales_amount) FROM product_sales ORDER BY category; If there is a mistake in the query, which of the following describes the mistake?",
    "options": [
      "There are no mistakes in the query.",
      "The query is using SUM(sales_amount), which should be COUNT(sales_amount) to aggregate the data.",
      "The query is selecting category, but category should only occur in the ORDER BY clause.",
      "The query is missing a GROUP BY category clause.",
      "The query is using ORDER BY, which is not allowed in an aggregation."
    ],
    "answer": 3,
    "category": "SQL Query",
    "explanation": "To calculate totals per category, a GROUP BY clause is necessary to group the data by each category before applying SUM.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "An analyst needs to calculate the running total of sales over time for each product in the 'sales_data' table. To calculate the running total (cumulative_sales) for each product_id, ordered by sale_date, the analyst should use the following SQL query: ```sql SELECT product_id, sale_date, sales_amount, SUM(sales_amount) OVER (_____ PARTITION BY product_id ORDER BY sale_date) AS cumulative_sales FROM sales_data; ``` Which function should be used to fill in the blank?",
    "options": [
      "ORDER",
      "WINDOW",
      "RANGE",
      "GROUP",
      "ROWS"
    ],
    "answer": 4,
    "category": "SQL",
    "explanation": "The correct function to use here is 'ROWS'. In SQL, when calculating a cumulative total in a window function, 'ROWS' specifies that the sum should include all previous rows in the defined partition up to the current row. This is ideal for cumulative calculations that progress row-by-row within the partitioned data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst creates two objects: a view and a temporary view. They use the following commands: CREATE VIEW vw_east_sales AS SELECT * FROM sales_data WHERE region = 'East'; CREATE TEMP VIEW vw_west_sales AS SELECT * FROM sales_data WHERE region = 'West'; The analyst logs out and logs back in. They then attempt to run queries against both vw_east_sales and vw_west_sales: SELECT * FROM vw_east_sales; SELECT * FROM vw_west_sales; What will happen when these queries are executed?",
    "options": [
      "The query on vw_west_sales will run successfully, but the query on vw_east_sales will fail because the view has expired.",
      "Both queries will fail because views are not persistent across sessions.",
      "The query on vw_east_sales will run successfully, but the query on vw_west_sales will fail because the temporary view no longer exists.",
      "The queries will return the same results since both views are stored in memory.",
      "Both queries will run successfully, returning the expected results."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Temporary views in SQL are session-scoped, meaning they do not persist beyond the session in which they are created. When the analyst logs out, the temporary view vw_west_sales is dropped. Therefore, the query on vw_west_sales will fail as the temporary view no longer exists, while vw_east_sales will run successfully as it is a persistent view.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "A data analyst is working with a customer transaction dataset that includes basic information such as customer ID, purchase amount, and transaction date. The marketing team wants to run more targeted campaigns based on customer demographics and purchasing behavior patterns. In which of the following scenarios would data enhancement be most beneficial?",
    "options": [
      "To convert the purchase amount into a different currency for international reports.",
      "To remove duplicate customer IDs from the dataset to improve data quality.",
      "To reduce the size of the dataset by aggregating transaction data on a monthly basis.",
      "To add additional columns with customer age, income level, and location to create more personalized marketing campaigns.",
      "To split the transaction date into separate columns for day, month, and year."
    ],
    "answer": 3,
    "category": "Data Enhancement",
    "explanation": "Adding demographic columns allows for the creation of more targeted and personalized marketing campaigns based on customer segmentation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "An analyst is working in Databricks and frequently runs similar queries on a large dataset while developing a new report. To OPTIMIZE their workflow and reduce query latency, they want to leverage query history and caching. Which of the following strategies would best achieve this goal?",
    "options": [
      "Rely on query history to copy and paste past queries into new notebooks, ensuring consistent query structure without the need for caching.",
      "Enable result caching to store the results of frequently run queries in memory, reducing the need to re-execute the underlying computations.",
      "Use query history to export past query results and manually import them into new sessions to avoid rerunning queries.",
      "Review query history to identify frequently run queries and set up automatic execution to pre-load results into memory.",
      "Disable caching to force every query to run from scratch, ensuring data freshness and accuracy in development."
    ],
    "answer": 1,
    "category": "Query Optimization",
    "explanation": "Enabling result caching reduces latency by storing results in memory, which is optimal for repetitive query executions.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "A data engineering team has built a Structured Streaming pipeline that processes data in micro-batches and updates gold-level tables every minute. A data analyst has created a dashboard based on this gold-level data. Stakeholders want the dashboard to reflect new data within one minute of it being available in the gold-level tables. Which caution should the data analyst raise before proceeding with this request?",
    "options": [
      "The dashboard cannot be refreshed that quickly.",
      "The required compute resources could be costly.",
      "The streaming data is not an appropriate data source for a dashboard.",
      "The streaming cluster is not fault tolerant.",
      "The gold-level tables are not appropriately clean for business reporting."
    ],
    "answer": 1,
    "category": "Structured Streaming",
    "explanation": "Frequent refreshes on a dashboard can be resource-intensive and costly, especially when working with real-time or near real-time data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is a key benefit of having ANSI SQL as the standard query language in a Lakehouse architecture?",
    "options": [
      "It provides a more secure environment by limiting access to certain SQL functions.",
      "It reduces the ability to perform real-time analytics in the Lakehouse.",
      "It forces the use of proprietary SQL extensions, making the system more vendor-dependent.",
      "It restricts the use of complex queries, ensuring that only basic operations can be performed.",
      "It ensures compatibility with legacy databases and allows analysts to use familiar SQL syntax across different platforms."
    ],
    "answer": 4,
    "category": "SQL Compatibility",
    "explanation": "ANSI SQL compatibility allows for greater flexibility and interoperability, making it easier for analysts to use existing SQL knowledge across multiple platforms.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst has added a parameter to a Databricks SQL dashboard that allows users to select a specific region when viewing the sales data. The parameter is configured to filter the data based on the region selected by the user. What is the behavior of this dashboard parameter?",
    "options": [
      "The parameter disables certain dashboard features when a region is selected.",
      "The parameter caches the results for each region to speed up query performance.",
      "The parameter updates the dashboard data in real-time based on the user’s input, filtering results according to the selected region.",
      "The parameter only changes the visualization’s appearance but doesn’t affect the data displayed.",
      "The parameter creates multiple dashboards for each region and switches between them based on the user’s selection."
    ],
    "answer": 2,
    "category": "Dashboard Parameters",
    "explanation": "The parameter dynamically filters the dashboard data in real-time according to the user-selected region, allowing for interactive and customizable data views.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "What is one advantage of Databricks SQL utilizing ANSI SQL as its standard SQL dialect?",
    "options": [
      "It simplifies the migration of existing SQL queries to Databricks SQL.",
      "It enables the use of Photon’s computation optimizations.",
      "It provides better performance compared to other SQL dialects.",
      "It offers greater customization options.",
      "It ensures better compatibility with Spark’s interpreters."
    ],
    "answer": 0,
    "category": "SQL Compatibility",
    "explanation": "Using ANSI SQL as the standard dialect simplifies the migration process for queries from other SQL environments to Databricks, ensuring compatibility and reducing the need for query rewrites.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "An analyst is tasked with loading and updating data in a Delta Lake table called customer_data. They are considering using MERGE INTO, INSERT INTO, or COPY INTO based on different requirements. Given the scenarios below, which option correctly matches the statement with its most appropriate use case? Scenarios: 1. Scenario 1: The analyst needs to update existing records and insert new records into customer_data from a staging table, ensuring that duplicates are handled based on a matching condition. 2. Scenario 2: The analyst has a set of new data files that need to be bulk-loaded into customer_data, appending the data without modifying any existing records. 3. Scenario 3: The analyst needs to append new records from another table into customer_data, with the source table already matching the structure of the target table.",
    "options": [
      "INSERT INTO for Scenario 1, MERGE INTO for Scenario 2, COPY INTO for Scenario 3",
      "INSERT INTO for Scenario 1, COPY INTO for Scenario 2, MERGE INTO for Scenario 3",
      "COPY INTO for Scenario 1, INSERT INTO for Scenario 2, MERGE INTO for Scenario 3",
      "MERGE INTO for Scenario 1, INSERT INTO for Scenario 2, COPY INTO for Scenario 3",
      "MERGE INTO for Scenario 1, COPY INTO for Scenario 2, INSERT INTO for Scenario 3"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "For Scenario 1, MERGE INTO is used as it allows both updating existing records and inserting new ones based on a condition. Scenario 2 requires COPY INTO, which is suitable for bulk-loading data into a Delta table without modifying existing records. Scenario 3 is best served by INSERT INTO, which appends new records from another table with a matching structure.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/delta-copy-into.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "When creating a database in Databricks, how does the LOCATION keyword affect the default location of the database contents?",
    "options": [
      "The LOCATION keyword is used to specify a temporary storage location for the database contents, which is cleared when the session ends.",
      "The LOCATION keyword is used to specify an alternative location for storing the database’s metadata only, while the actual data files are stored in the default location.",
      "The LOCATION keyword specifies where the database logs are stored, but does not impact where the data files are stored.",
      "The LOCATION keyword changes the storage location for both the database’s metadata and data files to the specified path, overriding the default storage location.",
      "The LOCATION keyword only affects the storage of table indexes, while the actual data files remain in the default location."
    ],
    "answer": 3,
    "category": "Database Management",
    "explanation": "The LOCATION keyword allows users to define a custom path for storing both metadata and data files, providing flexibility in data organization and storage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "An analyst is working with a large dataset containing nested arrays in Spark SQL. To improve performance when processing these arrays, the analyst wants to apply a function to each element of an array and return a transformed array. Which of the following higher-order Spark SQL functions should the analyst use to OPTIMIZE performance?",
    "options": [
      "MAP() to transform the values of a map column, optimizing key-value pair processing.",
      "ARRAY_CONTAINS() to check if an array contains a specific value, optimizing filtering operations.",
      "TRANSFORM() to apply a function to each element of an array, returning a new array with the transformed elements.",
      "AGGREGATE() to reduce the elements of an array into a single value, applying a binary function.",
      "EXPLODE() to flatten the array into individual rows for further processing."
    ],
    "answer": 2,
    "category": "Spark SQL Functions",
    "explanation": "TRANSFORM() is specifically designed to apply a transformation function to each element of an array, making it optimal for element-wise array transformations.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "When connecting Databricks to Fivetran using Partner Connect for data managed by Unity Catalog, several permissions and privileges are required to ensure proper integration and data ingestion. Which of the following is not one of those required permissions?",
    "options": [
      "USE CATALOG privilege on the catalog managed by Unity Catalog.",
      "CREATE EXTERNAL TABLE privilege on the external location.",
      "CAN USE permission for a SQL warehouse.",
      "CREATE SCHEMA privilege on the catalog managed by Unity Catalog.",
      "CAN USE permission for token usage."
    ],
    "answer": 4,
    "category": "Permissions and Access Control",
    "explanation": "The CAN USE permission for token usage is not typically required for integrating Databricks with Fivetran via Partner Connect; the other permissions ensure appropriate access to data and resources.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "A data analyst is tasked with combining data from two different source applications: a customer relationship management (CRM) system and an e-commerce platform. The goal is to create a unified view of customer transactions, where the CRM provides customer details and the e-commerce platform provides purchase history. What is the term used to describe this process of combining and integrating data from these two sources?",
    "options": [
      "Data migration",
      "Partitioning",
      "Data blending",
      "Data archiving",
      "Data deduplication"
    ],
    "answer": 2,
    "category": "Data Integration",
    "explanation": "Data blending involves combining data from multiple sources to create a unified view, which is essential when integrating CRM and e-commerce data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "An analyst needs to flatten a nested array structure within the order_items column of the orders table, so that each item appears as a separate row. Which function should be used to complete the following SQL query?",
    "options": [
      "group_by",
      "json_tuple",
      "explode",
      "cast",
      "from_json"
    ],
    "answer": 2,
    "category": "Data Transformation",
    "explanation": "The explode function is used to flatten nested array structures, creating individual rows for each element within an array.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is creating a new visualization in Databricks SQL for a dashboard to show total sales by product category. After running the SQL query to retrieve the sales data, the analyst quickly wants to visualize the results without manually selecting a chart type. Which type of visualization will be automatically selected as the default?",
    "options": [
      "Pivot table",
      "Pie chart",
      "Table",
      "Bar chart",
      "Line chart"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "By default, Databricks often selects the 'Table' view for a quick representation of SQL query results, as it allows for easy inspection of data.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "What is the purpose of using 'Z-Ordering' in Databricks Delta Lake tables?",
    "options": [
      "To compress data files for storage efficiency.",
      "To OPTIMIZE data skipping by colocating related information.",
      "To encrypt data at rest for security.",
      "To enable real-time data streaming capabilities.",
      "To partition data based on a hash of a column."
    ],
    "answer": 1,
    "category": "Delta Lake Optimization",
    "explanation": "Option B is correct because Z-Ordering improves data skipping during query execution by organizing data files based on the values of specified columns. Option A is incorrect; Z-Ordering is not for compression. Option C is incorrect; encryption is handled differently. Option D is incorrect; streaming is not enabled by Z-Ordering. Option E is incorrect; Z-Ordering is not about partitioning by hash.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Jobs' feature used for?",
    "options": [
      "To manage clusters.",
      "To schedule and run automated tasks.",
      "To create and edit notebooks.",
      "To set up data storage options.",
      "To configure user permissions."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because the Jobs feature allows you to schedule and run automated tasks, including notebooks and workflows. Options A, C, D, and E describe other features.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In data visualization, what is a 'treemap' used for?",
    "options": [
      "Displaying hierarchical data as nested rectangles.",
      "Showing trends over time.",
      "Comparing individual data points.",
      "Displaying the frequency distribution of a dataset.",
      "Representing data using a geographical map."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a treemap visualizes hierarchical data using nested rectangles, where the size and color represent different attributes.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'table' in the context of the Data Catalog?",
    "options": [
      "A collection of notebooks.",
      "A structured data set stored in a database.",
      "A cluster configuration.",
      "A user permission setting.",
      "An external data source."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because a table in the Data Catalog represents a structured data set stored within a database in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which command is used to display the list of columns and their data types for a specific table named 'customer_data'?",
    "options": [
      "SHOW COLUMNS FROM customer_data;",
      "DESCRIBE TABLE customer_data;",
      "LIST COLUMNS IN customer_data;",
      "SELECT * FROM customer_data;",
      "DISPLAY SCHEMA OF customer_data;"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because the `DESCRIBE TABLE customer_data;` command provides the schema information, listing all columns and their data types. Option A is incorrect because `SHOW COLUMNS FROM` may not display data types in all SQL dialects. Option C is incorrect because `LIST COLUMNS IN` is not standard SQL syntax. Option D is incorrect because `SELECT *` retrieves data, not schema information. Option E is incorrect because `DISPLAY SCHEMA OF` is not a valid SQL command.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "What is the primary purpose of the Databricks workspace in the Lakehouse architecture?",
    "options": [
      "To store raw data in its original format.",
      "To provide a collaborative environment for data engineers, data scientists, and analysts to develop and manage data and AI applications.",
      "To serve as a data visualization tool for creating dashboards.",
      "To act as a data warehouse for structured data storage.",
      "To manage user authentication and access control only."
    ],
    "answer": 1,
    "category": "Databricks Architecture",
    "explanation": "Option B is correct because the Databricks workspace is designed to be a collaborative environment where different roles can work together on data engineering, data science, and analytics tasks. Option A is incorrect because raw data storage is typically handled by the data lake storage layer. Option C is incorrect because while Databricks does offer visualization capabilities, the primary purpose of the workspace is broader. Option D is incorrect because the data warehouse functionality is part of the Lakehouse but not the primary purpose of the workspace. Option E is incorrect because while access control is part of the workspace, it is not its sole purpose.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main advantage of using 'broadcast join' in Spark?",
    "options": [
      "It allows joining large DataFrames efficiently.",
      "It sends a copy of a small DataFrame to all executor nodes to OPTIMIZE join performance.",
      "It automatically repartitions data before joining.",
      "It caches data in memory to speed up processing.",
      "It avoids the need for shuffling data across the network."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because a broadcast join sends a small DataFrame to all executor nodes, allowing each node to join with the local partition of the large DataFrame without shuffling the large DataFrame. Option A is partially correct but not specific. Option C is incorrect. Option D is about caching. Option E is incorrect; shuffling is still involved but minimized.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which API allows you to interact with object storage directly from a notebook?",
    "options": [
      "dbutils.fs",
      "dbutils.secrets",
      "dbutils.widgets",
      "dbutils.jobs",
      "dbutils.data"
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.fs` provides filesystem commands to interact with object storage directly from a Databricks notebook. Options B, C, D, and E are incorrect; they serve different purposes within Databricks.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In data processing, what is 'schema evolution'?",
    "options": [
      "The process of encrypting data schemas for security.",
      "Automatically adapting to changes in data structure over time without breaking applications.",
      "Normalizing data to eliminate redundancy.",
      "Transforming data from one format to another.",
      "Version controlling code changes in database schemas."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because schema evolution refers to the capability of data systems to handle changes in the data schema over time.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'data locality' in the context of distributed computing?",
    "options": [
      "Storing all data in a central location.",
      "Processing data on or near the node where it is stored to reduce network traffic.",
      "Encrypting data to secure it during processing.",
      "Distributing data evenly across all nodes regardless of location.",
      "Backing up data to multiple geographical locations."
    ],
    "answer": 1,
    "category": "Distributed Systems",
    "explanation": "Option B is correct because data locality refers to processing data close to where it is stored to minimize data movement and network congestion. Options A, C, D, and E are different concepts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is the 'Catalyst Optimizer'?",
    "options": [
      "A tool for managing cluster resources.",
      "A query optimization engine in Spark SQL.",
      "A library for machine learning algorithms.",
      "A function for data serialization.",
      "An interface for streaming data processing."
    ],
    "answer": 1,
    "category": "Spark SQL",
    "explanation": "Option B is correct because the Catalyst Optimizer is Spark SQL's query optimization engine that automatically optimizes queries for better performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data analysis, what is 'data wrangling'?",
    "options": [
      "The process of collecting raw data.",
      "The initial exploration of data to discover patterns.",
      "The process of cleaning and unifying complex data sets for easy access and analysis.",
      "The deployment of machine learning models.",
      "The process of storing data securely."
    ],
    "answer": 2,
    "category": "Data Preprocessing",
    "explanation": "Option C is correct because data wrangling involves cleaning, transforming, and mapping raw data into a more usable format for analysis.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method is used to write a DataFrame 'df' to a table named 'users' in a database?",
    "options": [
      "df.write.saveAsTable('users')",
      "df.saveTable('users')",
      "df.write.table('users')",
      "df.toTable('users')",
      "df.write.save('users')"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because `df.write.saveAsTable('users')` writes the DataFrame to a table named 'users'.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which function calculates the number of items in a group, ignoring NULL values?",
    "options": [
      "SUM()",
      "COUNT(*)",
      "COUNT(column_name)",
      "AVG()",
      "MAX()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because 'COUNT(column_name)' counts the number of non-NULL values in that column. Option B counts all rows, including those with NULLs.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'data democratization' in the context of data analytics?",
    "options": [
      "Restricting data access to top management.",
      "Making data accessible to all users in an organization without barriers.",
      "Encrypting data to prevent unauthorized access.",
      "Centralizing data storage in a single location.",
      "Deleting redundant data to save storage space."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data democratization refers to making data accessible to all users within an organization, enabling them to make data-driven decisions without barriers. Options A, C, D, and E do not define data democratization.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which function returns the smallest value in a specified column?",
    "options": [
      "MAX()",
      "SUM()",
      "MIN()",
      "AVG()",
      "COUNT()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because `MIN()` returns the smallest value in a specified column. Option A returns the largest value. Option B calculates the total. Option D calculates the average. Option E counts the number of rows.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method is used to remove duplicates based on specific columns 'col1' and 'col2' in DataFrame 'df'?",
    "options": [
      "df.dropDuplicates(['col1', 'col2'])",
      "df.removeDuplicates(['col1', 'col2'])",
      "df.deleteDuplicates(['col1', 'col2'])",
      "df.distinct(['col1', 'col2'])",
      "df.drop(['col1', 'col2'])"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df.dropDuplicates(['col1', 'col2'])' removes duplicate rows based on the specified columns.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you rename a column 'old_name' to 'new_name' in DataFrame 'df'?",
    "options": [
      "df.withColumnRenamed('old_name', 'new_name')",
      "df.renameColumn('old_name', 'new_name')",
      "df.column('old_name').alias('new_name')",
      "df.rename('old_name', 'new_name')",
      "df.withColumn('new_name', df['old_name'])"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `withColumnRenamed('old_name', 'new_name')` renames a column. Option B is not a valid method. Option C aliases the column in a select statement. Option D is not valid. Option E adds a new column but doesn't remove the old one.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the difference between 'INNER JOIN' and 'FULL OUTER JOIN'?",
    "options": [
      "'INNER JOIN' returns matched records; 'FULL OUTER JOIN' returns all records when there is a match in either left or right table.",
      "'INNER JOIN' returns all records from both tables; 'FULL OUTER JOIN' returns only matched records.",
      "'INNER JOIN' returns unmatched records; 'FULL OUTER JOIN' returns matched records only.",
      "There is no difference between 'INNER JOIN' and 'FULL OUTER JOIN'.",
      "'INNER JOIN' returns all records from the left table; 'FULL OUTER JOIN' returns all records from the right table."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because 'INNER JOIN' returns records that have matching values in both tables, while 'FULL OUTER JOIN' returns all records when there is a match in either left or right table, filling with NULLs where there is no match.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'data normalization' in data preprocessing?",
    "options": [
      "Converting categorical data into numerical form.",
      "Scaling numerical data to a common range without distorting differences in the ranges of values.",
      "Detecting and correcting errors in data.",
      "Organizing data into a database according to certain rules.",
      "Removing duplicate records from data."
    ],
    "answer": 1,
    "category": "Data Preprocessing",
    "explanation": "Option B is correct because data normalization scales numeric data to a common range, often between 0 and 1, which is important for certain algorithms.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'k-fold cross-validation' used for?",
    "options": [
      "To divide the dataset into k subsets and use one as validation while training on the others, rotating this process k times.",
      "To increase the size of the training dataset by a factor of k.",
      "To cluster data into k groups before training.",
      "To reduce the dimensionality of data to k features.",
      "To select the top k models based on performance."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because k-fold cross-validation involves partitioning the data into k subsets, training the model k times, each time using a different subset as the validation set.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'notebook'?",
    "options": [
      "A document that contains executable code, visualizations, and narrative text.",
      "A cluster configuration file.",
      "A dataset stored in the cloud.",
      "A log file for monitoring applications.",
      "A user profile setting."
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "Option A is correct because a notebook in Databricks is an interactive document where users can write code, visualize data, and add explanatory text.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you select only the 'name' and 'age' columns from DataFrame 'df'?",
    "options": [
      "df.select('name', 'age')",
      "df['name', 'age']",
      "df.selectColumns('name', 'age')",
      "df.getColumns('name', 'age')",
      "df.pick('name', 'age')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.select('name', 'age')` selects the specified columns from the DataFrame. Option B would cause an error. Options C, D, and E are not valid PySpark methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what function would you use to return the first non-null value in a list?",
    "options": [
      "COALESCE",
      "NVL",
      "IFNULL",
      "ISNULL",
      "NULLIF"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `COALESCE` returns the first non-null value in a list of expressions. Options B, C, and D are database-specific functions. Option E returns null if two expressions are equal.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which keyword is used to sort the result set of a query?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "SORT BY",
      "HAVING",
      "FILTER BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'ORDER BY' is used to sort the result set in ascending or descending order based on one or more columns.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method is used to replace null values in a DataFrame column 'age' with the value 0?",
    "options": [
      "df.na.fill({'age': 0})",
      "df.fillna({'age': 0})",
      "df.na.replace('age', 0)",
      "df.replaceNull('age', 0)",
      "df.fillNull('age', 0)"
    ],
    "answer": [
      0,
      1
    ],
    "category": "Data Transformation",
    "explanation": "Options A and B are correct because both `df.na.fill({'age': 0})` and `df.fillna({'age': 0})` replace null values in the 'age' column with 0. Option C is incorrect syntax for replacing nulls. Options D and E are not valid PySpark methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which of the following commands is used to remove old versions of data files that are no longer needed to save storage space?",
    "options": [
      "CLEAN TABLE my_table;",
      "VACUUM my_table;",
      "PURGE my_table;",
      "TRUNCATE HISTORY ON my_table;",
      "DELETE OLD VERSIONS FROM my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because the `VACUUM` command in Delta Lake is used to remove files that are no longer referenced by a Delta table, helping to save storage space. Option A is incorrect because `CLEAN TABLE` is not a valid command in this context. Option C is incorrect because `PURGE` is not the correct command for this operation in Databricks SQL. Option D is incorrect because `TRUNCATE HISTORY` is not a valid command in Delta Lake. Option E is incorrect because `DELETE OLD VERSIONS` is not a recognized command.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which visualization is best suited for comparing the proportions of categories within a dataset?",
    "options": [
      "Line Chart",
      "Scatter Plot",
      "Pie Chart",
      "Histogram",
      "Box Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a pie chart is designed to show the proportions of categories within a whole dataset. Option A is for trends over time. Option B shows relationships between two numerical variables. Option D shows the distribution of a single variable. Option E displays distribution based on quartiles.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which command is used to change the data type of a column in an existing table?",
    "options": [
      "MODIFY TABLE",
      "ALTER COLUMN",
      "CHANGE COLUMN",
      "ALTER TABLE... MODIFY COLUMN",
      "UPDATE COLUMN"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "Option D is correct because the `ALTER TABLE... MODIFY COLUMN` command is used to change a column's data type in many SQL dialects. Options A, B, and C are not standard or may vary by SQL implementation. Option E is not used for changing data types.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is true about Delta Lake's 'Time Travel' feature?",
    "options": [
      "It allows users to query older snapshots of a Delta Lake table using a version number or timestamp.",
      "It automatically backs up data to a remote location for disaster recovery.",
      "It enables real-time streaming of data into the Delta Lake table.",
      "It compresses historical data to save storage space.",
      "It provides real-time alerts when data is updated."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Lake's Time Travel feature allows users to access previous versions of the table using version numbers or timestamps. Option B is incorrect because Time Travel does not back up data to a remote location. Option C is incorrect because streaming data ingestion is handled differently. Option D is incorrect because Time Travel does not compress historical data. Option E is incorrect because Time Travel does not provide real-time alerts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data analysis, what is 'clustering'?",
    "options": [
      "A supervised learning technique for classification.",
      "A method of reducing the number of features in a dataset.",
      "An unsupervised learning technique for grouping similar data points.",
      "A way to normalize data distributions.",
      "A process of cleaning data by removing outliers."
    ],
    "answer": 2,
    "category": "Machine Learning Concepts",
    "explanation": "Option C is correct because clustering is an unsupervised learning method used to group similar data points based on their features.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main advantage of using 'Apache Arrow' in PySpark for data transfer?",
    "options": [
      "It provides built-in machine learning algorithms.",
      "It speeds up data transfer between JVM and Python processes by using a shared memory format.",
      "It compresses data to save storage space.",
      "It ensures data encryption during transfer.",
      "It allows for real-time streaming of data."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because Apache Arrow optimizes data transfer between processes by using a columnar memory format that reduces serialization overhead. Options A, C, D, and E are not primary advantages of Apache Arrow in this context.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method is used to add a new column to a DataFrame by applying a function to existing columns?",
    "options": [
      "withColumn()",
      "addColumn()",
      "insertColumn()",
      "appendColumn()",
      "modifyColumn()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `withColumn()` is used to add or replace a column in a DataFrame. Options B, C, D, and E are not valid PySpark DataFrame methods for this purpose.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which statement is used to modify the structure of an existing table?",
    "options": [
      "CHANGE TABLE",
      "UPDATE TABLE",
      "MODIFY TABLE",
      "ALTER TABLE",
      "RENAME TABLE"
    ],
    "answer": 3,
    "category": "Databricks SQL",
    "explanation": "Option D is correct because `ALTER TABLE` is used to modify the structure of an existing table. Options A, B, and C are not valid SQL statements for this purpose. Option E is used to rename a table.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which of the following statements is true about the 'JOIN' operations in SQL?",
    "options": [
      "An INNER JOIN returns all records from both tables, regardless of matching keys.",
      "A LEFT JOIN returns all records from the left table and matched records from the right table.",
      "A FULL OUTER JOIN returns only the records that have matching keys in both tables.",
      "A RIGHT JOIN returns all records from the right table only.",
      "JOIN operations cannot be performed on more than two tables."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because a LEFT JOIN returns all records from the left table and the matched records from the right table; unmatched records from the right table will be null. Option A describes a FULL OUTER JOIN incorrectly. Option C describes an INNER JOIN incorrectly. Option D is incorrect; a RIGHT JOIN returns all records from the right table and matched records from the left. Option E is incorrect; JOIN operations can be performed on multiple tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, what does the 'EXPLAIN' command do when used before a SELECT statement?",
    "options": [
      "It executes the query and displays the results.",
      "It shows the logical and physical execution plan of the query.",
      "It checks the syntax of the query without executing it.",
      "It optimizes the query for better performance.",
      "It converts the query into a stored procedure."
    ],
    "answer": 1,
    "category": "SQL Optimization",
    "explanation": "Option B is correct because `EXPLAIN` provides the execution plan, showing how Spark will execute the query. Option A is incorrect; it does not execute the query. Option C is incorrect; it does more than syntax checking. Option D is incorrect; it does not OPTIMIZE the query. Option E is incorrect; it does not create stored procedures.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1"
    ]
  },
  {
    "question": "In Spark Streaming, what does the 'window' operation allow you to do?",
    "options": [
      "Process data in micro-batches.",
      "Group data from a DStream over a sliding interval.",
      "Filter out duplicate records.",
      "Join two DStreams based on a key.",
      "Persist data to disk storage."
    ],
    "answer": 1,
    "category": "Streaming Data",
    "explanation": "Option B is correct because window operations in Spark Streaming allow you to apply transformations over a sliding window of data from a DStream.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how do you convert a string '2021-12-31' to a DATE data type?",
    "options": [
      "CAST('2021-12-31' AS DATE)",
      "CONVERT(DATE, '2021-12-31')",
      "TO_DATE('2021-12-31')",
      "PARSE_DATE('2021-12-31')",
      "FORMAT_DATE('2021-12-31')"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because using `CAST('2021-12-31' AS DATE)` converts the string to a DATE data type. Option B is SQL Server syntax. Option C is a common function in some SQL dialects but not standard. Option D and E are not standard SQL functions for type conversion.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "When using Databricks SQL, how can you schedule a query to run automatically at regular intervals?",
    "options": [
      "By configuring a job in the Jobs tab and setting a schedule.",
      "By adding a REFRESH SCHEDULE command at the end of the query.",
      "By setting up a cron job on your local machine to execute the query.",
      "By enabling auto-refresh in the query editor.",
      "Scheduling is not possible in Databricks SQL."
    ],
    "answer": 0,
    "category": "Query Scheduling",
    "explanation": "Option A is correct because Databricks allows you to schedule queries by creating a job in the Jobs tab and setting up the schedule. Option B is incorrect because `REFRESH SCHEDULE` is not a valid SQL command. Option C is impractical and not integrated with Databricks. Option D is incorrect because auto-refresh in the query editor is not used for scheduling query execution. Option E is incorrect because scheduling is possible.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In the context of data privacy, what does PII stand for, and why is it important?",
    "options": [
      "Personal Information Index; important for data backup.",
      "Publicly Indexed Information; important for marketing.",
      "Personally Identifiable Information; important for compliance with privacy laws.",
      "Personal Internet Information; important for network security.",
      "Protected Internal Insights; important for business strategy."
    ],
    "answer": 2,
    "category": "Data Privacy",
    "explanation": "Option C is correct because PII stands for Personally Identifiable Information, which includes any data that could potentially identify a specific individual, making it critical for compliance with privacy laws. Other options do not correctly define PII.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which Spark SQL function would you use to remove duplicate rows from a DataFrame?",
    "options": [
      "dropDuplicates()",
      "distinct()",
      "unique()",
      "dropna()",
      "filterDuplicates()"
    ],
    "answer": [
      0,
      1
    ],
    "category": "Data Transformation",
    "explanation": "Options A and B are correct because both `dropDuplicates()` and `distinct()` can be used to remove duplicate rows from a DataFrame. Option C is incorrect because `unique()` is not a standard method in Spark DataFrames. Option D is incorrect because `dropna()` removes rows with null values, not duplicates. Option E is incorrect because `filterDuplicates()` is not a standard Spark method.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When performing data ingestion into Databricks, which file format is columnar and supports efficient compression and encoding schemes?",
    "options": [
      "CSV",
      "JSON",
      "Parquet",
      "TXT",
      "XML"
    ],
    "answer": 2,
    "category": "Data Import",
    "explanation": "Option C is correct because Parquet is a columnar storage file format that supports efficient compression and encoding. Options A, B, D, and E are row-based formats and do not offer the same efficiency for large-scale analytics.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'feature scaling'?",
    "options": [
      "Selecting the most important features for a model.",
      "Reducing the number of features in a dataset.",
      "Adjusting the scale of features to a common range.",
      "Transforming categorical variables into numerical variables.",
      "Creating new features from existing ones."
    ],
    "answer": 2,
    "category": "Data Preprocessing",
    "explanation": "Option C is correct because feature scaling involves adjusting the scale of features so they contribute equally to the result, often using normalization or standardization.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of the 'Jobs UI'?",
    "options": [
      "To write and execute code in notebooks.",
      "To manage and monitor scheduled jobs.",
      "To configure cluster hardware settings.",
      "To set up user access permissions.",
      "To visualize data using built-in tools."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because the Jobs UI in Databricks is used to create, manage, and monitor scheduled jobs. Options A, C, D, and E describe other features in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In Databricks, what is the effect of the 'DROP TABLE' command when used on a managed Delta Lake table?",
    "options": [
      "It deletes the table metadata but leaves the data files intact.",
      "It deletes both the table metadata and the data files.",
      "It renames the table to 'deleted_table'.",
      "It archives the table data for recovery.",
      "It returns an error because 'DROP TABLE' cannot be used on Delta Lake tables."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because dropping a managed table in Delta Lake removes both the metadata and the data files. Option A is incorrect because unmanaged tables behave that way, not managed tables. Option C is incorrect; the table is not renamed. Option D is incorrect; data is not archived unless specifically configured. Option E is incorrect; 'DROP TABLE' can be used on Delta Lake tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'overfitting'?",
    "options": [
      "When a model performs well on training data but poorly on unseen data.",
      "When a model is too simple to capture underlying patterns.",
      "When a model has a high bias and low variance.",
      "When a model's performance improves with more features.",
      "When a model underutilizes the available data."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method allows you to read a CSV file located at '/data/sales.csv' into a DataFrame with headers included?",
    "options": [
      "df = spark.read.csv('/data/sales.csv', header=True)",
      "df = spark.read.csv('/data/sales.csv', headers=True)",
      "df = spark.read.csv('/data/sales.csv', includeHeaders=True)",
      "df = spark.read.format('csv').load('/data/sales.csv', header=True)",
      "df = spark.read.csv('/data/sales.csv', withHeader=True)"
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Option A is correct because setting `header=True` tells Spark to use the first line as column names. Option B is incorrect; the parameter is `header`, not `headers`. Option C and E use incorrect parameter names. Option D is partially correct but typically you pass options differently when using `format` and `load`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'ROW_NUMBER()' window function do?",
    "options": [
      "Assigns a unique sequential integer to rows within a partition.",
      "Calculates the average value over a window of rows.",
      "Ranks rows based on the value of a specified column with possible ties.",
      "Calculates the cumulative sum over a partition.",
      "Returns the number of rows in a table."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because 'ROW_NUMBER()' assigns a unique sequential integer to rows within a partition of a result set.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how do you create a global temporary view named 'global_temp_view' from a table 'sales_data'?",
    "options": [
      "CREATE TEMP VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE GLOBAL TEMPORARY VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE TEMPORARY VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE GLOBAL VIEW global_temp_view AS SELECT * FROM sales_data;"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because `CREATE GLOBAL TEMPORARY VIEW` creates a view that is available across all sessions and notebooks within the same Spark application. Options A and D create session-scoped temporary views. Option C creates a persistent view. Option E is not valid syntax in Databricks SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which of the following best describes the term 'data governance'?",
    "options": [
      "Technical process of moving data from one system to another.",
      "Policies and processes that ensure high data quality and data management.",
      "The physical storage of data in databases and data warehouses.",
      "The use of data visualization tools to present data insights.",
      "The encryption of data to protect it from unauthorized access."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data governance refers to the overall management, policies, and processes to ensure data quality, availability, integrity, and security. Options A, C, D, and E refer to specific technical aspects but not the broader concept of data governance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In a Databricks job, what is the purpose of setting up 'Retry' configurations?",
    "options": [
      "To automatically restart the cluster if it fails.",
      "To rerun failed tasks within a job without manual intervention.",
      "To test the job multiple times before final execution.",
      "To schedule the job to run multiple times a day.",
      "To replicate the job's output to multiple destinations."
    ],
    "answer": 1,
    "category": "Job Management",
    "explanation": "Option B is correct because 'Retry' settings allow Databricks to automatically retry failed tasks in a job, increasing reliability. Option A is incorrect; cluster restarts are handled differently. Option C is incorrect; retries are not for testing. Option D is incorrect; scheduling is set elsewhere. Option E is incorrect; retries do not replicate output.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In SQL, what is the 'TRIGGER' used for?",
    "options": [
      "To enforce data integrity by automatically updating data in response to certain events.",
      "To schedule routine database backups.",
      "To improve query performance with indexes.",
      "To define user roles and permissions.",
      "To create temporary tables during sessions."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because a TRIGGER is a database object that is automatically executed or fired when certain events occur, such as INSERT, UPDATE, or DELETE.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils.widgets' module?",
    "options": [
      "To create interactive input widgets in notebooks.",
      "To manage files in the Databricks File System.",
      "To handle secret management for credentials.",
      "To monitor and manage jobs.",
      "To interact with cluster configurations."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.widgets` allows you to create interactive widgets in Databricks notebooks for parameters like dropdowns and text inputs. Options B, C, D, and E are handled by other modules or interfaces.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'hyperparameter tuning'?",
    "options": [
      "Adjusting the parameters learned during training.",
      "Optimizing the configuration settings that guide the learning process.",
      "Scaling the input features to a standard range.",
      "Reducing the dimensionality of data.",
      "Validating the model on test data."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because hyperparameter tuning involves finding the best configuration settings (hyperparameters) that guide the learning process but are not learned from the data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary function of the 'collect()' action in Spark?",
    "options": [
      "To aggregate data across partitions.",
      "To bring all the data from executors to the driver node.",
      "To persist data in memory.",
      "To distribute data evenly across partitions.",
      "To write data to external storage."
    ],
    "answer": 1,
    "category": "Spark Actions",
    "explanation": "Option B is correct because `collect()` retrieves all the elements of the RDD or DataFrame to the driver node. Option A describes aggregation functions. Option C is about caching or persisting data. Option D is achieved with `repartition()` or `coalesce()`. Option E is done using `save` or `write` functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method would you use to apply a function to each element of a DataFrame column?",
    "options": [
      "map()",
      "apply()",
      "withColumn()",
      "transform()",
      "foreach()"
    ],
    "answer": 2,
    "category": "Data Transformation",
    "explanation": "Option C is correct because `withColumn()` allows you to create a new column or replace an existing one by applying a function to a column. Option A (`map()`) is used on RDDs, not DataFrames. Option B (`apply()`) is not a standard PySpark DataFrame method. Option D is used for array columns. Option E (`foreach()`) is used for actions, not transformations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'ALTER TABLE' statement do when used with 'ADD COLUMN'?",
    "options": [
      "It modifies an existing column.",
      "It deletes a column from the table.",
      "It adds a new column to the table.",
      "It renames the table.",
      "It changes the data type of a column."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `ALTER TABLE... ADD COLUMN` adds a new column to an existing table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how can you stop a SparkSession named 'spark'?",
    "options": [
      "spark.stop()",
      "spark.end()",
      "spark.shutdown()",
      "spark.terminate()",
      "spark.close()"
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because `spark.stop()` is used to stop the SparkSession.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary benefit of using 'Parquet' files in big data processing with Spark?",
    "options": [
      "Parquet files are human-readable, making debugging easier.",
      "Parquet is a row-based storage format that speeds up data ingestion.",
      "Parquet files support schema evolution without requiring changes.",
      "Parquet is a columnar storage format that provides efficient data compression and encoding schemes.",
      "Parquet files are the default format for all Spark DataFrames."
    ],
    "answer": 3,
    "category": "Data Storage Formats",
    "explanation": "Option D is correct because Parquet is a columnar storage format that provides efficient data compression and encoding, which optimizes performance in big data processing. Option A is incorrect; Parquet files are binary and not human-readable. Option B is incorrect; Parquet is columnar, not row-based. Option C is partially true but not the primary benefit. Option E is incorrect; Spark DataFrames can be in various formats.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what does the term 'lazy evaluation' mean?",
    "options": [
      "Transformations are executed immediately when called.",
      "Actions and transformations are executed in parallel.",
      "Transformations are not executed until an action is called.",
      "Data is cached in memory to speed up processing.",
      "The Spark engine automatically optimizes code at runtime."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because in Spark, transformations are lazily evaluated, meaning they are not executed until an action triggers the computation. Option A is incorrect; transformations are not executed immediately. Option B is partially correct but doesn't define lazy evaluation. Option D is about caching, not lazy evaluation. Option E refers to Spark's optimization but is not the definition of lazy evaluation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is an advantage of using JSON files in data storage?",
    "options": [
      "They are optimized for high-speed read and write operations.",
      "They support schema evolution and data types efficiently.",
      "They are human-readable and support hierarchical data structures.",
      "They provide the best compression ratios for big data.",
      "They enforce a strict schema, reducing data inconsistency."
    ],
    "answer": 2,
    "category": "Data Storage Formats",
    "explanation": "Option C is correct because JSON files are human-readable and support nested, hierarchical data structures, making them flexible for storing complex data. Options A, B, D, and E are not primary advantages of JSON files compared to other formats.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of data analysis, what does ETL stand for, and what is its primary purpose?",
    "options": [
      "Extract, Transform, Load; to move and reshape data from sources to destinations.",
      "Evaluate, Test, Learn; to improve machine learning models.",
      "Extract, Transfer, Link; to connect data between systems.",
      "Encrypt, Transmit, Log; to secure data during transfer.",
      "Estimate, Transform, Load; to approximate data before storage."
    ],
    "answer": 0,
    "category": "Data Integration",
    "explanation": "Option A is correct because ETL stands for Extract, Transform, Load, which involves extracting data from sources, transforming it to fit operational needs, and loading it into a destination system. Options B, C, D, and E do not correctly define ETL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'ALTER TABLE' command with 'ADD CONSTRAINT' do?",
    "options": [
      "Adds a new column to the table.",
      "Changes the data type of an existing column.",
      "Adds a rule to enforce data integrity, such as a primary key or foreign key.",
      "Removes an existing constraint from the table.",
      "Deletes data from the table based on a condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `ALTER TABLE... ADD CONSTRAINT` adds a new constraint to enforce data integrity rules. Options A and B are different uses of `ALTER TABLE`. Option D would use `DROP CONSTRAINT`. Option E is done with `DELETE`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-constraints.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'INNER JOIN' clause?",
    "options": [
      "To return all rows from both tables, matching and non-matching.",
      "To return rows when there is a match in either left or right table.",
      "To return rows when there is a match in both tables.",
      "To return all rows from the left table, and matched rows from the right table.",
      "To combine rows from two tables without any condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because an 'INNER JOIN' returns rows when there is a match in both tables based on the specified condition.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the 'CAP theorem' in distributed computing?",
    "options": [
      "It states that a distributed system can simultaneously provide consistency, availability, and partition tolerance.",
      "It asserts that it's impossible for a distributed data store to simultaneously provide more than two out of three guarantees: consistency, availability, and partition tolerance.",
      "It defines the best practices for capacity planning in cloud computing.",
      "It is a protocol for secure data transmission.",
      "It is a method for compressing data in distributed systems."
    ],
    "answer": 1,
    "category": "Distributed Systems",
    "explanation": "Option B is correct because the CAP theorem states that in the presence of a network partition, a distributed system can choose either consistency or availability, but not both. Option A is incorrect; it cannot provide all three simultaneously. Options C, D, and E are unrelated.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method is used to join two DataFrames 'df1' and 'df2' on a common column 'id'?",
    "options": [
      "df1.join(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.append(df2, on='id')",
      "df1.concat(df2, on='id')",
      "df1.combine(df2, 'id')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df1.join(df2, 'id')` joins the two DataFrames on the 'id' column. Option B is a pandas method. Options C and D are used for combining data but not for joining on a key in PySpark. Option E is not a valid PySpark method.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which function calculates the average value of a numeric column?",
    "options": [
      "SUM()",
      "COUNT()",
      "AVG()",
      "MAX()",
      "MIN()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because `AVG()` computes the average of the values in a numeric column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what does the 'persist(StorageLevel.DISK_ONLY)' method do?",
    "options": [
      "Caches the RDD in memory only.",
      "Caches the RDD on disk only.",
      "Caches the RDD both in memory and disk.",
      "Removes the RDD from cache.",
      "Serializes the RDD for network transmission."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because 'persist(StorageLevel.DISK_ONLY)' stores the RDD partitions on disk, not in memory.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'overfitting' in machine learning models?",
    "options": [
      "When a model is too simple and fails to capture underlying patterns.",
      "When a model performs well on training data but poorly on new, unseen data.",
      "When a model has too few parameters to make accurate predictions.",
      "When a model's performance improves with more data.",
      "When a model has balanced bias and variance."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because overfitting occurs when a model learns the training data too well, including noise, and does not generalize well to new data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you filter rows in a DataFrame 'df' where the column 'age' is greater than 30?",
    "options": [
      "df.filter(df.age > 30)",
      "df.where(df.age > 30)",
      "df.select('*').where('age > 30')",
      "All of the above",
      "None of the above"
    ],
    "answer": 3,
    "category": "Data Transformation",
    "explanation": "Option D is correct because all the methods listed are valid ways to filter rows where 'age' is greater than 30 in PySpark. Option A uses `filter` with a condition. Option B uses `where`, which is an alias for `filter`. Option C uses `select` followed by `where` with a SQL expression.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you drop a column named 'age' from DataFrame 'df'?",
    "options": [
      "df.drop('age')",
      "df.removeColumn('age')",
      "df.delete('age')",
      "df.dropColumn('age')",
      "df.withoutColumn('age')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.drop('age')` returns a new DataFrame without the 'age' column. Options B, C, D, and E are not valid PySpark DataFrame methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'GROUP BY' clause do in a SQL query?",
    "options": [
      "It sorts the result set based on one or more columns.",
      "It filters rows based on a condition.",
      "It groups rows that have the same values in specified columns into summary rows.",
      "It combines the result sets of two SELECT statements.",
      "It specifies the database to use for the query."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `GROUP BY` clause groups rows sharing a value in specified columns, allowing aggregate functions to be applied. Option A describes `ORDER BY`. Option B describes `WHERE`. Option D describes `UNION`. Option E is incorrect; selecting a database is done with `USE database_name;`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is the purpose of the 'mapPartitions' transformation?",
    "options": [
      "To apply a function to each element of the RDD individually.",
      "To apply a function to each partition of the RDD.",
      "To reduce the number of partitions.",
      "To cache the RDD in memory.",
      "To group elements based on a key."
    ],
    "answer": 1,
    "category": "Spark Transformations",
    "explanation": "Option B is correct because `mapPartitions` allows you to apply a function to each partition of the RDD, offering performance optimizations over `map`. Option A describes `map`. Option C is done with `coalesce` or `repartition`. Option D is done with `cache`. Option E is done with `groupByKey`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following Spark transformations is an action that triggers computation and returns a value to the driver program?",
    "options": [
      "map()",
      "filter()",
      "reduce()",
      "flatMap()",
      "groupBy()"
    ],
    "answer": 2,
    "category": "Spark Actions",
    "explanation": "Option C is correct because `reduce()` is an action that aggregates data and returns a value to the driver. Options A, B, D, and E are transformations, not actions. Transformations are lazy and define a new RDD/DataFrame but do not compute until an action is called.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, how do you select distinct values from a column 'category' in a DataFrame 'df'?",
    "options": [
      "df.select('category').distinct()",
      "df.selectDistinct('category')",
      "df.distinct('category')",
      "df.select('category').unique()",
      "df.unique('category')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.select('category').distinct()` selects the 'category' column and returns distinct values. Options B, C, D, and E are not valid methods in PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is NOT a valid Spark shuffle partitioning method?",
    "options": [
      "HashPartitioner",
      "RangePartitioner",
      "RoundRobinPartitioner",
      "CustomPartitioner",
      "BroadcastPartitioner"
    ],
    "answer": 4,
    "category": "Spark Concepts",
    "explanation": "Option E is correct because there is no `BroadcastPartitioner` in Spark. Options A, B, C, and D are valid partitioners.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'EXPLAIN' command do?",
    "options": [
      "Executes a query and returns the results.",
      "Provides the execution plan for a query without executing it.",
      "Optimizes a query for better performance.",
      "Creates an index on a table.",
      "Displays the schema of a table."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'EXPLAIN' provides the execution plan for a query, which helps in understanding and optimizing query performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, what does the 'groupBy' method return?",
    "options": [
      "A DataFrame object.",
      "A RelationalGroupedDataset object.",
      "A list of grouped DataFrames.",
      "A key-value pair RDD.",
      "An aggregated DataFrame."
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because `groupBy` returns a `GroupedData` object (also known as `RelationalGroupedDataset`), which can be used to perform aggregate functions. Option A is incorrect; `groupBy` does not return a DataFrame directly. Option C is incorrect. Option D is related to RDDs, not DataFrames. Option E is the result after applying an aggregation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Auto Loader' feature primarily used for?",
    "options": [
      "To automatically scale clusters based on workload.",
      "To incrementally and efficiently process new data files as they arrive in cloud storage.",
      "To load data into memory for faster processing.",
      "To automate the deployment of machine learning models.",
      "To schedule notebooks to run at specified intervals."
    ],
    "answer": 1,
    "category": "Data Ingestion",
    "explanation": "Option B is correct because Auto Loader is a feature in Databricks that incrementally processes new data files as they arrive in cloud storage, supporting efficient ingestion for streaming and batch workloads. Options A, C, D, and E describe different functionalities.",
    "sources": [
      "https://docs.databricks.com/en/ingestion/auto-loader/index.html"
    ]
  },
  {
    "question": "Which command in Databricks SQL allows you to view all tables within the current database?",
    "options": [
      "SHOW DATABASES;",
      "LIST TABLES;",
      "SHOW TABLES;",
      "DESCRIBE DATABASE;",
      "SELECT * FROM information_schema.tables;"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "Option C is correct because `SHOW TABLES;` lists all tables in the current database. Option A is incorrect because it shows databases, not tables. Option B is incorrect because `LIST TABLES;` is not standard SQL syntax. Option D is incorrect because it describes the database but doesn't list tables. Option E is incorrect in Databricks SQL context as accessing `information_schema` may not be supported directly.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In SQL, which clause is used to rename a column in the result set of a query?",
    "options": [
      "RENAME COLUMN",
      "AS",
      "MODIFY COLUMN",
      "CHANGE COLUMN",
      "ALIAS"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the 'AS' keyword is used to provide an alias for a column in the result set. Option A is not standard SQL for renaming in the result set. Options C and D are used to alter table structures. Option E is a concept but not a SQL keyword.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'persist' method do in Spark, and how is it different from 'cache'?",
    "options": [
      "Persist saves the DataFrame to disk; cache saves it to memory.",
      "Persist allows storage level specification; cache uses the default storage level (memory only).",
      "Persist is used for streaming data; cache is for batch data.",
      "Persist removes the DataFrame from memory; cache keeps it.",
      "There is no difference; they are interchangeable."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because `persist` allows you to specify the storage level (e.g., memory, disk), while `cache` is a shorthand for `persist` with the default storage level of memory only. Option A is not entirely accurate. Option C is incorrect; both can be used for streaming or batch. Option D is incorrect. Option E is incorrect; there is a difference in flexibility.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'Unity Catalog'?",
    "options": [
      "To provide a unified governance solution for all data and AI assets.",
      "To manage cluster configurations and performance.",
      "To develop machine learning models collaboratively.",
      "To offer a centralized dashboard for job monitoring.",
      "To store code repositories for version control."
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "Option A is correct because Unity Catalog is a unified governance solution in Databricks for managing and securing data and AI assets across clouds. Options B, C, D, and E describe different features.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how do you create a new managed Delta table named 'customers' using data from an existing table 'raw_customers'?",
    "options": [
      "CREATE TABLE customers AS SELECT * FROM raw_customers;",
      "CREATE DELTA TABLE customers FROM raw_customers;",
      "SELECT * INTO customers FROM raw_customers;",
      "MAKE TABLE customers AS SELECT * FROM raw_customers;",
      "INSERT INTO customers SELECT * FROM raw_customers;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because 'CREATE TABLE customers AS SELECT * FROM raw_customers;' creates a new table with data from 'raw_customers'. Option B is not standard syntax. Option C syntax varies by SQL dialect. Option D is incorrect syntax. Option E assumes 'customers' already exists.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data analysis, what does 'EDA' stand for and what is its purpose?",
    "options": [
      "Enterprise Data Access; to manage user permissions.",
      "Exploratory Data Analysis; to summarize main characteristics of data.",
      "External Data Algorithm; to process data from external sources.",
      "Efficient Data Aggregation; to OPTIMIZE query performance.",
      "Encoded Data Architecture; to structure data storage."
    ],
    "answer": 1,
    "category": "Data Analysis",
    "explanation": "Option B is correct because EDA stands for Exploratory Data Analysis, which involves analyzing data sets to summarize their main characteristics, often using visual methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, when would you choose to use a heatmap?",
    "options": [
      "To display the distribution of a single variable over time.",
      "To compare parts of a whole using percentages.",
      "To show the relationship between two categorical variables.",
      "To track changes in data across geographical locations.",
      "To display hierarchical data in a tree-like structure."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because heatmaps are effective for showing relationships between two categorical variables, with color intensity representing the frequency or magnitude. Option A is better suited for line charts. Option B is for pie charts. Option D is for maps or choropleths. Option E is for tree maps.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In machine learning, what is the 'Recall' metric?",
    "options": [
      "The ratio of true positives to all actual positives.",
      "The ratio of true negatives to all predicted negatives.",
      "The proportion of correct predictions over total predictions.",
      "The ability of the model to correctly predict negative classes.",
      "The ratio of false positives to true negatives."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because recall measures the proportion of actual positives that were correctly identified by the model.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which function in SQL would you use to convert a string to uppercase?",
    "options": [
      "UPPER(string)",
      "TO_UPPERCASE(string)",
      "MAKE_UPPER(string)",
      "CAPITALIZE(string)",
      "STRING_UPPER(string)"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `UPPER(string)` converts the input string to uppercase in SQL. Options B, C, D, and E are not standard SQL functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, when is a 'heatmap' most appropriately used?",
    "options": [
      "To display hierarchical data using nested rectangles.",
      "To show the frequency distribution of a single variable.",
      "To represent the correlation between two variables using colors.",
      "To compare proportions of categories within a whole.",
      "To plot individual data points on a two-dimensional plane."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a heatmap uses color shading to represent values of variables, often used to show correlation between two variables. Option A describes a treemap. Option B is better represented by a histogram. Option D is for pie charts. Option E describes a scatter plot.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'sharding' in the context of databases?",
    "options": [
      "Encrypting data to secure it.",
      "Replicating data across multiple servers for redundancy.",
      "Partitioning data horizontally to distribute it across multiple databases.",
      "Backing up data to prevent loss.",
      "Indexing data to improve query performance."
    ],
    "answer": 2,
    "category": "Data Management",
    "explanation": "Option C is correct because sharding involves partitioning a database horizontally to spread data across multiple machines, improving scalability and performance. Options A, B, D, and E describe different concepts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'data lineage' in data governance?",
    "options": [
      "A record of data's origin and transformations over time.",
      "The security protocol for accessing data.",
      "The hierarchical structure of data storage.",
      "The process of cleaning and preparing data.",
      "The method of storing data in cloud environments."
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "Option A is correct because data lineage refers to the lifecycle of data, including its origins and transformations over time.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html"
    ]
  },
  {
    "question": "In machine learning, what is 'regularization' used for?",
    "options": [
      "To increase the complexity of a model.",
      "To prevent overfitting by adding a penalty for larger weights.",
      "To reduce the size of the dataset.",
      "To improve the speed of training by simplifying calculations.",
      "To adjust the learning rate during training."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because regularization adds a penalty to the loss function for large weights, helping to prevent overfitting. Option A is incorrect; it reduces complexity. Option C is data reduction. Option D is not the primary purpose. Option E describes learning rate scheduling.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what is a 'snowflake schema'?",
    "options": [
      "A schema with denormalized dimension tables.",
      "A centralized fact table connected to multiple normalized dimension tables.",
      "A single table containing all data.",
      "A method for indexing data for faster retrieval.",
      "A schema used exclusively for time-series data."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a snowflake schema is a variation of the star schema where dimension tables are normalized into multiple related tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a common use case for the 'groupByKey' transformation in Spark?",
    "options": [
      "To reduce data across all keys in the dataset.",
      "To group all the values with the same key into a single sequence.",
      "To filter out keys based on a condition.",
      "To sort the dataset by key.",
      "To join two datasets based on keys."
    ],
    "answer": 1,
    "category": "Spark Transformations",
    "explanation": "Option B is correct because `groupByKey` groups all values associated with the same key. Option A is for `reduceByKey`. Option C is for `filter`. Option D is for `sortByKey`. Option E is for `join` operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, what does the 'CAST' function do?",
    "options": [
      "It changes the data type of a column to a specified type.",
      "It removes duplicates from a DataFrame.",
      "It splits a column into multiple columns.",
      "It filters rows based on a condition.",
      "It merges two DataFrames together."
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `CAST` is used to convert the data type of a column to another data type. Options B, C, D, and E describe different operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'COALESCE' function do?",
    "options": [
      "Returns the first non-null value in a list of expressions.",
      "Combines two strings into one.",
      "Counts the number of non-null values.",
      "Calculates the average of a set of values.",
      "Splits a string into multiple parts."
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `COALESCE` returns the first non-null value from the list of expressions provided.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of the 'checkpointing' feature in Spark Streaming?",
    "options": [
      "To save intermediate results to OPTIMIZE performance.",
      "To provide fault tolerance by saving state information to reliable storage.",
      "To distribute data evenly across partitions.",
      "To cache data in memory for faster processing.",
      "To secure data by encrypting it during processing."
    ],
    "answer": 1,
    "category": "Streaming Data",
    "explanation": "Option B is correct because checkpointing saves the state of the streaming application to reliable storage, enabling recovery in case of failures. Options A, C, D, and E describe different functionalities.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which SQL function would you use to calculate the total number of rows in a table named 'orders'?",
    "options": [
      "SELECT COUNT(*) FROM orders;",
      "SELECT SUM(*) FROM orders;",
      "SELECT TOTAL(*) FROM orders;",
      "SELECT NUMBER(*) FROM orders;",
      "SELECT LENGTH(*) FROM orders;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because `COUNT(*)` returns the total number of rows in the table. Options B, C, D, and E are not appropriate functions for counting rows.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils' library?",
    "options": [
      "To perform database optimization tasks.",
      "To provide utility functions for file system operations, widgets, and notebook tasks.",
      "To create visualizations in notebooks.",
      "To manage user permissions and authentication.",
      "To develop machine learning models."
    ],
    "answer": 1,
    "category": "Databricks Utilities",
    "explanation": "Option B is correct because `dbutils` provides utility functions in Databricks for various tasks such as file system operations, creating widgets, and controlling notebook execution. Options A, C, D, and E are not primary functions of `dbutils`.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In Spark, what does the 'collect()' action do when called on a DataFrame or RDD?",
    "options": [
      "It saves the DataFrame or RDD to disk.",
      "It returns all the elements of the DataFrame or RDD to the driver program as a list.",
      "It caches the DataFrame or RDD in memory.",
      "It applies a function to each element without returning any value.",
      "It partitions the DataFrame or RDD across the cluster."
    ],
    "answer": 1,
    "category": "Spark Actions",
    "explanation": "Option B is correct because `collect()` retrieves all the elements from the DataFrame or RDD to the driver as a list. Option A describes `save` operations. Option C describes `cache()`. Option D describes `foreach()`. Option E describes `repartition()` or `coalesce()`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'data augmentation' in the context of machine learning?",
    "options": [
      "Adding more data by collecting additional samples.",
      "Enhancing the dataset by generating synthetic data to increase diversity.",
      "Reducing the size of the dataset for faster processing.",
      "Combining multiple datasets into one.",
      "Cleaning the dataset by removing outliers."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because data augmentation involves creating synthetic data based on existing data to increase the diversity and size of the training dataset, often used in image processing. Option A refers to data collection. Option C is the opposite. Option D is data merging. Option E is data cleaning.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following statements about Delta Lake's 'Schema Enforcement' feature is true?",
    "options": [
      "It allows any data to be written to the table, regardless of schema.",
      "It prevents updates to the table schema once created.",
      "It ensures that data written to a table adheres to the table's schema, preventing corrupt data.",
      "It automatically modifies the table schema to match incoming data.",
      "It only applies to streaming data sources."
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Option C is correct because Schema Enforcement (also known as schema validation) ensures that incoming data matches the table schema, preventing the introduction of corrupt data. Option A is incorrect; schema enforcement restricts data that doesn't match the schema. Option B is incorrect; schema can be altered deliberately. Option D is incorrect; automatic schema merging must be enabled explicitly. Option E is incorrect; schema enforcement applies to both batch and streaming data.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data processing, what is 'data partitioning'?",
    "options": [
      "Dividing data into subsets to improve performance and manageability.",
      "Encrypting data for secure transmission.",
      "Backing up data to prevent loss.",
      "Compressing data to save storage space.",
      "Removing duplicate data from datasets."
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because data partitioning involves dividing data into smaller, manageable pieces, which can improve performance and scalability.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method allows you to write a DataFrame 'df' as a partitioned table by the column 'year'?",
    "options": [
      "df.write.partitionBy('year').save('/path/to/table')",
      "df.write.byPartition('year').save('/path/to/table')",
      "df.saveAsPartitionedTable('/path/to/table', 'year')",
      "df.write.partition('year').save('/path/to/table')",
      "df.write.savePartitioned('/path/to/table', partition='year')"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because 'df.write.partitionBy('year').save('/path/to/table')' writes the DataFrame partitioned by the 'year' column. Options B, C, D, and E are not valid PySpark methods or syntax.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'INNER JOIN' clause do?",
    "options": [
      "Returns all rows from both tables, matching and non-matching.",
      "Returns rows when there is a match in either left or right table.",
      "Returns rows when there is a match in both tables.",
      "Returns all rows from the left table, and matched rows from the right table.",
      "Returns all rows from the right table, and matched rows from the left table."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `INNER JOIN` returns rows that have matching values in both tables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which command would you use to display the current databases available in the environment?",
    "options": [
      "SHOW DATABASES;",
      "LIST ALL DATABASES;",
      "DISPLAY DATABASES;",
      "GET DATABASE LIST;",
      "SELECT DATABASES;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `SHOW DATABASES;` is the standard SQL command to list all databases in the current environment. Option B is incorrect because `LIST ALL DATABASES;` is not standard SQL syntax. Option C is incorrect because `DISPLAY DATABASES;` is not a recognized SQL command. Option D is incorrect because `GET DATABASE LIST;` is not valid SQL syntax. Option E is incorrect because `SELECT DATABASES;` is not the correct way to retrieve a list of databases.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is 'MLflow' used for?",
    "options": [
      "Managing and deploying machine learning models.",
      "Visualizing data with interactive dashboards.",
      "Optimizing cluster performance.",
      "Storing large datasets.",
      "Encrypting data in transit."
    ],
    "answer": 0,
    "category": "Databricks Machine Learning",
    "explanation": "Option A is correct because MLflow is an open-source platform for managing the end-to-end machine learning lifecycle.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best defines 'data lake'?",
    "options": [
      "A centralized repository that allows you to store all your structured and unstructured data at any scale.",
      "A database optimized for online transaction processing.",
      "A repository for storing machine learning models.",
      "A visualization tool for big data.",
      "A methodology for data governance."
    ],
    "answer": 0,
    "category": "Data Management",
    "explanation": "Option A is correct because a data lake is a centralized repository that stores structured and unstructured data at any scale. Options B, C, D, and E describe different concepts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data processing, what is an 'ETL' pipeline?",
    "options": [
      "An automated process for evaluating, testing, and logging data.",
      "A process for extracting data, transforming it, and loading it into a target system.",
      "A pipeline for encrypting, transmitting, and logging sensitive data.",
      "A method for estimating, training, and learning in machine learning models.",
      "A real-time data streaming architecture."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because ETL stands for Extract, Transform, Load, which is a common process in data warehousing and processing. Options A, C, D, and E describe different processes.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'OVER' clause in SQL window functions?",
    "options": [
      "To specify the partitions and ordering of data for the window function.",
      "To filter rows before aggregation.",
      "To join tables based on a condition.",
      "To group rows that have the same values.",
      "To limit the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because the `OVER` clause defines the partitioning and ordering of rows for window functions in SQL. Options B, C, D, and E are not related to the `OVER` clause.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which command is used to remove a view named 'sales_view'?",
    "options": [
      "DROP TABLE sales_view;",
      "DELETE FROM sales_view;",
      "DROP VIEW sales_view;",
      "TRUNCATE VIEW sales_view;",
      "REMOVE VIEW sales_view;"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `DROP VIEW sales_view;` is the command used to remove a view from the database. Option A would attempt to drop a table, not a view. Option B deletes data from a table. Option D is not valid syntax for views. Option E is not a standard SQL command.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In data visualization, which chart is best for showing the distribution of data over time?",
    "options": [
      "Pie Chart",
      "Line Chart",
      "Scatter Plot",
      "Bar Chart",
      "Histogram"
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a line chart is ideal for displaying data trends over time. Option A shows proportions. Option C shows relationships between two variables. Option D can show data over time but is less effective for trends. Option E shows data distribution, not over time.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method can be used to join DataFrame 'df1' with DataFrame 'df2' on column 'id' using an inner join?",
    "options": [
      "df1.join(df2, 'id', 'inner')",
      "df1.innerJoin(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.combine(df2, 'id', 'inner')",
      "df1.union(df2)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df1.join(df2, 'id', 'inner')' performs an inner join on the 'id' column. Options B, C, D, and E are not valid methods for joining DataFrames in PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of 'Delta Engine' in Databricks?",
    "options": [
      "To provide a query optimization layer for Delta Lake.",
      "To manage clusters and resources.",
      "To encrypt data at rest.",
      "To stream data in real-time.",
      "To version control notebooks."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Engine is a query optimizer and execution engine for Delta Lake, enhancing performance for data analytics workloads.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following is an advantage of using Delta Lake over traditional data lakes?",
    "options": [
      "Delta Lake does not require schema definition, allowing for flexible data ingestion.",
      "Delta Lake provides ACID transactions for reliable data operations.",
      "Delta Lake automatically visualizes data without additional tools.",
      "Delta Lake stores data in proprietary formats, enhancing security.",
      "Delta Lake is only compatible with structured data."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because Delta Lake supports ACID transactions, ensuring data reliability and consistency. Option A is incorrect; Delta Lake enforces schema, which is beneficial. Option C is incorrect; visualization requires separate tools. Option D is incorrect; Delta Lake uses open formats like Parquet. Option E is incorrect; Delta Lake can handle semi-structured data as well.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Spark, what is 'lazy evaluation'?",
    "options": [
      "A strategy where computations are executed immediately.",
      "A method of caching data in memory.",
      "A way to delay the execution of transformations until an action is called.",
      "An optimization technique to repartition data.",
      "A process of checkpointing data."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because Spark uses lazy evaluation, meaning it waits until an action is called to execute transformations, optimizing the computation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is the function of the 'repartition' method on a DataFrame?",
    "options": [
      "To increase or decrease the number of partitions randomly.",
      "To sort the DataFrame based on a column.",
      "To group data based on a key.",
      "To persist the DataFrame in memory.",
      "To collect data to the driver node."
    ],
    "answer": 0,
    "category": "Spark Optimization",
    "explanation": "Option A is correct because `repartition` reshuffles the data to increase or decrease the number of partitions, distributing data randomly unless a column is specified. Option B describes `sort` or `orderBy`. Option C describes `groupBy`. Option D describes `persist` or `cache`. Option E describes `collect`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, what is the result of using the 'GROUPING SETS' clause in a GROUP BY statement?",
    "options": [
      "It creates multiple groupings in a single query, allowing for subtotal computations.",
      "It groups data based on sets of columns and removes duplicates.",
      "It sorts the result set based on multiple columns.",
      "It filters groups based on a condition after aggregation.",
      "It combines the results of multiple SELECT statements."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because 'GROUPING SETS' allows you to specify multiple groupings in the same query, which is useful for computing subtotals and grand totals in reports. Option B describes removing duplicates, which is not the function of 'GROUPING SETS'. Option C is related to 'ORDER BY'. Option D describes the 'HAVING' clause. Option E refers to the 'UNION' operator.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data modeling, what is a 'factless fact table'?",
    "options": [
      "A table that contains only foreign keys without measurable facts.",
      "A dimension table with no attributes.",
      "A table that combines both fact and dimension data.",
      "A table that stores summary data only.",
      "A deprecated concept in modern data warehousing."
    ],
    "answer": 0,
    "category": "Data Modeling",
    "explanation": "Option A is correct because a factless fact table captures events or conditions but contains no measurable data, only foreign keys to dimensions. Options B, C, D, and E are incorrect definitions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the role of the 'CACHE' command in Spark SQL when used in Databricks?",
    "options": [
      "It saves a DataFrame to disk for persistence.",
      "It loads data into memory to improve query performance on repeated accesses.",
      "It clears the memory cache to free up resources.",
      "It archives data files into a compressed format.",
      "It checkpoints data for fault tolerance."
    ],
    "answer": 1,
    "category": "Spark SQL",
    "explanation": "Option B is correct because `CACHE` tells Spark to keep the data in memory, which speeds up subsequent actions on the same data. Option A is incorrect; saving to disk is done with `write`. Option C is incorrect; to clear cache, you use `UNCACHE`. Option D is incorrect; caching does not compress data. Option E is incorrect; checkpointing is different from caching.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you register a DataFrame as a temporary view named 'sales_view' to run SQL queries against it?",
    "options": [
      "df.createTempView('sales_view')",
      "df.registerAsView('sales_view')",
      "spark.registerDataFrameAsTable(df, 'sales_view')",
      "df.createOrReplaceTempView('sales_view')",
      "df.registerTempTable('sales_view')"
    ],
    "answer": [
      0,
      3
    ],
    "category": "Data Transformation",
    "explanation": "Options A and D are correct because both `df.createTempView('sales_view')` and `df.createOrReplaceTempView('sales_view')` register the DataFrame as a temporary view. Option A will create the view and throw an error if a view with the same name already exists. Option D will create the view or replace it if it already exists. Option B is incorrect; `registerAsView` is not a standard method. Option C is deprecated in newer versions of Spark. Option E is deprecated in favor of `createOrReplaceTempView`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In PySpark, how do you convert a DataFrame 'df' to a pandas DataFrame?",
    "options": [
      "df.toPandas()",
      "df.convertToPandas()",
      "df.asPandas()",
      "df.to_pandas()",
      "df.pandas()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df.toPandas()' converts a PySpark DataFrame to a pandas DataFrame.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'GROUP BY' clause do?",
    "options": [
      "Filters rows based on a condition.",
      "Orders the result set.",
      "Aggregates data across multiple rows.",
      "Combines rows from two or more tables.",
      "Limits the number of rows returned."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `GROUP BY` groups rows that have the same values in specified columns and allows aggregate functions to be applied to each group. Option A describes `WHERE`. Option B describes `ORDER BY`. Option D describes `JOIN`. Option E uses `LIMIT`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'UNION' operator?",
    "options": [
      "To combine the results of two queries and include duplicates.",
      "To combine the results of two queries and remove duplicates.",
      "To find the intersection of two queries.",
      "To subtract one query's results from another.",
      "To join two tables based on a condition."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `UNION` combines the results of two SELECT statements and removes any duplicate rows. Option A describes `UNION ALL`. Option C is `INTERSECT`. Option D is `EXCEPT` or `MINUS`. Option E describes a `JOIN`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'DISTINCT' keyword do?",
    "options": [
      "Sorts the results in ascending order.",
      "Filters records based on a condition.",
      "Removes duplicate rows from the result set.",
      "Limits the number of rows returned.",
      "Groups rows that have the same values."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because 'DISTINCT' eliminates duplicate rows from the result set of a SELECT statement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of the 'Secret Scope'?",
    "options": [
      "To store and manage sensitive credentials securely.",
      "To define the scope of variables in notebooks.",
      "To set permissions for cluster usage.",
      "To manage database schemas.",
      "To monitor resource utilization."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because Secret Scopes in Databricks are used to store and manage sensitive information like passwords and keys securely.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which SQL clause is used to combine rows from two or more tables, based on a related column between them?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "JOIN",
      "HAVING",
      "WHERE"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `JOIN` clauses are used to combine rows from two or more tables based on a related column. Options A, B, D, and E serve different purposes and are not used for combining tables in this way.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'LIKE' operator with wildcards?",
    "options": [
      "To perform arithmetic operations.",
      "To compare exact string matches.",
      "To search for a specified pattern in a column.",
      "To sort query results.",
      "To group results based on a condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the 'LIKE' operator is used with wildcards '%' and '_' to search for patterns within string columns.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'UNION' operator in SQL?",
    "options": [
      "To select records that have matching values in both tables.",
      "To combine the result sets of two or more SELECT statements, removing duplicates.",
      "To update records in a table based on another table.",
      "To join tables based on non-matching values.",
      "To subtract one result set from another."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `UNION` operator combines the results of two or more SELECT statements into a single result set, removing duplicates. Option A describes an INNER JOIN. Option C is about UPDATE statements. Option D is more like a FULL OUTER JOIN. Option E describes the `EXCEPT` or `MINUS` operator.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Delta Lake, what command would you use to revert a table named 'transactions' to a previous version?",
    "options": [
      "RESTORE TABLE transactions TO VERSION AS OF 5;",
      "ROLLBACK TABLE transactions TO 5;",
      "REVERT TABLE transactions TO VERSION 5;",
      "UNDO TABLE transactions TO VERSION 5;",
      "RESET TABLE transactions TO VERSION 5;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `RESTORE TABLE transactions TO VERSION AS OF 5;` reverts the table to a specific version. Options B, C, D, and E are incorrect because they are not valid Delta Lake commands for reverting table versions.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following is a key advantage of using Databricks Delta Live Tables (DLT)?",
    "options": [
      "They allow for manual data pipeline management.",
      "They automatically manage data dependencies and OPTIMIZE data flows.",
      "They require extensive coding for pipeline setup.",
      "They are designed only for batch data processing.",
      "They do not support data quality checks."
    ],
    "answer": 1,
    "category": "Data Pipeline Management",
    "explanation": "Option B is correct because Delta Live Tables automate data pipeline management, handling dependencies, and optimizing data flows. Option A is incorrect; DLT reduces the need for manual management. Option C is incorrect; DLT simplifies pipeline setup with declarative configurations. Option D is incorrect; DLT supports both streaming and batch data. Option E is incorrect; DLT supports data quality checks through expectations.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "In data analytics, what is 'feature engineering'?",
    "options": [
      "The process of collecting raw data.",
      "The creation of new input features from existing ones to improve model performance.",
      "The selection of machine learning algorithms.",
      "The deployment of models into production.",
      "The evaluation of model accuracy using test data."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because feature engineering involves creating new features from existing data to improve the performance of machine learning models. Options A, C, D, and E describe other stages of the machine learning workflow.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In Spark, what is the difference between 'map' and 'flatMap' transformations?",
    "options": [
      "'map' returns a new RDD by applying a function to each element; 'flatMap' does the same but flattens the result.",
      "'map' groups data; 'flatMap' sorts data.",
      "'map' filters data; 'flatMap' aggregates data.",
      "'map' is an action; 'flatMap' is a transformation.",
      "There is no difference between 'map' and 'flatMap'."
    ],
    "answer": 0,
    "category": "Spark Transformations",
    "explanation": "Option A is correct because `map` applies a function to each element and returns an RDD of the same length, while `flatMap` may return multiple elements for each input element and flattens the result into a single RDD.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, how can you display the list of files in a directory '/mnt/data'?",
    "options": [
      "dbutils.fs.ls('/mnt/data')",
      "display('/mnt/data')",
      "list('/mnt/data')",
      "dbutils.fs.list('/mnt/data')",
      "fs.ls('/mnt/data')"
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because 'dbutils.fs.ls('/mnt/data')' lists the files in the specified directory.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Delta Lake, which command can you use to view the version history and operation metrics of a table named 'inventory'?",
    "options": [
      "DESCRIBE HISTORY inventory;",
      "SHOW METRICS inventory;",
      "LIST VERSIONS FOR inventory;",
      "GET HISTORY OF inventory;",
      "DISPLAY LOGS FOR inventory;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `DESCRIBE HISTORY inventory;` shows the history of operations, including version numbers and metrics. Options B, C, D, and E are not valid Delta Lake commands for viewing history and metrics.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which of the following is a correct way to create a DataFrame in PySpark from a Python list of tuples?",
    "options": [
      "df = spark.createDataFrame(data_list)",
      "df = spark.DataFrame(data_list)",
      "df = spark.read.DataFrame(data_list)",
      "df = spark.loadDataFrame(data_list)",
      "df = spark.makeDataFrame(data_list)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `spark.createDataFrame(data_list)` creates a DataFrame from a list of tuples or a list of rows. Options B, C, D, and E are not valid methods for creating DataFrames in PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which clause is used to specify the sort order of the result set?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "SORT BY",
      "ARRANGE BY",
      "ALIGN BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `ORDER BY` is used to sort the result set in ascending or descending order. Option A is used for grouping. Option C is used in some SQL dialects but is not standard. Options D and E are not standard SQL clauses.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What does the 'collect_set' function do in Spark SQL?",
    "options": [
      "Collects all elements into a list including duplicates.",
      "Aggregates elements into a set, removing duplicates.",
      "Sorts the elements in ascending order.",
      "Collects elements and counts the frequency of each.",
      "Collects elements into a map based on a key."
    ],
    "answer": 1,
    "category": "Spark SQL Functions",
    "explanation": "Option B is correct because `collect_set` returns a set of objects with duplicate elements eliminated. Option A describes `collect_list`. Option C is not related to `collect_set`. Option D describes a counting function. Option E is incorrect; `collect_set` does not create a map.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the purpose of the 'OPTIMIZE' command in Databricks Delta Lake?",
    "options": [
      "It compresses the data to save storage space.",
      "It reorganizes the data into larger files to improve query performance.",
      "It updates the statistics for the query optimizer.",
      "It checks the table for inconsistencies and repairs corrupt data.",
      "It removes old versions of data to save storage space."
    ],
    "answer": 1,
    "category": "Delta Lake Optimization",
    "explanation": "Option B is correct because `OPTIMIZE` in Delta Lake compacts small files into larger ones, which can improve query performance. Option A is partially correct but not the main purpose. Option C is incorrect; statistics are updated with `ANALYZE TABLE`. Option D is incorrect; `FSCK REPAIR TABLE` is used for repair. Option E is incorrect; `VACUUM` removes old versions.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data processing, what is the purpose of 'data partitioning'?",
    "options": [
      "To encrypt data for security purposes.",
      "To divide data into distinct chunks based on a key for parallel processing.",
      "To merge multiple datasets into one.",
      "To backup data to multiple locations.",
      "To compress data for efficient storage."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because data partitioning involves dividing data into chunks based on keys, allowing parallel processing and improved performance. Options A, C, D, and E describe different data operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method can you use to sort a DataFrame 'df' by the column 'salary' in descending order?",
    "options": [
      "df.sort(df.salary.desc())",
      "df.orderBy('salary')",
      "df.sort_values('salary', ascending=False)",
      "df.arrange(desc('salary'))",
      "df.sortBy('salary', descending=True)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.sort(df.salary.desc())` sorts the DataFrame by 'salary' in descending order. Option B sorts in ascending order by default. Option C is a pandas method. Option D is not a valid PySpark method. Option E is not the correct syntax.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what does the term 'slowly changing dimension' (SCD) refer to?",
    "options": [
      "A table that contains only static data.",
      "A dimension table where data changes slowly over time and requires strategies to manage historical data.",
      "A fact table that accumulates data slowly.",
      "A dimension that changes so rapidly that it cannot be tracked.",
      "A temporary table used during ETL processes."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because SCD refers to dimension tables that change slowly and for which historical changes need to be tracked. Options A, C, D, and E do not accurately describe SCDs.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, what does the 'collect()' action do?",
    "options": [
      "Returns all the elements of the DataFrame to the driver node.",
      "Saves the DataFrame to a file.",
      "Performs a group by operation.",
      "Counts the number of rows in the DataFrame.",
      "Caches the DataFrame in memory."
    ],
    "answer": 0,
    "category": "Spark Actions",
    "explanation": "Option A is correct because 'collect()' retrieves all the elements of the DataFrame to the driver node, which can be useful for small datasets but should be used with caution due to memory constraints.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-constraints.html"
    ]
  },
  {
    "question": "In Spark, what is the role of an 'executor'?",
    "options": [
      "To manage the overall Spark application.",
      "To store data permanently.",
      "To run individual tasks and cache data for applications.",
      "To provide a user interface for monitoring.",
      "To compile code into executable programs."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because executors in Spark are responsible for running individual tasks on worker nodes and can cache data for the application.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, how can you access secrets stored in Azure Key Vault or AWS Secrets Manager?",
    "options": [
      "Using `dbutils.secrets.get()` with the appropriate scope and key name.",
      "Directly reading from the file system.",
      "Storing secrets in plaintext within the notebook.",
      "Using environment variables set in the cluster configuration.",
      "Secrets cannot be accessed from Databricks notebooks."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.secrets.get()` allows you to retrieve secrets from configured secret scopes securely. Options B and C are insecure or incorrect. Option D is less secure and not the standard method. Option E is incorrect; secrets can be accessed.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'MERGE INTO' command in Databricks SQL?",
    "options": [
      "To combine multiple tables into one without conditions.",
      "To insert, update, or delete records in a target table based on a source table.",
      "To remove duplicates from a table.",
      "To create a backup of a table.",
      "To split a table into multiple smaller tables."
    ],
    "answer": 1,
    "category": "SQL DML",
    "explanation": "Option B is correct because `MERGE INTO` allows you to perform upserts (insert, update, or delete operations) on a target table based on conditions applied to a source table. Option A is incorrect because merging without conditions is not the function of `MERGE INTO`. Option C is incorrect because removing duplicates is done using different methods. Option D is incorrect; `MERGE INTO` does not create backups. Option E is incorrect because splitting tables is not related to `MERGE INTO`.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Delta Lake, how do you query a previous version of a table named 'sales_data'?",
    "options": [
      "SELECT * FROM sales_data VERSION AS OF 5;",
      "SELECT * FROM sales_data@5;",
      "SELECT * FROM sales_data WHERE _version = 5;",
      "SELECT * FROM sales_data.history(5);",
      "SELECT * FROM sales_data PREVIOUS VERSION 5;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Lake supports time travel queries using `VERSION AS OF` or `TIMESTAMP AS OF`.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "What is the primary purpose of the 'HAVING' clause in SQL?",
    "options": [
      "To filter groups based on aggregate conditions after grouping.",
      "To specify the order in which results are returned.",
      "To filter rows before grouping.",
      "To join tables based on a condition.",
      "To limit the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because the `HAVING` clause is used to filter groups based on aggregate conditions after the `GROUP BY` clause has been applied. Option B describes `ORDER BY`. Option C describes the `WHERE` clause. Option D describes `JOIN` clauses. Option E describes `LIMIT`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which clause is used to specify a search condition for a group or an aggregate?",
    "options": [
      "WHERE",
      "GROUP BY",
      "HAVING",
      "ORDER BY",
      "FILTER"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `HAVING` clause is used to specify conditions on groups created by the `GROUP BY` clause, often involving aggregates. Option A filters rows before grouping. Option B is used to group rows. Option D sorts the result set. Option E is not a standard SQL clause.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'INDEX'?",
    "options": [
      "To enforce uniqueness on a column.",
      "To improve the speed of data retrieval operations.",
      "To define relationships between tables.",
      "To store large binary objects.",
      "To backup the database."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because an INDEX is used to improve the speed of data retrieval by providing quick access to rows in a database table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what is a 'fact table'?",
    "options": [
      "A table that contains historical data only.",
      "A central table in a star schema that contains quantitative data for analysis.",
      "A table that holds metadata about the database schema.",
      "A lookup table used to enforce data integrity.",
      "A table that stores unstructured data."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a fact table is the central table in a star schema, containing measurable, quantitative data (facts) for analysis. Option A is incorrect; fact tables can contain current and historical data. Option C describes a metadata table. Option D describes a dimension or lookup table. Option E is incorrect; fact tables store structured data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the function of the 'GROUP BY' clause?",
    "options": [
      "To filter records based on a condition.",
      "To group rows that have the same values in specified columns.",
      "To sort the result set.",
      "To join tables based on a related column.",
      "To limit the number of rows returned."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the 'GROUP BY' clause groups rows that have the same values in specified columns, allowing aggregate functions to be applied to each group.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which command would you use to view the history of operations performed on a Delta Lake table, including versioning information?",
    "options": [
      "DESCRIBE HISTORY my_table;",
      "SHOW TRANSACTIONS FOR my_table;",
      "SELECT * FROM my_table VERSION AS OF 'timestamp';",
      "DISPLAY HISTORY my_table;",
      "ALTER TABLE my_table SHOW HISTORY;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because the `DESCRIBE HISTORY` command in Databricks is used to display the history of operations performed on a Delta Lake table, including versioning information. Option B is incorrect because `SHOW TRANSACTIONS` is not a valid command in this context. Option C is incorrect because `SELECT * FROM my_table VERSION AS OF 'timestamp'` is used to query a specific version of the table, not to view the history. Option D is incorrect because `DISPLAY HISTORY` is not a valid command in Databricks SQL. Option E is incorrect because `ALTER TABLE my_table SHOW HISTORY` is not a valid syntax.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In the context of data visualization, which chart type is most appropriate for showing the relationship between two numerical variables?",
    "options": [
      "Pie Chart",
      "Bar Chart",
      "Scatter Plot",
      "Histogram",
      "Line Chart"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a scatter plot is ideal for displaying the relationship between two numerical variables. Option A is incorrect; pie charts are for parts of a whole. Option B is incorrect; bar charts are for categorical data. Option D is incorrect; histograms show distribution of a single variable. Option E is incorrect; line charts show trends over time.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command would you use in SQL to remove all records from a table named 'logs' without deleting the table itself?",
    "options": [
      "DROP TABLE logs;",
      "DELETE FROM logs;",
      "TRUNCATE TABLE logs;",
      "REMOVE * FROM logs;",
      "CLEAR TABLE logs;"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `TRUNCATE TABLE logs;` removes all records from the table efficiently without deleting the table structure. Option A deletes the entire table. Option B deletes records but can be slower and may not reset identity counters. Options D and E are not valid SQL commands.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'precision' in the context of classification metrics?",
    "options": [
      "The ratio of true positives to all predicted positives.",
      "The ratio of true positives to all actual positives.",
      "The ratio of true negatives to all predicted negatives.",
      "The overall accuracy of the model.",
      "The ability of the model to find all relevant cases within a dataset."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because precision measures the proportion of true positives among all positive predictions made by the model.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In machine learning, what is 'cross-validation' used for?",
    "options": [
      "To increase the size of the training dataset.",
      "To assess how the results of a model will generalize to an independent dataset.",
      "To test multiple models simultaneously.",
      "To adjust the model parameters to overfit the training data.",
      "To visualize the performance of a model."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because cross-validation is a technique for evaluating how a model will perform on unseen data by partitioning the data into training and validation sets. Options A, C, D, and E are not accurate descriptions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the default file format for Delta Lake tables in Databricks?",
    "options": [
      "CSV",
      "JSON",
      "ORC",
      "Parquet",
      "Avro"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "Option D is correct because Delta Lake uses Parquet as the underlying file format, adding transaction logs for ACID properties. Options A, B, C, and E are not the default formats for Delta Lake.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how can you list all the tables available in the current database?",
    "options": [
      "SHOW TABLES;",
      "LIST ALL TABLES;",
      "DISPLAY TABLES;",
      "SELECT * FROM information_schema.tables;",
      "GET TABLES;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `SHOW TABLES;` is the standard SQL command to list all tables in the current database. Option B is not standard SQL. Option C is not a valid SQL command. Option D may not be supported in all SQL dialects or require additional permissions. Option E is not a valid SQL command.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'workspace' used for?",
    "options": [
      "Managing data storage and retrieval.",
      "Configuring clusters and resources.",
      "Developing notebooks and organizing projects.",
      "Monitoring jobs and workflows.",
      "Setting up security policies."
    ],
    "answer": 2,
    "category": "Databricks Features",
    "explanation": "Option C is correct because the workspace in Databricks is where users develop notebooks, organize projects, and collaborate on code. Options A, B, D, and E are managed in other areas of Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Jobs API' used for?",
    "options": [
      "Interacting with jobs programmatically to create, run, and manage them.",
      "Accessing and modifying database tables.",
      "Monitoring cluster health and performance.",
      "Implementing security protocols.",
      "Developing custom visualization tools."
    ],
    "answer": 0,
    "category": "Databricks APIs",
    "explanation": "Option A is correct because the Jobs API allows users to programmatically manage Databricks jobs, including creation and execution.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In SQL, what is a 'correlated subquery'?",
    "options": [
      "A subquery that can be executed independently of the outer query.",
      "A subquery that depends on values from the outer query.",
      "A subquery that uses aggregation functions.",
      "A subquery that joins multiple tables.",
      "A subquery that only returns a single value."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because a correlated subquery is one that uses values from the outer query and must be re-evaluated for each row in the outer query. Option A describes an independent subquery. Options C, D, and E do not specifically define a correlated subquery.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data science, what is 'bias-variance tradeoff'?",
    "options": [
      "The compromise between the complexity of a model and its performance on unseen data.",
      "The adjustment of model parameters to reduce training time.",
      "The balance between the size of the dataset and the number of features.",
      "The process of selecting important features to include in a model.",
      "The tradeoff between model accuracy and computational resources."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because the bias-variance tradeoff refers to the balance between a model's ability to generalize to new data (variance) and its accuracy on the training data (bias).",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of machine learning, what is 'overfitting'?",
    "options": [
      "When a model performs well on training data but poorly on new, unseen data.",
      "When a model is too simple to capture underlying patterns in data.",
      "When a model has an error due to randomness in the data.",
      "When a model's predictions are consistently biased in one direction.",
      "When a model's parameters are too large to be processed efficiently."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data. Option B describes underfitting. Option C refers to variance or noise. Option D refers to bias. Option E is about computational efficiency, not overfitting.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data modeling, what is a 'fact table'?",
    "options": [
      "A table that contains dimension data.",
      "A table that stores detailed business measurements or metrics.",
      "A temporary table used during data loading.",
      "A table that contains metadata about the database.",
      "A table used for logging database changes."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a fact table stores quantitative data for analysis and is typically at the center of a star schema in a data warehouse.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'Notebook' primarily used for?",
    "options": [
      "Scheduling jobs and workflows.",
      "Managing clusters and resources.",
      "Writing code, visualizing results, and documenting analysis interactively.",
      "Configuring user permissions.",
      "Storing raw data files."
    ],
    "answer": 2,
    "category": "Databricks Notebooks",
    "explanation": "Option C is correct because Databricks Notebooks are interactive environments where users can write code, visualize results, and document their analysis. Options A, B, D, and E are managed elsewhere in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html",
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'FileStore' directory used for?",
    "options": [
      "Storing libraries and dependencies.",
      "Sharing files between the driver and worker nodes.",
      "Persisting data generated by notebooks for access via web URLs.",
      "Logging cluster activity.",
      "Storing configuration files."
    ],
    "answer": 2,
    "category": "Databricks Utilities",
    "explanation": "Option C is correct because the 'FileStore' directory in Databricks is used to store files that can be accessed via web URLs, making it easy to share results.",
    "sources": [
      "https://docs.databricks.com/en/sharing/index.html"
    ]
  },
  {
    "question": "What is the primary benefit of 'data caching' in Spark?",
    "options": [
      "It reduces storage costs.",
      "It allows data to be shared between different applications.",
      "It improves performance by keeping frequently accessed data in memory.",
      "It ensures data is stored securely.",
      "It automatically replicates data across clusters."
    ],
    "answer": 2,
    "category": "Spark Optimization",
    "explanation": "Option C is correct because caching data in memory can significantly improve performance for iterative algorithms and interactive queries by avoiding recomputation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, what is the primary use of a 'choropleth map'?",
    "options": [
      "To display changes over time.",
      "To represent data values through variations in color on a map.",
      "To show relationships between two variables.",
      "To compare quantities across different categories.",
      "To display hierarchical data."
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a choropleth map represents data values by varying the color intensity on geographic regions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main advantage of using 'vectorized UDFs' (User Defined Functions) in PySpark?",
    "options": [
      "They allow the use of SQL syntax within PySpark code.",
      "They improve performance by processing data in batches using pandas UDFs.",
      "They enable the use of Scala code in PySpark applications.",
      "They automatically parallelize Python code without any additional effort.",
      "They provide built-in functions for machine learning tasks."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because vectorized UDFs (also known as pandas UDFs) in PySpark allow for high-performance data processing by utilizing Apache Arrow and pandas for vectorized operations. Option A is incorrect. Option C refers to language interoperability. Option D is partially true but not the main advantage. Option E is incorrect; vectorized UDFs are not built-in ML functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you read a JSON file located at '/data/records.json' into a DataFrame?",
    "options": [
      "df = spark.read.json('/data/records.json')",
      "df = spark.read.format('json').load('/data/records.json')",
      "df = spark.read('/data/records.json', format='json')",
      "df = spark.load.json('/data/records.json')",
      "Both options A and B are correct"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because both `spark.read.json()` and `spark.read.format('json').load()` are valid methods to read a JSON file into a DataFrame.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a characteristic of Structured Streaming in Spark?",
    "options": [
      "It processes data in real-time using micro-batches.",
      "It requires data to be fully loaded before processing.",
      "It does not support fault tolerance.",
      "It can only read data from static sources.",
      "It cannot be integrated with Delta Lake."
    ],
    "answer": 0,
    "category": "Streaming Data",
    "explanation": "Option A is correct because Structured Streaming uses micro-batching to process streaming data in near real-time. Option B is incorrect; it processes data as it arrives. Option C is incorrect; it supports fault tolerance. Option D is incorrect; it can read from streaming sources. Option E is incorrect; it can integrate with Delta Lake.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In PySpark, which method allows you to read data from a JDBC source into a DataFrame?",
    "options": [
      "spark.read.jdbc(url, table, properties)",
      "spark.read.format('jdbc').load(url, table, properties)",
      "spark.read.jdbc(url, table)",
      "spark.read.jdbcSource(url, table, properties)",
      "spark.read.fromJdbc(url, table, properties)"
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Option A is correct because `spark.read.jdbc(url, table, properties)` reads data from a JDBC source into a DataFrame.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is a 'broadcast variable' used for?",
    "options": [
      "To share large read-only data efficiently across all worker nodes.",
      "To stream data in real-time to the Spark application.",
      "To collect results from all worker nodes back to the driver.",
      "To partition data based on a hash function.",
      "To cache data in memory for faster access."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. Option B describes streaming. Option C is related to actions like `collect()`. Option D describes partitioning. Option E describes caching.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which function is used to aggregate data after grouping, similar to SQL's GROUP BY clause?",
    "options": [
      "df.groupBy().agg()",
      "df.aggregate()",
      "df.rollup()",
      "df.cube()",
      "df.groupby().sum()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.groupBy().agg()` allows for grouping and aggregation. Option B is not a standard method. Option C and D are used for hierarchical aggregations. Option E is valid but limited to the sum function.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, what does the 'broadcast' function do when used in a join operation?",
    "options": [
      "It distributes the smaller DataFrame to all worker nodes to OPTIMIZE the join.",
      "It broadcasts the result of the join to all clients.",
      "It partitions the data across different nodes.",
      "It collects data to the driver node.",
      "It sorts the DataFrame before joining."
    ],
    "answer": 0,
    "category": "Spark Optimization",
    "explanation": "Option A is correct because the 'broadcast' function hints Spark to broadcast the smaller DataFrame to all executor nodes, which can greatly improve the performance of joins when one of the DataFrames is small.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes 'data skew' in big data processing?",
    "options": [
      "Even distribution of data across partitions.",
      "Inefficient data serialization formats.",
      "Uneven distribution of data leading to performance bottlenecks.",
      "Data that is encrypted for security purposes.",
      "Outliers in data that affect statistical analysis."
    ],
    "answer": 2,
    "category": "Data Processing",
    "explanation": "Option C is correct because data skew refers to the uneven distribution of data across partitions, which can lead to some tasks taking much longer than others, causing performance issues. Option A is the opposite of data skew. Option B refers to serialization inefficiencies. Option D is about data security. Option E refers to statistical outliers.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command in Databricks allows you to install libraries on a cluster?",
    "options": [
      "%install library",
      "dbutils.library.install()",
      "pip install",
      "conda install",
      "Cluster libraries UI"
    ],
    "answer": 4,
    "category": "Databricks Utilities",
    "explanation": "Option E is correct because you can install libraries on a Databricks cluster using the Cluster libraries UI in the Databricks workspace. Option A is not a valid Databricks command. Option B is deprecated. Option C and D are package managers but not the recommended way to install libraries on clusters in Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data analysis, what does the term 'data lineage' refer to?",
    "options": [
      "The chronological order of data entries in a database.",
      "The historical record of data origins, movements, and transformations.",
      "The family tree of database schemas and tables.",
      "The process of cleaning and organizing raw data.",
      "The relationship between primary and foreign keys in tables."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data lineage tracks the lifecycle of data, including its origins, movements, characteristics, and transformations over time. Option A is incorrect; data lineage is not just about chronological entries. Option C is incorrect; it's not about schema hierarchies. Option D is incorrect; that's data cleansing. Option E is incorrect; data lineage is broader than key relationships.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html"
    ]
  },
  {
    "question": "In Databricks, which cluster mode is optimized for concurrent execution of multiple tasks and provides automatic resource management?",
    "options": [
      "Standard Mode",
      "High Concurrency Mode",
      "Single Node Mode",
      "Local Mode",
      "Interactive Mode"
    ],
    "answer": 1,
    "category": "Databricks Clusters",
    "explanation": "Option B is correct because High Concurrency clusters are optimized for concurrent execution and provide features like automatic resource management and improved security. Options A, C, D, and E are not optimized in the same way.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following commands in Databricks SQL would you use to create a new database named 'sales_db'?",
    "options": [
      "CREATE DATABASE sales_db;",
      "NEW DATABASE sales_db;",
      "MAKE DATABASE sales_db;",
      "ADD DATABASE sales_db;",
      "INIT DATABASE sales_db;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `CREATE DATABASE sales_db;` is the standard SQL command to create a new database. Options B, C, D, and E are not valid SQL commands for creating databases.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'Magic Commands' (e.g., %sql, %python)?",
    "options": [
      "To perform file system operations.",
      "To switch between different programming languages within a notebook.",
      "To configure cluster settings.",
      "To manage libraries and dependencies.",
      "To control access permissions."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because Magic Commands in Databricks notebooks allow users to switch between languages like SQL, Python, R, etc., within different cells.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data processing, what is 'serialization'?",
    "options": [
      "The process of converting an object into a format that can be stored or transmitted.",
      "The process of compressing data to save space.",
      "The process of encrypting data for security.",
      "The process of cleaning data.",
      "The process of dividing data into partitions."
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because serialization involves converting an object into a format that can be stored or transmitted and reconstructed later.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, which method would you use to read a Parquet file located at '/data/parquet_files' into a DataFrame?",
    "options": [
      "df = spark.read.parquet('/data/parquet_files')",
      "df = spark.read.format('parquet').load('/data/parquet_files')",
      "df = spark.read('/data/parquet_files')",
      "df = spark.read.load('/data/parquet_files', format='parquet')",
      "All of the above except option C"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because options A, B, and D are valid methods to read Parquet files in PySpark, whereas option C is incomplete. Option A uses `read.parquet`. Option B uses `read.format().load()`. Option D uses `read.load()` with the format specified.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which built-in function would you use to remove leading and trailing whitespace from a string column 'customer_name' in a SELECT statement?",
    "options": [
      "CLEAN(customer_name)",
      "TRIM(customer_name)",
      "STRIP(customer_name)",
      "REMOVE_WHITESPACE(customer_name)",
      "LTRIM(customer_name)"
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because the `TRIM` function removes both leading and trailing whitespace from a string. Option A is incorrect because `CLEAN` is not a standard SQL function in Databricks. Option C is incorrect because `STRIP` is not a standard SQL function in Databricks SQL. Option D is incorrect because `REMOVE_WHITESPACE` is not a recognized function. Option E is incorrect because `LTRIM` removes only leading whitespace, not trailing whitespace.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In PySpark, how do you replace all occurrences of a substring 'old' with 'new' in column 'text' of DataFrame 'df'?",
    "options": [
      "df.withColumn('text', regexp_replace('text', 'old', 'new'))",
      "df.replace('old', 'new', 'text')",
      "df.update('text', replace('old', 'new'))",
      "df.withColumn('text', translate('text', 'old', 'new'))",
      "df.transform('text', 'old', 'new')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `regexp_replace` can be used to replace all occurrences of a substring in a column. Option B is used for value replacement but not within strings. Option C is not a valid PySpark method. Option D replaces characters, not substrings. Option E is not a standard method.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When dealing with skewed data in a Spark DataFrame, which of the following techniques can help improve performance?",
    "options": [
      "Repartition the DataFrame to a single partition.",
      "Use broadcast joins to distribute data evenly.",
      "Increase the number of shuffle partitions to distribute data more evenly.",
      "Cache the DataFrame before performing any operations.",
      "Disable catalyst optimizer to prevent query optimization."
    ],
    "answer": 2,
    "category": "Spark Performance Optimization",
    "explanation": "Option C is correct because increasing the number of shuffle partitions can help distribute skewed data more evenly across the cluster. Option A is incorrect because repartitioning to a single partition can worsen performance. Option B is partially correct but broadcast joins are effective when one of the datasets is small. Option D is incorrect because caching doesn't resolve data skew. Option E is incorrect; disabling the optimizer generally reduces performance.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "When configuring a Databricks SQL warehouses, which of the following settings directly impacts both performance and cost?",
    "options": [
      "The number of users connected to the warehouse.",
      "The size of the warehouse cluster (e.g., Small, Medium, Large).",
      "The region where the warehouse is deployed.",
      "The default database selected in the warehouse configuration.",
      "The color theme of the Databricks SQL UI."
    ],
    "answer": 1,
    "category": "Databricks SQL warehouses",
    "explanation": "Option B is correct because the size of the warehouse cluster determines the computational resources allocated, which directly affects both performance and cost. Larger clusters offer better performance but at a higher cost. Option A is incorrect because while the number of users may affect performance due to concurrency, it does not directly impact cost unless it leads to scaling up the cluster. Option C is incorrect because the region affects latency and data residency but not directly performance and cost in this context. Option D is incorrect because the default database does not impact performance or cost. Option E is incorrect because UI color themes have no impact on performance or cost.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Community Edition'?",
    "options": [
      "A free version of Databricks with limited features for learning and experimentation.",
      "A version of Databricks intended for enterprise deployment.",
      "An open-source alternative to Databricks.",
      "A plugin for integrating Databricks with community forums.",
      "A special edition for academic institutions."
    ],
    "answer": 0,
    "category": "Databricks Features",
    "explanation": "Option A is correct because the Databricks Community Edition is a free version that provides limited features and resources for users to learn and experiment with Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main purpose of the 'explode' function in Spark SQL?",
    "options": [
      "To flatten an array or map column into multiple rows.",
      "To split a string into multiple columns.",
      "To merge multiple columns into a struct.",
      "To aggregate multiple rows into an array.",
      "To remove null values from a DataFrame."
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `explode` turns each element of an array or each key-value pair of a map into separate rows. Options B, C, D, and E describe different functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you convert a DataFrame 'df' to a temporary view named 'temp_view'?",
    "options": [
      "df.createTempView('temp_view')",
      "df.registerTempView('temp_view')",
      "df.createOrReplaceTempView('temp_view')",
      "All of the above",
      "None of the above"
    ],
    "answer": 3,
    "category": "Data Transformation",
    "explanation": "Option D is correct because both `createTempView` and `createOrReplaceTempView` can be used to create a temporary view from a DataFrame. `registerTempView` is deprecated but still works in some versions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In Databricks, what is the 'Delta Live Tables' feature?",
    "options": [
      "A tool for creating and managing streaming pipelines with minimal coding.",
      "A visualization library for real-time dashboards.",
      "A feature for automated machine learning model selection.",
      "An interface for managing user permissions.",
      "A storage optimization technique."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "What is the role of the 'SparkContext' in a Spark application?",
    "options": [
      "To execute tasks assigned by the driver.",
      "To coordinate the execution of tasks and manage resources.",
      "To store data permanently.",
      "To provide an interface for programming with Spark.",
      "To manage the cluster nodes."
    ],
    "answer": 3,
    "category": "Spark Concepts",
    "explanation": "Option D is correct because `SparkContext` is the entry point for a Spark application, allowing the programmer to interact with the Spark cluster.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which function allows you to display a DataFrame 'df' in a tabular format within a notebook cell?",
    "options": [
      "show(df)",
      "display(df)",
      "print(df)",
      "render(df)",
      "visualize(df)"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because the `display(df)` function is specific to Databricks notebooks and renders the DataFrame in a tabular format with interactive features. Option A (`show(df)`) is incorrect syntax; the correct usage is `df.show()`, which prints the DataFrame but without interactive features. Option C would attempt to print the DataFrame object, leading to less readable output. Options D and E are not valid functions in this context.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In Delta Lake, what is the default retention period for the 'VACUUM' command to protect against accidental data loss?",
    "options": [
      "7 days",
      "1 day",
      "30 days",
      "0 hours",
      "168 hours"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because the default retention period for `VACUUM` in Delta Lake is 7 days to prevent accidental data deletion. Option B is incorrect; although 1 day can be specified, it's not the default. Option C is incorrect. Option D (0 hours) is not allowed without setting a configuration. Option E is equivalent to 7 days but less commonly used.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, how would you grant a user named 'data_analyst' SELECT permissions on a table named 'sales_data' in the database 'sales_db'?",
    "options": [
      "GRANT SELECT ON sales_data TO data_analyst;",
      "GRANT SELECT ON TABLE sales_db.sales_data TO USER data_analyst;",
      "GRANT SELECT TO data_analyst ON sales_db.sales_data;",
      "ALTER TABLE sales_db.sales_data ADD PERMISSION SELECT TO data_analyst;",
      "GIVE data_analyst SELECT RIGHTS ON sales_db.sales_data;"
    ],
    "answer": 1,
    "category": "Access Control",
    "explanation": "Option B is correct because the syntax `GRANT SELECT ON TABLE database.table TO USER username;` is used to grant permissions. Option A is incorrect because it lacks the 'TABLE' keyword and database reference. Option C is incorrect syntax. Option D is incorrect because `ALTER TABLE` does not manage permissions this way. Option E is incorrect; `GIVE` is not a valid SQL command for permissions.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'NULLIF' function do?",
    "options": [
      "Returns NULL if the two expressions are equal, otherwise returns the first expression.",
      "Returns NULL if the expression is NULL, otherwise returns the expression.",
      "Replaces NULL values with a specified replacement value.",
      "Checks if an expression is NULL.",
      "Compares two expressions and returns TRUE if both are NULL."
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `NULLIF(expr1, expr2)` returns NULL if `expr1` equals `expr2`; otherwise, it returns `expr1`. Option B does not describe `NULLIF`. Option C describes `ISNULL` or `COALESCE`. Option D describes `IS NULL`. Option E is not the function of `NULLIF`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of using Databricks Workflows over standard scheduling tools?",
    "options": [
      "Support for only SQL-based tasks.",
      "Integration with Git repositories is not supported.",
      "Ability to orchestrate complex data pipelines with dependencies between tasks.",
      "Limited to running on a single cluster.",
      "Inability to send notifications upon task completion."
    ],
    "answer": 2,
    "category": "Job Management",
    "explanation": "Option C is correct because Databricks Workflows allow for orchestration of complex data pipelines, managing dependencies between various tasks. Option A is incorrect; Workflows support multiple task types. Option B is incorrect; Git integration is supported. Option D is incorrect; Workflows can run on multiple clusters. Option E is incorrect; notifications can be configured.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'workspace' area?",
    "options": [
      "To manage data storage and retrieval.",
      "To develop and organize notebooks, libraries, and experiments.",
      "To monitor cluster performance and logs.",
      "To configure user permissions and access control.",
      "To schedule and manage jobs."
    ],
    "answer": 1,
    "category": "Databricks Architecture",
    "explanation": "Option B is correct because the workspace is where users develop notebooks, manage libraries, and organize experiments. Options A, C, D, and E are managed in different areas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In Spark, what is a 'DataFrame'?",
    "options": [
      "A distributed collection of data organized into named columns.",
      "A single-node data structure for storing arrays.",
      "A graphical representation of data.",
      "A file format for storing structured data.",
      "A machine learning model."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because a DataFrame in Spark is a distributed collection of data organized into named columns, similar to a table in a relational database.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils.secrets' module?",
    "options": [
      "To manage and access encrypted secrets like passwords and keys.",
      "To perform file system operations.",
      "To create interactive widgets.",
      "To submit jobs programmatically.",
      "To monitor cluster performance."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because 'dbutils.secrets' provides utilities for managing secrets within Databricks, allowing secure access to sensitive credentials.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which method in PySpark is used to union two DataFrames 'df1' and 'df2' with the same schema?",
    "options": [
      "df1.union(df2)",
      "df1.join(df2)",
      "df1.merge(df2)",
      "df1.concat(df2)",
      "df1.append(df2)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df1.union(df2)' combines the rows of both DataFrames as long as they have the same schema. Option B performs a join operation based on keys. Option C is not a standard PySpark method. Option D is used in pandas, not PySpark. Option E is not a standard method in PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which command would you use to create a temporary view named 'temp_sales' based on a SELECT query?",
    "options": [
      "CREATE TEMPORARY VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE GLOBAL TEMPORARY VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE TEMP VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE TEMPORARY TABLE temp_sales AS SELECT * FROM sales_data;"
    ],
    "answer": 0,
    "category": "SQL Views",
    "explanation": "Option A is correct because the syntax `CREATE TEMPORARY VIEW` creates a temporary view accessible in the current session. Option B is incorrect because it creates a persistent view, not temporary. Option C is incorrect because `CREATE GLOBAL TEMPORARY VIEW` creates a temporary view accessible across all sessions but may not be desired. Option D is incorrect because `CREATE TEMP VIEW` is not standard syntax. Option E is incorrect because `CREATE TEMPORARY TABLE` creates a temporary table, not a view.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In SQL, what does the 'EXCEPT' operator do?",
    "options": [
      "Returns all distinct rows selected by both queries.",
      "Combines the results of two queries, including duplicates.",
      "Returns rows from the first query that are not present in the second query.",
      "Returns the intersection of two queries.",
      "Joins two tables based on a foreign key relationship."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `EXCEPT` returns rows from the first query that are not in the second query. Option A describes `INTERSECT`. Option B describes `UNION ALL`. Option D is incorrect. Option E describes a JOIN operation.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following best describes the concept of 'idempotency' in RESTful APIs?",
    "options": [
      "An operation that always returns the same result when called multiple times.",
      "An operation that cannot be reversed once executed.",
      "An operation that requires authentication.",
      "An operation that is only executed once per client.",
      "An operation that increases system load proportionally with each call."
    ],
    "answer": 0,
    "category": "API Concepts",
    "explanation": "Option A is correct because idempotent operations produce the same result regardless of how many times they are executed. Options B, C, D, and E do not accurately describe idempotency.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of the 'display()' function when used in a notebook?",
    "options": [
      "To print text output to the console.",
      "To render DataFrames as interactive tables or visualizations.",
      "To execute shell commands.",
      "To save a DataFrame to a file.",
      "To log messages for debugging purposes."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because `display()` renders DataFrames as interactive displays in notebooks, allowing for sorting, filtering, and visualization. Option A is incorrect; `print()` is used for text output. Option C is incorrect; `%sh` is used for shell commands. Option D is incorrect; saving is done with `write` methods. Option E is incorrect; logging is done differently.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "Which window function would you use in SQL to assign a unique sequential integer to rows within a partition, without any gaps in numbering?",
    "options": [
      "RANK()",
      "DENSE_RANK()",
      "ROW_NUMBER()",
      "NTILE(n)",
      "LEAD()"
    ],
    "answer": 2,
    "category": "SQL Window Functions",
    "explanation": "Option C is correct because `ROW_NUMBER()` assigns a unique sequential integer to rows within a partition. Option A (`RANK()`) assigns the same rank to ties, resulting in gaps. Option B (`DENSE_RANK()`) assigns the same rank to ties without gaps but may not be sequential. Option D (`NTILE(n)`) distributes rows into n buckets. Option E (`LEAD()`) accesses data from subsequent rows.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which function would you use to extract the year from a timestamp column named 'order_date'?",
    "options": [
      "YEAR(order_date)",
      "EXTRACT(YEAR FROM order_date)",
      "GET_YEAR(order_date)",
      "DATEPART('year', order_date)",
      "FORMAT_DATE('yyyy', order_date)"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `EXTRACT(YEAR FROM order_date)` is standard SQL syntax supported in Databricks SQL. Option A is not standard SQL but may work in some dialects. Option C is incorrect; `GET_YEAR` is not a standard function. Option D is incorrect; `DATEPART` is T-SQL syntax. Option E is incorrect; `FORMAT_DATE` returns a string representation.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Databricks, what is the purpose of the 'display()' function in a notebook?",
    "options": [
      "To print text output to the console.",
      "To visualize DataFrames as interactive tables or charts.",
      "To log messages for debugging purposes.",
      "To execute shell commands.",
      "To display images stored in the file system."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because 'display()' in Databricks notebooks renders DataFrames as interactive tables and supports visualization.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In data analytics, what does 'ETL' stand for?",
    "options": [
      "Extract, Transform, Load",
      "Evaluate, Train, Learn",
      "Execute, Test, Launch",
      "Encrypt, Transfer, Log",
      "Estimate, Tune, Loop"
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because ETL stands for Extract, Transform, Load, which is a process used to collect data from various sources, transform it into a suitable format, and load it into a destination system.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'CREATE DATABASE' statement in SQL?",
    "options": [
      "To create a new table within an existing database.",
      "To create a new database schema.",
      "To create a backup of an existing database.",
      "To delete an existing database.",
      "To update the structure of an existing database."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `CREATE DATABASE` is used to create a new database schema. Option A is done using `CREATE TABLE`. Option C is not the function of `CREATE DATABASE`. Option D is done using `DROP DATABASE`. Option E is done using `ALTER DATABASE`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following statements about Databricks Notebooks is true?",
    "options": [
      "Notebooks can only be written in Python.",
      "Notebooks support multiple languages within the same notebook using magic commands.",
      "Notebooks cannot be shared between users.",
      "Notebooks are only accessible through the Databricks CLI.",
      "Notebooks do not support version control integration."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because Databricks Notebooks allow users to use multiple languages in the same notebook by using magic commands like `%python`, `%sql`, `%scala`, `%r`. Option A is incorrect because multiple languages are supported. Option C is incorrect; notebooks can be shared. Option D is incorrect; notebooks are accessible through the web UI. Option E is incorrect; Databricks supports version control integration.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, how can you limit the number of rows returned by a query to 10?",
    "options": [
      "Use `LIMIT 10` at the end of the query.",
      "Set `MAX_ROWS = 10` in the query.",
      "Include `ROWCOUNT 10` in the SELECT statement.",
      "Use `TOP 10` before the column names.",
      "Configure the dashboard settings to display only 10 rows."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because adding `LIMIT 10` at the end of the query restricts the result set to 10 rows. Option B is incorrect; `MAX_ROWS` is not standard SQL syntax. Option C is incorrect; `ROWCOUNT` is not used in this way. Option D is partially correct; `TOP 10` is valid in some SQL dialects like T-SQL but may not be supported in Databricks SQL. Option E is incorrect; dashboard settings do not affect the query's result set.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In machine learning, what is 'Gradient Descent'?",
    "options": [
      "An optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent.",
      "A statistical method for classification problems.",
      "A technique for dimensionality reduction.",
      "An algorithm for clustering data points.",
      "A method for data normalization."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because Gradient Descent is an optimization algorithm that minimizes a function by iteratively moving towards the steepest descent as defined by the negative of the gradient.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark SQL, what does the 'OVER' clause do in window functions?",
    "options": [
      "Defines the partitioning and ordering of rows.",
      "Filters rows based on a condition.",
      "Groups rows into buckets.",
      "Sorts the result set.",
      "Limits the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because the `OVER` clause specifies how to partition and order rows for window functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In the context of data storage, what does the acronym 'ACID' stand for?",
    "options": [
      "Analysis, Computation, Integration, Delivery",
      "Atomicity, Consistency, Isolation, Durability",
      "Automation, Control, Integrity, Deployment",
      "Access, Connectivity, Integration, Data",
      "Algorithm, Calculation, Input, Data"
    ],
    "answer": 1,
    "category": "Data Management",
    "explanation": "Option B is correct because ACID stands for Atomicity, Consistency, Isolation, Durability, which are properties that guarantee reliable processing of database transactions. Other options do not correctly represent ACID.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, what is a 'box plot' used for?",
    "options": [
      "To show the distribution of a dataset based on five summary statistics.",
      "To compare parts of a whole.",
      "To display frequencies of different categories.",
      "To plot individual data points in a two-dimensional space.",
      "To show changes over time."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a box plot displays the distribution of data based on minimum, first quartile, median, third quartile, and maximum values.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the difference between the WHERE and HAVING clauses?",
    "options": [
      "WHERE filters rows before aggregation; HAVING filters groups after aggregation.",
      "HAVING filters rows before aggregation; WHERE filters groups after aggregation.",
      "They are interchangeable and have no difference.",
      "WHERE is used only in DELETE statements; HAVING is used in SELECT statements.",
      "WHERE filters columns; HAVING filters rows."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because WHERE filters data before aggregation occurs, while HAVING filters the aggregated data (groups). Option B is incorrect; it reverses their functions. Option C is incorrect; they have distinct purposes. Option D is incorrect; WHERE is used in multiple statement types. Option E is incorrect; WHERE filters rows, not columns.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data analysis, what is 'normalization' typically used for?",
    "options": [
      "Reducing redundancy in database tables.",
      "Scaling numerical data to a common range.",
      "Transforming categorical data into numerical data.",
      "Increasing data variance for better analysis.",
      "Encrypting data for secure storage."
    ],
    "answer": 1,
    "category": "Data Preprocessing",
    "explanation": "Option B is correct because normalization in data analysis often refers to scaling numerical data to a common range, such as [0,1], to improve the performance of machine learning algorithms. Option A is more related to database normalization. Option C describes encoding. Option D is the opposite effect. Option E is about data security.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a characteristic of a 'managed table' in Databricks?",
    "options": [
      "Data is stored externally, and Databricks only manages metadata.",
      "Data and metadata are both managed by Databricks, and data is stored in the default storage location.",
      "Only the data is managed by Databricks; metadata is managed externally.",
      "The table schema cannot be altered after creation.",
      "The table supports only read operations."
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because in a managed table, both the data and metadata are managed by Databricks, and the data is stored in the default storage location. Option A describes an unmanaged (external) table. Option C is incorrect; both data and metadata are managed. Option D is incorrect; schemas can be altered. Option E is incorrect; managed tables support all CRUD operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "Which of the following is a characteristic of a 'NoSQL' database?",
    "options": [
      "It uses structured query language for data manipulation.",
      "It is best suited for relational data models.",
      "It provides flexible schemas and can handle unstructured data.",
      "It does not support distributed computing.",
      "It enforces ACID transactions strictly."
    ],
    "answer": 2,
    "category": "Data Storage",
    "explanation": "Option C is correct because NoSQL databases are designed for flexible schemas and can handle unstructured or semi-structured data. Options A and B describe SQL databases. Option D is incorrect; many NoSQL databases support distributed computing. Option E is incorrect; NoSQL databases often relax ACID properties in favor of performance and scalability.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data analytics, what is the difference between 'classification' and 'regression' in machine learning?",
    "options": [
      "Classification predicts continuous numerical values; regression predicts categorical outcomes.",
      "Classification predicts categorical outcomes; regression predicts continuous numerical values.",
      "They are the same; both predict future data points.",
      "Classification groups data; regression splits data.",
      "Regression is used for clustering; classification is used for association rules."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because classification algorithms predict categorical outcomes (e.g., yes/no), while regression algorithms predict continuous numerical values (e.g., price). Option A is incorrect; it reverses the definitions. Option C is incorrect; they serve different purposes. Option D is incorrect; grouping and splitting are not accurate descriptions. Option E is incorrect; regression is not used for clustering.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, what is a 'Cluster Policy'?",
    "options": [
      "A set of rules to automate cluster creation and termination.",
      "A template that restricts the configuration options available to users when creating clusters.",
      "A method to assign users to specific clusters.",
      "A policy that encrypts data on clusters.",
      "A monitoring tool for cluster performance."
    ],
    "answer": 1,
    "category": "Databricks Administration",
    "explanation": "Option B is correct because Cluster Policies in Databricks are templates that limit the configuration options available to users, ensuring compliance and cost control.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, which chart type is best suited for showing parts of a whole as percentages?",
    "options": [
      "Bar Chart",
      "Line Chart",
      "Pie Chart",
      "Histogram",
      "Scatter Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a pie chart represents data in a circular graph, showing parts of a whole as slices of the pie, useful for percentages. Option A is better for comparing quantities across categories. Option B is for trends over time. Option D displays distribution of a single variable. Option E shows relationships between two numerical variables.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the difference between 'DELETE' and 'TRUNCATE'?",
    "options": [
      "'DELETE' removes all rows and cannot be rolled back; 'TRUNCATE' removes specific rows and can be rolled back.",
      "'DELETE' removes specific rows and can be rolled back; 'TRUNCATE' removes all rows and cannot be rolled back in some systems.",
      "There is no difference; they perform the same action.",
      "'DELETE' removes the table structure; 'TRUNCATE' only removes data.",
      "'DELETE' is faster than 'TRUNCATE'."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'DELETE' removes specific rows and can be rolled back if within a transaction, while 'TRUNCATE' removes all rows and may not be rolled back depending on the system.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a benefit of using Delta Lake for data storage?",
    "options": [
      "It does not support ACID transactions.",
      "It allows for schema enforcement and evolution.",
      "It requires proprietary hardware to operate.",
      "It does not integrate with Apache Spark.",
      "It lacks support for time travel queries."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because Delta Lake provides schema enforcement and evolution, allowing for robust data pipelines. Option A is incorrect; Delta Lake supports ACID transactions. Option C is incorrect; it runs on commodity hardware. Option D is incorrect; Delta Lake is built to work with Spark. Option E is incorrect; Delta Lake supports time travel.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data visualization, what does a 'scatter plot' represent?",
    "options": [
      "The distribution of a single variable.",
      "The relationship between two numerical variables.",
      "The composition of categories within a whole.",
      "The trend of data over time.",
      "The hierarchical structure of data."
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a scatter plot displays values for typically two variables for a set of data, showing the relationship or correlation between them.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is a 'stored procedure'?",
    "options": [
      "A prepared SQL statement that is stored on the client side.",
      "A function that returns a single value.",
      "A batch of SQL statements that can be executed as a program on the database server.",
      "A temporary table used during query execution.",
      "A constraint applied to a table to enforce data integrity."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because a stored procedure is a set of SQL statements that can be stored in the database and executed as a program.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, which chart type is best for showing the trend of data over a continuous time interval?",
    "options": [
      "Bar Chart",
      "Pie Chart",
      "Line Chart",
      "Scatter Plot",
      "Histogram"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because line charts are ideal for showing trends over continuous time intervals. Option A is better for comparing quantities across categories. Option B shows parts of a whole. Option D is for relationships between two variables. Option E shows the distribution of a single variable.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which method would you use to save a DataFrame 'df' as a Delta table named 'sales_data' in the default database?",
    "options": [
      "df.write.format('delta').saveAsTable('sales_data')",
      "df.saveAsDelta('sales_data')",
      "df.write.delta('sales_data')",
      "df.createDeltaTable('sales_data')",
      "df.write.saveTable('sales_data')"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `df.write.format('delta').saveAsTable('sales_data')` saves the DataFrame as a Delta table. Options B, C, D, and E are not standard methods for saving DataFrames as Delta tables.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In data visualization, what is a primary use case for a 'box plot'?",
    "options": [
      "To display the frequency distribution of a dataset.",
      "To show the relationship between two categorical variables.",
      "To represent the distribution of a dataset through quartiles, highlighting the median and outliers.",
      "To compare proportions of categories within a whole.",
      "To visualize changes over time for multiple categories."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a box plot (or box-and-whisker plot) displays the distribution of data based on a five-number summary: minimum, first quartile, median, third quartile, and maximum, highlighting outliers. Option A is better suited for histograms. Option B is suited for heatmaps or clustered bar charts. Option D is for pie charts. Option E is for line charts or area charts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data modeling, what is 'denormalization'?",
    "options": [
      "The process of organizing data to reduce redundancy.",
      "The process of adding redundant data to OPTIMIZE read performance.",
      "Encrypting data to secure it.",
      "Partitioning data across multiple tables.",
      "Converting unstructured data into structured form."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because denormalization involves adding redundant data to database tables to improve read performance at the expense of write performance and storage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'HAVING' clause do when used in a SELECT statement with GROUP BY?",
    "options": [
      "It filters rows before grouping.",
      "It filters groups after aggregation.",
      "It sorts the result set.",
      "It specifies the columns to group by.",
      "It limits the number of rows returned."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `HAVING` clause filters groups after the `GROUP BY` and aggregation have been performed. Option A describes the `WHERE` clause. Option C describes `ORDER BY`. Option D is the function of `GROUP BY`. Option E is achieved using `LIMIT`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data modeling, what is 'dimensional modeling'?",
    "options": [
      "A technique used to reduce the number of dimensions in data.",
      "A design concept used to build data warehouses for easier reporting and analysis.",
      "A method for encrypting data in databases.",
      "A way to normalize data to the third normal form.",
      "A process of creating flat files from relational databases."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because dimensional modeling is a design approach for data warehouses that optimizes for query performance and ease of use, often involving star or snowflake schemas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the main advantage of using the Parquet file format in big data processing?",
    "options": [
      "It is a human-readable text format.",
      "It supports complex data types like arrays and nested structures.",
      "It stores data in row-based format, improving write performance.",
      "It is proprietary and offers enhanced security features.",
      "It automatically indexes data for faster querying."
    ],
    "answer": 1,
    "category": "Data Storage Formats",
    "explanation": "Option B is correct because Parquet supports complex data types and nested structures, making it suitable for big data processing. Option A is incorrect; Parquet is a binary format. Option C is incorrect; Parquet is columnar, not row-based. Option D is incorrect; Parquet is open-source. Option E is incorrect; while Parquet may improve query performance due to its columnar nature, it doesn't automatically index data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is a key feature of 'High Concurrency' clusters in Databricks?",
    "options": [
      "They support only a single user at a time.",
      "They are optimized for interactive workloads with low latency.",
      "They do not support Apache Spark jobs.",
      "They require manual scaling of resources.",
      "They do not provide built-in security features."
    ],
    "answer": 1,
    "category": "Databricks Clusters",
    "explanation": "Option B is correct because High Concurrency clusters in Databricks are optimized for concurrent interactive workloads, providing low-latency performance and built-in security features.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the purpose of the 'DROP' command in SQL?",
    "options": [
      "To delete data from a table.",
      "To remove a table or database entirely.",
      "To truncate data in a table.",
      "To remove duplicates from a result set.",
      "To rollback a transaction."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `DROP` command is used to remove an entire table or database, including its structure and data. Option A is done with `DELETE`. Option C uses `TRUNCATE`. Option D is achieved with `DISTINCT`. Option E uses the `ROLLBACK` command.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data analysis, what is the 'curse of dimensionality'?",
    "options": [
      "The phenomenon where adding more data improves model performance indefinitely.",
      "The difficulties that arise when analyzing data in high-dimensional spaces.",
      "The problem of having too few data points to make accurate predictions.",
      "The tendency for data to become more correlated as dimensions increase.",
      "The issue of data privacy becoming more complex with more features."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because the 'curse of dimensionality' refers to various phenomena that arise when analyzing data with a large number of features, making it sparse and harder to model. Options A, C, D, and E are incorrect interpretations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'CASE' statement?",
    "options": [
      "To perform conditional logic in queries, similar to if-else statements.",
      "To group rows based on a condition.",
      "To create a temporary table.",
      "To enforce uniqueness on a column.",
      "To define a transaction block."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because the `CASE` statement allows you to add conditional logic within SQL queries, functioning like an if-else construct. Options B, C, D, and E are not purposes of the `CASE` statement.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data visualization, when is it most appropriate to use a 'histogram'?",
    "options": [
      "To show the distribution of numerical data.",
      "To compare parts of a whole.",
      "To display trends over time.",
      "To represent relationships between two variables.",
      "To map data geographically."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a histogram is used to display the distribution of numerical data by showing the number of data points that fall within specified ranges (bins).",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is 'Data Lakehouse' in the context of data architecture?",
    "options": [
      "A centralized repository for structured data only.",
      "An architecture that combines features of data lakes and data warehouses.",
      "A methodology for processing streaming data exclusively.",
      "A type of database optimized for OLTP workloads.",
      "A tool for data visualization."
    ],
    "answer": 1,
    "category": "Data Architecture",
    "explanation": "Option B is correct because a Data Lakehouse combines the scalability and flexibility of data lakes with the ACID transactions and schema enforcement of data warehouses.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, how do you rename an existing table 'old_table' to 'new_table'?",
    "options": [
      "RENAME TABLE old_table TO new_table;",
      "ALTER TABLE old_table RENAME TO new_table;",
      "MODIFY TABLE old_table TO new_table;",
      "CHANGE TABLE old_table TO new_table;",
      "UPDATE TABLE old_table SET NAME = new_table;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'ALTER TABLE old_table RENAME TO new_table;' is the standard SQL syntax for renaming a table. Option A is not standard in SQL. Options C, D, and E are incorrect.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is a 'Transformation'?",
    "options": [
      "An operation that returns a new RDD/DataFrame and is lazily evaluated.",
      "An action that triggers computation and returns a value.",
      "A method to persist data in memory.",
      "An operation to repartition data.",
      "A function to collect data to the driver."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because transformations create a new RDD or DataFrame from an existing one and are lazily evaluated, meaning they are not computed until an action is called.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary function of the 'EXISTS' clause in a SQL query?",
    "options": [
      "To check if a table exists in the database.",
      "To return true if a subquery returns any rows.",
      "To join two tables based on a condition.",
      "To select unique values from a column.",
      "To aggregate data over a specified window."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `EXISTS` clause is used to test for the existence of any record in a subquery; it returns true if the subquery returns one or more records. Option A is incorrect; checking if a table exists is done differently. Option C is incorrect; joining tables is done with JOIN clauses. Option D is incorrect; selecting unique values is done with `DISTINCT`. Option E is incorrect; window functions are used for aggregation over windows.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what does the term 'star schema' refer to?",
    "options": [
      "A schema with multiple fact tables connected to a single dimension table.",
      "A central fact table connected to multiple dimension tables in a shape resembling a star.",
      "A schema where all tables are interconnected without a central fact table.",
      "A design that involves nested subqueries for data retrieval.",
      "A schema that supports only real-time data processing."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because in a star schema, a central fact table is connected to multiple dimension tables, resembling a star shape. Option A is incorrect; typically, there is one fact table. Option C describes a network or mesh schema. Option D is about query design, not schema. Option E is incorrect; star schemas are used in various processing scenarios.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary difference between the functions COLLECT_LIST() and COLLECT_SET() in Spark SQL?",
    "options": [
      "COLLECT_LIST() returns unique elements; COLLECT_SET() allows duplicates.",
      "COLLECT_LIST() maintains order and duplicates; COLLECT_SET() returns unique elements in random order.",
      "COLLECT_LIST() sorts the elements; COLLECT_SET() does not.",
      "There is no difference; both functions behave identically.",
      "COLLECT_LIST() works only with numeric data; COLLECT_SET() works with all data types."
    ],
    "answer": 1,
    "category": "Spark SQL Functions",
    "explanation": "Option B is correct because `COLLECT_LIST()` returns an array including duplicates and maintains the order, while `COLLECT_SET()` returns an array of unique elements without guaranteed order. Option A is incorrect because it's the opposite. Option C is incorrect because neither function sorts elements. Option D is incorrect because they do behave differently. Option E is incorrect because both functions work with all data types.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which function would you use to extract the month from a date column 'order_date'?",
    "options": [
      "MONTH(order_date)",
      "EXTRACT(MONTH FROM order_date)",
      "DATEPART('month', order_date)",
      "TO_MONTH(order_date)",
      "GETMONTH(order_date)"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `EXTRACT(MONTH FROM order_date)` is the standard SQL syntax for extracting the month from a date. Option A may work in some SQL dialects but is not standard SQL. Option C is T-SQL specific. Option D and E are not standard SQL functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In PySpark, how do you write a DataFrame 'df' to a CSV file at '/output/data.csv' with headers included?",
    "options": [
      "df.write.csv('/output/data.csv', header=True)",
      "df.write.format('csv').save('/output/data.csv')",
      "df.to_csv('/output/data.csv', header=True)",
      "df.saveAsCsv('/output/data.csv', header=True)",
      "df.exportCsv('/output/data.csv', header=True)"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because `df.write.csv('/output/data.csv', header=True)` writes the DataFrame to a CSV file with headers.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what does the 'LIKE' operator do in a WHERE clause?",
    "options": [
      "It checks if a value is equal to another value.",
      "It searches for a specified pattern in a column.",
      "It compares two columns for similarity.",
      "It sorts the result set based on a column.",
      "It groups rows that have the same values."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `LIKE` operator is used in a WHERE clause to search for a specified pattern in a column. Option A describes `=`. Option C is not accurate; `LIKE` searches for patterns, not similarity. Option D describes `ORDER BY`. Option E describes `GROUP BY`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, what is the effect of the following command? ```sql CREATE OR REPLACE TABLE my_table AS SELECT * FROM source_table; ```",
    "options": [
      "It creates 'my_table' only if it does not exist; otherwise, it does nothing.",
      "It appends data from 'source_table' into 'my_table'.",
      "It drops 'my_table' if it exists and then creates a new 'my_table' with data from 'source_table'.",
      "It merges data from 'source_table' into 'my_table'.",
      "It creates a view named 'my_table' based on 'source_table'."
    ],
    "answer": 2,
    "category": "SQL DDL",
    "explanation": "Option C is correct because `CREATE OR REPLACE TABLE` will drop the existing table if it exists and create a new one with the data from the `SELECT` statement. Option A is incorrect; that behavior is for `CREATE TABLE IF NOT EXISTS`. Option B is incorrect; it does not append but replaces. Option D is incorrect; `MERGE INTO` is used for merging. Option E is incorrect; it creates a table, not a view.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html",
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "In SQL, what is the purpose of the 'UNION ALL' operator?",
    "options": [
      "To combine the results of two SELECT statements and remove duplicates.",
      "To combine the results of two SELECT statements including duplicates.",
      "To find the intersection of two SELECT statements.",
      "To subtract one SELECT statement from another.",
      "To join two tables based on a common column."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `UNION ALL` combines the results of two SELECT statements and includes all duplicates. Option A describes `UNION`. Option C is the purpose of `INTERSECT`. Option D describes `EXCEPT` or `MINUS`. Option E is about JOIN operations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Delta Lake, which command is used to clean up invalid data files and reduce storage costs?",
    "options": [
      "CLEAN TABLE my_table;",
      "VACUUM my_table;",
      "PURGE TABLE my_table;",
      "OPTIMIZE my_table;",
      "REFRESH TABLE my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because `VACUUM my_table;` removes unreferenced files and helps reduce storage costs. Option A is not a valid command. Option C is not standard in Delta Lake. Option D is used to OPTIMIZE data layout but not to remove files. Option E refreshes metadata but does not remove files.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In SQL, which keyword is used to fetch only a specified number of records from a table?",
    "options": [
      "FETCH",
      "LIMIT",
      "TOP",
      "ROWNUM",
      "All of the above, depending on the SQL dialect"
    ],
    "answer": 4,
    "category": "SQL",
    "explanation": "Option E is correct because different SQL dialects use different keywords to limit records: `LIMIT` (MySQL, PostgreSQL), `TOP` (SQL Server), `ROWNUM` (Oracle), and `FETCH` (SQL:2008 standard).",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which visualization type in Databricks SQL is most appropriate for showing the distribution of a single numerical variable?",
    "options": [
      "Line Chart",
      "Bar Chart",
      "Histogram",
      "Pie Chart",
      "Scatter Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a histogram is used to represent the distribution of a single numerical variable by grouping data into bins. Option A is incorrect because line charts are best for trends over time. Option B is incorrect because bar charts compare different categories. Option D is incorrect because pie charts show parts of a whole. Option E is incorrect because scatter plots show relationships between two numerical variables.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In Spark, what is the role of 'SparkSession'?",
    "options": [
      "It is the entry point to programming Spark with the Dataset and DataFrame API.",
      "It manages the cluster resources and node allocation.",
      "It is responsible for executing tasks on worker nodes.",
      "It provides a user interface for monitoring Spark jobs.",
      "It stores metadata about the data sources."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because `SparkSession` is the entry point for using the DataFrame and Dataset API in Spark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data modeling, what is a 'dimension table'?",
    "options": [
      "A table that contains business facts or measures.",
      "A table that contains historical transaction data.",
      "A table that stores detailed transactional data.",
      "A table that contains attributes to describe business entities.",
      "A temporary table used during ETL processes."
    ],
    "answer": 3,
    "category": "Data Modeling",
    "explanation": "Option D is correct because a dimension table contains attributes that describe business entities, providing context to the data in fact tables. Option A describes a fact table. Option B is not specific to dimension tables. Option C also describes a fact table. Option E is unrelated.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is the primary role of the 'driver' in a Spark application?",
    "options": [
      "To store data across the cluster nodes.",
      "To execute tasks assigned by the executors.",
      "To coordinate the execution of tasks and manage the SparkContext.",
      "To provide a user interface for monitoring jobs.",
      "To cache data in memory for quick access."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because the driver program coordinates all the tasks, manages the SparkContext, and schedules tasks on executors. Option A describes data storage. Option B is incorrect; executors execute tasks assigned by the driver. Option D refers to the Spark UI. Option E describes caching, which is managed by executors.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In data visualization, what is the main use of a 'heatmap'?",
    "options": [
      "To show the distribution of a single variable.",
      "To compare parts of a whole.",
      "To display data values as colors in a matrix.",
      "To represent data on a geographical map.",
      "To track changes over time."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a heatmap displays data values as colors within a matrix, allowing for quick visual identification of patterns and correlations.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks, which of the following statements about Unity Catalog is accurate?",
    "options": [
      "Unity Catalog is a data storage system for raw data files.",
      "Unity Catalog provides centralized governance for data and AI assets across clouds.",
      "Unity Catalog is a visualization tool for creating dashboards.",
      "Unity Catalog is a feature for real-time data streaming into Delta Lake.",
      "Unity Catalog manages only machine learning models and their versions."
    ],
    "answer": 1,
    "category": "Unity Catalog",
    "explanation": "Option B is correct because Unity Catalog is designed to provide centralized governance, managing access controls, and audit information across data and AI assets in Databricks. Options A, C, D, and E are incorrect because they do not accurately describe Unity Catalog's purpose.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "In SQL, what is the function of the 'EXPLAIN' statement?",
    "options": [
      "To execute the query and display the results.",
      "To provide a description of the table schema.",
      "To show the execution plan of a query without executing it.",
      "To OPTIMIZE the query for better performance.",
      "To create a view based on the query."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `EXPLAIN` provides the execution plan for a query, helping developers understand how the database will execute it without actually running the query.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, what is the difference between 'DELETE' and 'TRUNCATE' commands?",
    "options": [
      "'DELETE' removes all rows; 'TRUNCATE' removes specific rows based on a condition.",
      "'DELETE' removes specific rows; 'TRUNCATE' removes all rows and cannot be rolled back in some systems.",
      "There is no difference; they are interchangeable.",
      "'DELETE' drops the table structure; 'TRUNCATE' only removes data.",
      "'DELETE' requires more storage space than 'TRUNCATE'."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'DELETE' removes specific rows and can be rolled back, while 'TRUNCATE' removes all rows, often cannot be rolled back, and is faster. Option A is incorrect. Option C is incorrect; they are not interchangeable. Option D is incorrect; 'DELETE' does not drop the table. Option E is misleading.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "What is a common reason to use the 'coalesce()' function in Spark?",
    "options": [
      "To combine multiple DataFrames into one.",
      "To increase the number of partitions in a DataFrame.",
      "To reduce the number of partitions in a DataFrame, often after filtering.",
      "To remove null values from a DataFrame.",
      "To cache a DataFrame in memory for faster access."
    ],
    "answer": 2,
    "category": "Spark Optimization",
    "explanation": "Option C is correct because `coalesce()` reduces the number of partitions, which can be useful after a filter operation that significantly reduces data size. Option A is done with `union()` or `join()`. Option B is done with `repartition()`. Option D is achieved with `na.drop()` or filtering. Option E is done with `cache()`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which command in Delta Lake is used to combine small files into larger ones to improve read performance?",
    "options": [
      "COMPACT TABLE my_table;",
      "OPTIMIZE my_table;",
      "MERGE FILES IN my_table;",
      "CONSOLIDATE my_table;",
      "VACUUM my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because `OPTIMIZE my_table;` compacts small files into larger ones in Delta Lake, improving read performance. Option A is not a valid command. Option C and D are incorrect; these commands do not exist in Delta Lake. Option E (`VACUUM`) removes unreferenced files but does not compact files.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/optimize.html#z-order-by-1",
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "In Databricks, how can you read data from a JSON file stored in DBFS into a DataFrame using PySpark?",
    "options": [
      "df = spark.read.format('json').load('/path/to/file.json')",
      "df = spark.read.json('/path/to/file.json')",
      "df = spark.load.json('/path/to/file.json')",
      "df = spark.readData('/path/to/file.json', format='json')",
      "Both A and B are correct"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because both Option A and Option B are valid ways to read a JSON file into a DataFrame using PySpark in Databricks. Option C is incorrect because `spark.load.json` is not a valid method. Option D is incorrect because `readData` is not a standard PySpark method. Option A and B are standard ways to read JSON files.",
    "sources": [
      "https://docs.databricks.com/en/dbfs/index.html"
    ]
  },
  {
    "question": "What is the default behavior of the 'JOIN' keyword in SQL when no type (e.g., INNER, LEFT) is specified?",
    "options": [
      "It performs an INNER JOIN.",
      "It performs a LEFT JOIN.",
      "It performs a RIGHT JOIN.",
      "It performs a FULL OUTER JOIN.",
      "An error occurs due to missing JOIN type."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because by default, `JOIN` without a specified type performs an INNER JOIN. Options B, C, and D are incorrect. Option E is incorrect; most SQL dialects default to INNER JOIN without errors.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how can you add a constant column named 'status' with value 'active' to DataFrame 'df'?",
    "options": [
      "df.withColumn('status', 'active')",
      "df.withColumn('status', lit('active'))",
      "df.addColumn('status', 'active')",
      "df.withConstantColumn('status', 'active')",
      "df.insertColumn('status', 'active')"
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because `lit('active')` creates a column with a constant value in PySpark. Option A would attempt to add a column with a string column reference, not a constant value. Options C, D, and E are not valid PySpark methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what is 'data mart'?",
    "options": [
      "A large centralized repository of data.",
      "A subset of a data warehouse focused on a particular subject or department.",
      "A system for real-time data processing.",
      "An unstructured data storage system.",
      "A process for data cleaning and transformation."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a data mart is a subset of a data warehouse, typically focused on a single subject area or department within an organization. Option A describes a data warehouse. Options C, D, and E are unrelated to data marts.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Spark, what is a 'checkpoint' used for?",
    "options": [
      "To save the current state of an RDD to reliable storage.",
      "To cache data in memory for faster access.",
      "To partition data across nodes.",
      "To collect data back to the driver.",
      "To OPTIMIZE query execution plans."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because checkpointing saves the state of an RDD or DataFrame to reliable storage (like HDFS) to prevent data loss and recomputation in case of failures. Options B, C, D, and E describe different Spark functionalities.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is a key characteristic of a 'distributed file system' like HDFS?",
    "options": [
      "Data is stored on a single machine for quick access.",
      "It does not support redundancy or fault tolerance.",
      "Data is spread across multiple machines to enable parallel processing.",
      "It requires proprietary hardware to function.",
      "It is only suitable for small datasets."
    ],
    "answer": 2,
    "category": "Distributed Systems",
    "explanation": "Option C is correct because distributed file systems like HDFS store data across multiple machines, enabling parallel processing and fault tolerance. Option A is incorrect. Option B is incorrect; HDFS supports redundancy. Option D is incorrect; HDFS runs on commodity hardware. Option E is incorrect; it's designed for large datasets.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In data warehousing, what is a 'conformed dimension'?",
    "options": [
      "A dimension that is unique to a single fact table.",
      "A dimension that has multiple hierarchies.",
      "A dimension that is shared across multiple fact tables or data marts.",
      "A dimension that changes rapidly over time.",
      "A dimension that contains calculated measures."
    ],
    "answer": 2,
    "category": "Data Modeling",
    "explanation": "Option C is correct because a conformed dimension is consistent and shared across multiple fact tables or data marts, ensuring data integrity and uniform reporting.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In Databricks SQL, which function would you use to concatenate two strings 'first_name' and 'last_name' with a space in between?",
    "options": [
      "CONCAT(first_name, ' ', last_name)",
      "first_name || ' ' || last_name",
      "CONCATENATE(first_name, ' ', last_name)",
      "CONCAT_WS(' ', first_name, last_name)",
      "All of the above except option C"
    ],
    "answer": 4,
    "category": "SQL Functions",
    "explanation": "Option E is correct because options A, B, and D are valid methods to concatenate strings in SQL, depending on the SQL dialect. Option C is not a standard SQL function.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "In SQL, which keyword is used to remove duplicate rows from the result set of a SELECT query?",
    "options": [
      "UNIQUE",
      "DISTINCT",
      "DELETE DUPLICATES",
      "FILTER",
      "GROUP BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `DISTINCT` keyword ensures that duplicate rows are removed from the result set. Option A (`UNIQUE`) is not used in SELECT statements. Option C is not a valid SQL command. Option D (`FILTER`) is not used for removing duplicates. Option E (`GROUP BY`) groups rows but does not remove duplicates unless combined with aggregate functions.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In SQL, which function would you use to return the current date and time?",
    "options": [
      "GETDATE()",
      "CURRENT_TIMESTAMP",
      "NOW()",
      "TODAY()",
      "DATE()"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `CURRENT_TIMESTAMP` returns the current date and time in SQL. Option A (`GETDATE()`) is specific to SQL Server. Option C (`NOW()`) is used in MySQL. Option D (`TODAY()`) returns only the date, not time. Option E (`DATE()`) typically converts a datetime to a date.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In a Databricks notebook, what is the main purpose of the MAGIC command '%sql'?",
    "options": [
      "To execute operating system commands from within the notebook.",
      "To switch the notebook's default language to SQL.",
      "To load SQL-specific libraries into the notebook environment.",
      "To comment out a block of code in the notebook.",
      "To render SQL query results as interactive visualizations."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because the MAGIC command `%sql` tells the notebook to interpret the following cell as SQL code. Option A is incorrect; that's done with `%sh`. Option C is incorrect; libraries are imported differently. Option D is incorrect; comments are made with `--` or `/* */`. Option E is incorrect; while `%sql` can be used before a query that can be visualized, the main purpose is to execute SQL code.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "In SQL, which operator would you use to check if a value exists within a set of values?",
    "options": [
      "BETWEEN",
      "LIKE",
      "IN",
      "EXISTS",
      "ANY"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `IN` operator is used to check if a value exists within a specified set of values.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-constraints.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para criar um índice em uma table?",
    "options": [
      "CREATE INDEX",
      "ADD INDEX",
      "GENERATE INDEX",
      "MAKE INDEX"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'CREATE INDEX' é usado para criar um índice em uma ou mais columns de uma table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Caching' no contexto do Spark?",
    "options": [
      "Armazenar data em memória para acesso rápido",
      "Excluir data não utilizados",
      "Compactar arquivos de data",
      "Dividir data em partições"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "O 'Caching' armazena data frequentemente acessados em memória para melhorar o desempenho das querys.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which função SQL retorna o maior valor de uma column numérica?",
    "options": [
      "SUM()",
      "COUNT()",
      "MIN()",
      "MAX()"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "A função 'MAX()' retorna o maior valor encontrado em uma column específica.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para combinar registros de duas tables com base em uma column comum?",
    "options": [
      "UNION",
      "JOIN",
      "GROUP BY",
      "INTERSECT"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'JOIN' é utilizado para combinar registros de duas tables com base em uma column comum.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'COUNT()' no SQL?",
    "options": [
      "Somar todos os valores de uma column",
      "Contar o número de rows em uma table ou resultado de query",
      "Encontrar o valor mínimo em uma column",
      "Calcular a média dos valores em uma column"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A função 'COUNT()' retorna o número de rows what correspondem a um critério específico.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what significa ETL em Data Engineering?",
    "options": [
      "Extract, Transform, Load",
      "Evaluate, Test, Launch",
      "Edit, Transfer, Link",
      "Encrypt, Transfer, Log"
    ],
    "answer": 0,
    "category": "Data Engineering",
    "explanation": "ETL significa 'Extract, Transform, Load', what são as etapas para mover e transformar data entre sistemas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'DATEDIFF()' no SQL?",
    "options": [
      "Calcular a diferenca entre duas datas",
      "Formatar uma data em um determinado padrao",
      "Adicionar dias a uma data",
      "Extrair o dia da semana de uma data"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A funcao 'DATEDIFF()' e usada para calcular a diferenca entre duas datas especificadas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Pipeline de data' no Databricks?",
    "options": [
      "Um modelo de Machine Learning",
      "Uma sequência de etapas para ingestão, processamento e saída de data",
      "Uma técnica de otimização de querys",
      "Um sistema de storage de data"
    ],
    "answer": 1,
    "category": "Data Engineering",
    "explanation": "'Pipeline de data' é uma sequência de operações what extrai, transforma e carrega data em um sistema.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what significa 'time travel' no contexto do Delta Lake?",
    "options": [
      "Permite restaurar data de backups",
      "Permite consultar versões anteriores dos data",
      "Aumenta a velocidade das querys",
      "Fornece segurança para data antigos"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Time travel' permite acessar versões anteriores de data no Delta Lake para auditoria ou recuperação.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'SUBSTRING()' no SQL?",
    "options": [
      "Converter texto para maiusculas",
      "Extrair uma parte de uma string",
      "Substituir caracteres em uma string",
      "Concatenar duas strings"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A funcao 'SUBSTRING()' e usada para extrair uma porcao especifica de uma string com base em posicao e comprimento.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para inserir novos data em uma table?",
    "options": [
      "INSERT INTO",
      "ADD DATA",
      "UPDATE TABLE",
      "APPEND ROW"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'INSERT INTO' é usado para adicionar novos registros a uma table existente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Cluster Manager' no Spark?",
    "options": [
      "Um componente what gerencia recursos e agendamento de tarefas",
      "Uma ferramenta de view de data",
      "Um módulo para executar querys SQL",
      "Um sistema de storage de data"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "O 'Cluster Manager' gerencia os recursos do cluster e agenda a execucao de aplicativos Spark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Catalyst Optimizer' no Spark SQL?",
    "options": [
      "Um módulo para visualizar data",
      "O mecanismo de otimização de querys do Spark SQL",
      "Um componente para gerenciamento de clusters",
      "Uma ferramenta de seguranca"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "O 'Catalyst Optimizer' é o mecanismo interno do Spark SQL what otimiza o plano de execução de querys.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'DataFrame API' no Spark?",
    "options": [
      "Uma interface para manipulação de data estruturados",
      "Uma ferramenta de view de data",
      "Um modulo de seguranca",
      "Uma linguagem de programacao proprietaria"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "A 'DataFrame API' permite manipular data estruturados de forma eficiente, semelhante a tables em bancos de data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Spark, o what é um 'DataSet'?",
    "options": [
      "Uma coleção distribuída de data organizada em columns nomeadas com tipagem forte",
      "Um tipo de banco de data relacional",
      "Um arquivo de configuracao do Spark",
      "Uma biblioteca de view de data"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "Um 'DataSet' é uma coleção distribuída de data com tipagem forte, combinando as vantagens de RDDs e DataFrames.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é um 'Accumulator' no Spark?",
    "options": [
      "Uma variavel somente leitura compartilhada entre executores",
      "Uma variável what permite operações de soma agregada",
      "Um módulo de cache de data",
      "Uma funcao para criar DataFrames"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "Um 'Accumulator' é usado para realizar operações de agregação, how somas, de forma paralela entre os executores.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Spark, o what é uma 'Wide Transformation'?",
    "options": [
      "Uma transformação what afeta apenas uma partição",
      "Uma transformação what requer 'shuffling' dos data entre executores",
      "Uma transformação what reduz o número de partições",
      "Uma transformação what aumenta o paralelismo"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "Uma 'Wide Transformation' envolve operações what requerem 'shuffling', redistribuindo data entre partições e executores.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Structured Streaming' no Spark?",
    "options": [
      "Uma API para processamento de fluxos de data em tempo real",
      "Uma ferramenta de agendamento de tarefas",
      "Um módulo para análise de data estruturados",
      "Uma biblioteca de view"
    ],
    "answer": 0,
    "category": "Streaming",
    "explanation": "O 'Structured Streaming' é uma API what permite o processamento de fluxos de data de forma contínua e escalável.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Apache Spark, o what é 'Broadcast Join'?",
    "options": [
      "Um join what replica uma pequena table para todos os nós",
      "Um join what distribui data igualmente entre nós",
      "Um join what só funciona com data de streaming",
      "Um join what ignora data duplicados"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'Broadcast Join' é uma técnica where uma pequena table é enviada para todos os nós para otimizar o join com uma table grande.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Lazy Evaluation' no contexto do Spark?",
    "options": [
      "Execucao imediata de todas as operacoes",
      "Adiar a execução até what seja necessária",
      "Um metodo de otimizacao de memoria",
      "Uma técnica para limpar data"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "'Lazy Evaluation' significa what o Spark adia a execução das operações até what uma ação seja chamada.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Databricks, o what é um 'workspace'?",
    "options": [
      "Um ambiente colaborativo para projetos",
      "Uma maquina virtual individual",
      "Um tipo de cluster de computacao",
      "Uma biblioteca de funcoes pre-construidas"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "Um 'workspace' é um ambiente where usuários podem colaborar em notebooks, trabalhos e outros recursos.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Cluster Auto-Scaling' no Databricks?",
    "options": [
      "A capacidade de ajustar automaticamente o tamanho do cluster com base na carga de trabalho",
      "Um metodo de seguranca de clusters",
      "Uma ferramenta para criar visualizacoes",
      "Um serviço para armazenar data"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "'Cluster Auto-Scaling' permite what o Databricks aumente ou diminua automaticamente os recursos do cluster conforme necessário.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Delta Engine' no contexto do Databricks?",
    "options": [
      "Um mecanismo de storage de data",
      "Uma biblioteca de Machine Learning",
      "Um mecanismo de processamento otimizado para Delta Lake",
      "Uma ferramenta de view de data"
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "O 'Delta Engine' é um mecanismo de processamento de querys de alto desempenho otimizado para trabalhar com o Delta Lake.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "O what é 'Schema Enforcement' no Delta Lake?",
    "options": [
      "Permitir qualquer tipo de data sem restrições",
      "Garantir what os data escritos correspondam ao schema definido",
      "Atualizar automaticamente o schema com base nos data",
      "Remover columns não utilizadas"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Schema Enforcement' assegura what apenas data compatíveis com o schema definido sejam gravados na table Delta.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "O what é 'DataFrame' no Apache Spark?",
    "options": [
      "Uma coleção distribuída de data organizada em columns nomeadas",
      "Um tipo de banco de data relacional",
      "Um arquivo de configuracao do Spark",
      "Uma biblioteca de view de data"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "Um DataFrame é uma estrutura de data do Spark what armazena data tabulares em columns nomeadas, similar a uma table SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Schema Evolution' no Delta Lake?",
    "options": [
      "A capacidade de alterar o schema de data ao longo do tempo",
      "Uma técnica de otimização de querys",
      "Um método de particionamento de data",
      "Uma ferramenta de segurança de data"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "O 'Schema Evolution' permite what o schema de uma table Delta seja alterado automaticamente para acomodar novos data.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'Photon'?",
    "options": [
      "Um acelerador de querys SQL de alto desempenho",
      "Uma linguagem de programacao",
      "Um sistema de storage de data",
      "Um servico de aprendizado de maquina"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "O 'Photon' é um mecanismo nativo de execução de querys SQL what oferece alto desempenho no Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é um 'Notebook Widget'?",
    "options": [
      "Uma ferramenta para agendar tarefas",
      "Um recurso para adicionar interatividade aos notebooks",
      "Um tipo de cluster de computacao",
      "Uma biblioteca para view de data"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Os 'Notebook Widgets' permitem adicionar elementos interativos, how caixas de seleção e menus suspensos, aos notebooks.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "No contexto do Spark, o what é um 'Resilient Distributed Dataset' (RDD)?",
    "options": [
      "Uma colecao imutavel de objetos distribuidos",
      "Um tipo de banco de data relacional",
      "Uma funcao para processamento de streams",
      "Um mecanismo de storage em cache"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "Um RDD é uma coleção distribuída e imutável de objetos what podem ser processados em paralelo.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Databricks File System' (DBFS)?",
    "options": [
      "Um sistema de arquivos distribuido",
      "Uma interface de row de comando",
      "Uma biblioteca de Machine Learning",
      "Um banco de data relacional"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "O DBFS é um sistema de arquivos distribuído what permite o acesso a data armazenados em nuvem how se fossem locais.",
    "sources": [
      "https://docs.databricks.com/en/dbfs/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'DBFS Fuse'?",
    "options": [
      "Um sistema de arquivos virtual what permite acessar o DBFS localmente",
      "Uma ferramenta de seguranca de arquivos",
      "Um formato de arquivo proprietario",
      "Um módulo para streaming de data"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "'DBFS Fuse' permite montar o Databricks File System (DBFS) em sistemas locais, tornando os arquivos acessiveis via comandos do sistema operacional.",
    "sources": [
      "https://docs.databricks.com/en/dbfs/index.html"
    ]
  },
  {
    "question": "O what é 'Data Partitioning' no Spark?",
    "options": [
      "Dividir os data em partes para processamento paralelo",
      "Compactar data para economizar espaço",
      "Combinar multiplos datasets",
      "Transformar data em formato binário"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "O 'Data Partitioning' divide os data em partições menores para what possam ser processados em paralelo pelos executores do Spark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Databricks, o what é o 'Databricks Runtime'?",
    "options": [
      "Um sistema operacional personalizado",
      "Uma distribuicao otimizada do Apache Spark",
      "Uma linguagem de programacao proprietaria",
      "Um banco de data relacional"
    ],
    "answer": 1,
    "category": "Conceitos do Lakehouse",
    "explanation": "O Databricks Runtime é uma distribuição otimizada do Apache Spark what inclui melhorias de desempenho e funcionalidades adicionais.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto de view de data, o what é um 'Heatmap'?",
    "options": [
      "Um gráfico what mostra a distribuição de data geográficos",
      "Uma representação gráfica what usa cores para mostrar a magnitude dos valores",
      "Um gráfico what conecta pontos de data em uma row",
      "Um tipo de grafico de barras vertical"
    ],
    "answer": 1,
    "category": "view de data",
    "explanation": "Um 'Heatmap' usa cores para representar a magnitude dos valores em uma matriz bidimensional.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'EXPLAIN' no SQL?",
    "options": [
      "Executar uma query",
      "Fornecer o plano de execução de uma query",
      "Atualizar data em uma table",
      "Criar uma nova table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'EXPLAIN' mostra o plano de execução what o banco de data usará para executar uma query.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a vantagem de usar 'Photon' no Databricks?",
    "options": [
      "Aumenta a segurança dos data",
      "Melhora o desempenho de querys SQL",
      "Simplifica a criacao de pipelines",
      "Permite gerenciamento de permissoes"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "'Photon' é um mecanismo de execução nativo no Databricks what otimiza o desempenho de querys SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what significa 'APIs de Alto Nível' no Spark?",
    "options": [
      "APIs what interagem diretamente com o código binário",
      "APIs what fornecem abstrações how DataFrames e Datasets",
      "APIs para gerenciar clusters",
      "APIs para view de data"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "As 'APIs de Alto Nível' do Spark incluem DataFrames e Datasets, what facilitam a manipulação de data estruturados.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é um 'External Table' no contexto do SQL?",
    "options": [
      "Uma table armazenada fora do banco de data",
      "Uma visão temporária de data",
      "Uma table what referencia data externos sem importá-los",
      "Uma table what existe apenas durante a sessão atual"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "Uma 'External Table' referencia data armazenados fora do banco de data, permitindo querys sem importação.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "No Databricks, which recurso permite versionar código dentro de notebooks?",
    "options": [
      "Notebook Revisions",
      "Git Integration",
      "Cluster Snapshots",
      "Version Control System"
    ],
    "answer": 1,
    "category": "Conceitos do Lakehouse",
    "explanation": "A integracao com Git permite versionar e controlar o codigo dentro dos notebooks do Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é um 'Executor' no contexto do Spark?",
    "options": [
      "O processo what coordena o cluster",
      "Um processo responsável por executar tarefas e armazenar data",
      "Uma interface de usuario para monitorar jobs",
      "Um modulo para escrever codigo SQL"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "Um 'Executor' é um processo what executa tarefas individuais e armazena data em cache conforme necessário.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'Parquet'?",
    "options": [
      "Um formato de arquivo colunares otimizado",
      "Uma linguagem de programacao",
      "Um tipo de cluster",
      "Uma ferramenta de view de data"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "'Parquet' é um formato de arquivo colunares what é eficiente para storage e processamento de data analíticos.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o principal benefício de usar o Delta Lake no Databricks?",
    "options": [
      "Facilidade na criacao de visualizacoes",
      "Suporte a transacoes ACID",
      "Execucao de Machine Learning",
      "Integracao com sistemas de terceiros"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "O Delta Lake oferece suporte a transações ACID, garantindo consistência e confiabilidade nos data.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "O what é o 'Auto Loader' no contexto do Databricks?",
    "options": [
      "Uma ferramenta para carregar data manualmente",
      "Um serviço para automatizar a ingestão de data",
      "Um recurso para limpar data duplicados",
      "Um modulo de Machine Learning"
    ],
    "answer": 1,
    "category": "Ingestão de data",
    "explanation": "O Auto Loader é um recurso what automatiza a ingestão contínua de data em tables Delta.",
    "sources": [
      "https://docs.databricks.com/en/ingestion/auto-loader/index.html"
    ]
  },
  {
    "question": "Which é a função da operação 'map()' no Spark?",
    "options": [
      "Combinar multiplos RDDs em um",
      "Aplicar uma funcao a cada elemento de um RDD",
      "Ordenar data dentro de uma partição",
      "Executar uma acao no driver"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "A funcao 'map()' aplica uma funcao especificada a cada elemento de um RDD ou DataFrame no Spark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'Auto Termination' de clusters?",
    "options": [
      "Encerramento automatico de clusters apos um periodo de inatividade",
      "Escalonamento automatico de recursos do cluster",
      "Reinicio automatico de clusters falhos",
      "Atualizacao automatica do software do cluster"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "'Auto Termination' encerra automaticamente um cluster se ele permanecer ocioso por um periodo definido, economizando recursos.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é um 'Notebook Job'?",
    "options": [
      "Um job what executa um notebook específico",
      "Um job what gerencia clusters",
      "Um job what realiza backup de data",
      "Um job what atualiza a interface de usuário"
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "Um 'Notebook Job' permite agendar a execucao de um notebook em um horario especifico ou de forma recorrente.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html",
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which das seguintes opções NÃO é um tipo de join no SQL?",
    "options": [
      "INNER JOIN",
      "OUTER JOIN",
      "CROSS JOIN",
      "UPPER JOIN"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "Nao existe um 'UPPER JOIN' no SQL; os joins comuns sao INNER, OUTER, LEFT, RIGHT e CROSS JOIN.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Spark, o what é 'reduceByKey()'?",
    "options": [
      "Combinar valores com base em uma chave comum",
      "Filtrar data duplicados",
      "Dividir data em partições menores",
      "Classificar data com base em uma chave"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'reduceByKey()' é uma transformação no Spark what combina valores what compartilham a mesma chave usando uma função de redução.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Data Governance' no contexto de gerenciamento de data?",
    "options": [
      "Análise de data para insights",
      "Processo de armazenar data em um data lake",
      "Conjunto de práticas para garantir a qualidade, segurança e gerenciamento de data",
      "Execução de querys em bancos de data"
    ],
    "answer": 2,
    "category": "Data Governance",
    "explanation": "'Data Governance' refere-se às políticas e procedimentos para gerenciar a disponibilidade, usabilidade, integridade e segurança dos data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what significa 'idempotência' no contexto de operações de data?",
    "options": [
      "Uma operação what falha se repetida",
      "Uma operação what produz o mesmo resultado mesmo se executada várias vezes",
      "Uma operação what sempre retorna valores únicos",
      "Uma operação what depende da ordem de execução"
    ],
    "answer": 1,
    "category": "Data Engineering",
    "explanation": "Uma operacao idempotente produz o mesmo resultado independentemente de quantas vezes e executada.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what são 'Global Temp Views'?",
    "options": [
      "Visualizacoes temporarias disponiveis apenas para a sessao atual",
      "Visualizacoes temporarias compartilhadas entre todas as sessoes e clusters",
      "tables permanentes armazenadas no sistema",
      "Visualizações usadas apenas para view de data"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "'Global Temp Views' são visualizações temporárias what podem ser acessadas por diferentes sessões e clusters.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para combinar registros de duas tables baseadas em uma column relacionada?",
    "options": [
      "UNION",
      "JOIN",
      "INTERSECT",
      "MERGE"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O 'JOIN' combina registros de duas tables com base em uma column compartilhada ou relacionada.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Spark, o what é 'Broadcast Variable'?",
    "options": [
      "Uma variavel compartilhada enviada a todos os executores",
      "Uma variável what muda frequentemente durante a execução",
      "Uma variavel usada apenas pelo driver",
      "Uma variável what armazena logs"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "Uma 'Broadcast Variable' é uma variável imutável what é distribuída para todos os executores para uso eficiente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'CAST()' no SQL?",
    "options": [
      "Converter um valor de um tipo de data para outro",
      "Remover espacos em branco de uma string",
      "Combinar duas ou mais strings",
      "Gerar numeros aleatorios"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A função 'CAST()' é usada para converter expressões de um tipo de data para outro.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Persistência' no contexto do Spark?",
    "options": [
      "Armazenar data em cache ou memória para reutilização",
      "Salvar data em disco permanentemente",
      "Remover data não utilizados",
      "Compactar data para economizar espaço"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "A 'Persistência' permite what os usuários armazenem RDDs em cache ou memória para uso repetido, melhorando o desempenho.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a diferença entre 'DROP' e 'TRUNCATE' no SQL?",
    "options": [
      "'DROP' remove todos os registros, 'TRUNCATE' remove a estrutura da table",
      "'DROP' remove a estrutura da table, 'TRUNCATE' remove todos os registros",
      "Nao ha diferenca entre eles",
      "'TRUNCATE' é mais lento what 'DROP'"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "'DROP' exclui a table inteira, incluindo estrutura e data; 'TRUNCATE' remove apenas os data, mantendo a estrutura.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No SQL, o what significa a cláusula 'GROUP BY'?",
    "options": [
      "Ordenar os resultados",
      "Filtrar registros",
      "Agrupar registros com valores idênticos em columns especificadas",
      "Limitar o numero de resultados"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "A cláusula 'GROUP BY' agrupa rows what compartilham valores em columns especificadas, permitindo agregações.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Parquet' no contexto do Spark?",
    "options": [
      "Um formato de arquivo colunares eficiente para analises",
      "Um modulo de Machine Learning",
      "Uma ferramenta de view",
      "Um tipo de cluster"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'Parquet' é um formato de arquivo colunares what oferece eficiência no storage e processamento analítico de data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto de view de data, para what é usado um gráfico de rows?",
    "options": [
      "Comparar categorias distintas",
      "Mostrar a distribuicao de uma variavel",
      "Representar tendencias ao longo do tempo",
      "Analisar a relacao entre duas variaveis numericas"
    ],
    "answer": 2,
    "category": "view de data",
    "explanation": "Gráficos de rows são ideais para mostrar how os valores de uma variável mudam ao longo do tempo.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para criar uma visão virtual de uma query?",
    "options": [
      "CREATE VIEW",
      "CREATE TABLE",
      "CREATE INDEX",
      "CREATE PROCEDURE"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "O comando 'CREATE VIEW' é usado para criar uma visão virtual com base nos resultados de uma query SQL.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para modificar os data existentes em uma table?",
    "options": [
      "ALTER TABLE",
      "UPDATE",
      "MODIFY",
      "CHANGE"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'UPDATE' é utilizado para modificar os data existentes em uma table específica.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No SQL, o what faz a cláusula 'HAVING'?",
    "options": [
      "Filtra resultados apos agregacoes",
      "Ordena os resultados",
      "Limita o numero de resultados",
      "Agrupa registros com valores identicos"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A clausula 'HAVING' e usada para filtrar grupos de resultados apos uma agregacao com 'GROUP BY'.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Spark, which é a função da operação 'filter()'?",
    "options": [
      "Aplicar uma funcao em cada elemento",
      "Selecionar elementos what atendem a uma condição",
      "Reduzir o numero de particoes",
      "Agrupar elementos com base em uma chave"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "A função 'filter()' no Spark é usada para retornar apenas os elementos what atendem a uma condição específica.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Spark, o what é uma 'Action'?",
    "options": [
      "Uma operação what retorna um valor ao driver",
      "Uma transformação de data",
      "Um método para definir schemas",
      "Uma ferramenta de view"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "Uma 'Action' é uma operação what força a computação e retorna um resultado ao programa do driver.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'USE DATABASE' no SQL?",
    "options": [
      "Criar um novo banco de data",
      "Selecionar um banco de data para uso",
      "Excluir um banco de data",
      "Atualizar registros em um banco de data"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'USE DATABASE' seleciona um banco de data específico para what as operações subsequentes sejam executadas nele.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'GRANT' no SQL?",
    "options": [
      "Remover permissoes de um usuario",
      "Conceder permissoes a um usuario",
      "Criar um novo banco de data",
      "Atualizar registros em uma table"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "O comando 'GRANT' é usado para conceder permissões específicas a usuários ou roles em objetos de banco de data.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "No contexto de segurança de data, o what é 'Mascaramento de data'?",
    "options": [
      "Criptografar data para proteção",
      "Ocultar data sensíveis em resultados de querys",
      "Excluir data obsoletos",
      "Fazer backup de data importantes"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "'Mascaramento de data' envolve ocultar informações sensíveis, mostrando valores fictícios ou parciais aos usuários.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Checkpointing' no Spark Streaming?",
    "options": [
      "Uma técnica para recuperar data perdidos",
      "Salvar o estado de um streaming em disco para tolerancia a falhas",
      "Uma forma de otimizar querys",
      "Uma tecnica de balanceamento de carga"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Checkpointing' salva o estado do streaming em disco, permitindo what o sistema se recupere em caso de falhas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é um 'Job Cluster'?",
    "options": [
      "Um cluster dedicado para execucao de jobs agendados",
      "Um cluster para uso interativo",
      "Um grupo de clusters interconectados",
      "Um cluster utilizado para storage de data"
    ],
    "answer": 0,
    "category": "Cluster Management",
    "explanation": "Um 'Job Cluster' e criado automaticamente para executar um job especifico e e encerrado apos a conclusao.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which é a função da cláusula 'ORDER BY' no SQL?",
    "options": [
      "Agrupar registros com valores identicos",
      "Filtrar resultados apos uma agregacao",
      "Ordenar os resultados de uma query",
      "Limitar o numero de resultados retornados"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "A cláusula 'ORDER BY' é usada para ordenar os resultados de uma query em ordem ascendente ou descendente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a função do comando 'DROP TABLE' no SQL?",
    "options": [
      "Remover todas as rows de uma table",
      "Excluir uma table do banco de data",
      "Limpar data duplicados de uma table",
      "Criar uma nova table vazia"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'DROP TABLE' é usado para excluir completamente uma table do banco de data, incluindo sua estrutura e data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'Unity Catalog'?",
    "options": [
      "Um catalog unificado de data para governança",
      "Um serviço de storage em nuvem",
      "Uma ferramenta de view",
      "Um editor de codigo"
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "O 'Unity Catalog' é um serviço de governança what oferece gerenciamento centralizado de metadados e permissões.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "Which é o papel do 'Command Mode' em um notebook do Databricks?",
    "options": [
      "Executar comandos do sistema operacional",
      "Navegar e editar celulas do notebook",
      "Compilar codigo de programacao",
      "Visualizar logs de execucao"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "O 'Command Mode' permite what o usuário navegue entre células e execute operações de edição no notebook.",
    "sources": [
      "https://docs.databricks.com/en/notebooks/index.html"
    ]
  },
  {
    "question": "O what é 'Lazy Evaluation' no Spark?",
    "options": [
      "Executar operacoes imediatamente",
      "Executar operações somente when necessário",
      "Otimizar querys em tempo real",
      "Dividir data em partições"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "'Lazy Evaluation' no Spark significa what as transformações não são executadas imediatamente, mas são adiadas até what uma ação seja chamada.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, which é a vantagem de usar 'Tables ACID'?",
    "options": [
      "Melhora a compressão de data",
      "Suporta transacoes com propriedades ACID",
      "Facilita a criacao de visualizacoes",
      "Permite querys em linguagem natural"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "tables ACID garantem Atomicidade, Consistência, Isolamento e Durabilidade nas transações de data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para conceder todas as permissões em um objeto a um usuário?",
    "options": [
      "GRANT ALL PRIVILEGES ON objeto TO usuario",
      "GRANT FULL ACCESS ON objeto TO usuario",
      "GRANT PERMISSIONS ON objeto TO usuario",
      "GRANT ACCESS ON objeto TO usuario"
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "O comando 'GRANT ALL PRIVILEGES ON objeto TO usuario' concede todas as permissoes disponiveis no objeto especificado.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/permissions/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é um 'Interactive Cluster'?",
    "options": [
      "Um cluster usado para executar jobs agendados",
      "Um cluster what permite desenvolvimento e análise interativos",
      "Um cluster what é sempre executado em segundo plano",
      "Um cluster utilizado apenas para storage de data"
    ],
    "answer": 1,
    "category": "Cluster Management",
    "explanation": "Um 'Interactive Cluster' é usado para desenvolvimento interativo, permitindo what os usuários executem comandos e vejam resultados imediatamente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which função SQL é usada para calcular a média de um conjunto de valores?",
    "options": [
      "SUM()",
      "COUNT()",
      "AVG()",
      "MAX()"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "A função 'AVG()' retorna a média aritmética dos valores em uma column.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'CREATE VIEW' no SQL?",
    "options": [
      "Criar uma nova table",
      "Atualizar uma table existente",
      "Criar uma visão virtual de uma query",
      "Excluir uma visao existente"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "O 'CREATE VIEW' cria uma visão virtual baseada em uma query SQL, what pode ser usada how uma table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "No Databricks, which é a função do 'Job Scheduler'?",
    "options": [
      "Agendar e gerenciar a execucao de jobs",
      "Visualizar data em tempo real",
      "Gerenciar permissoes de usuario",
      "Criar clusters de computacao"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "O 'Job Scheduler' permite agendar e monitorar a execução de jobs, how notebooks ou scripts.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which é a função do comando 'DESCRIBE TABLE' no SQL?",
    "options": [
      "Inserir data em uma table",
      "Mostrar a estrutura de uma table",
      "Excluir uma table",
      "Atualizar registros em uma table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'DESCRIBE TABLE' é usado para exibir a estrutura e os detalhes de uma table, how columns e tipos de data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Shuffling' no Spark?",
    "options": [
      "Processo de reorganização de data entre partições",
      "Remoção de data duplicados",
      "Combinacao de dois datasets",
      "storage de data em cache"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "'Shuffling' é o processo de redistribuir data entre partições, o what pode ser custoso em termos de desempenho.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é um 'Managed Table' no Databricks?",
    "options": [
      "Uma table cujos data são gerenciados pelo sistema",
      "Uma table externa",
      "Uma table temporária",
      "Uma table what existe apenas em memória"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Em uma 'Managed Table', o Databricks gerencia tanto os metadados quanto os data da table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "Which comando SQL é usado para remover uma column de uma table?",
    "options": [
      "DELETE COLUMN",
      "REMOVE COLUMN",
      "ALTER TABLE... DROP COLUMN",
      "DROP COLUMN FROM TABLE"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "O comando 'ALTER TABLE... DROP COLUMN' é usado para remover uma column específica de uma table existente.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a função da operação 'Collect' no Spark?",
    "options": [
      "Coletar data para o driver how um array",
      "Distribuir data entre executores",
      "Persistir data em disco",
      "Filtrar data com base em uma condição"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "A operação 'Collect' recupera todos os elementos do RDD ou DataFrame e os traz para o driver how um array.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Data Skew' em processamento distribuído?",
    "options": [
      "When os data estão uniformemente distribuídos?",
      "When a maioria dos data está concentrada em poucas chaves, causando desequilíbrio?",
      "When os data são corrompidos durante a transferência?",
      "When os data estão encriptados?"
    ],
    "answer": 1,
    "category": "Data Engineering",
    "explanation": "'Data Skew' ocorre when alguns nós processam muito mais data what outros, levando a um desempenho desigual.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'ALTER TABLE... RENAME COLUMN' no SQL?",
    "options": [
      "Alterar o tipo de data de uma column",
      "Renomear uma column existente em uma table",
      "Adicionar uma nova column a uma table",
      "Excluir uma column de uma table"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O comando 'ALTER TABLE... RENAME COLUMN' é usado para renomear uma column existente em uma table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, which é o propósito do 'Cluster Manager'?",
    "options": [
      "Gerenciar recursos computacionais",
      "Criar visualizações de data",
      "Escrever codigo SQL",
      "Armazenar data em tables"
    ],
    "answer": 0,
    "category": "Conceitos do Lakehouse",
    "explanation": "O 'Cluster Manager' e responsavel por gerenciar a criacao e operacao de clusters de computacao.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a diferença entre 'DataFrame' e 'Dataset' no Spark?",
    "options": [
      "DataFrames sao nao tipados, Datasets sao tipados",
      "Datasets sao nao tipados, DataFrames sao tipados",
      "Nao ha diferenca",
      "Datasets so suportam linguagem SQL"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "DataFrames não possuem tipagem forte e são how Datasets de Row; Datasets adicionam tipagem forte usando objetos Java/Scala.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o propósito do comando 'REVOKE' no SQL?",
    "options": [
      "Conceder permissoes a um usuario",
      "Remover permissoes de um usuario",
      "Atualizar data em uma table",
      "Criar um novo banco de data"
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "O 'REVOKE' e usado para remover permissoes previamente concedidas a um usuario ou role.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Repartition' no Spark?",
    "options": [
      "Combinar varias particoes em uma",
      "Aumentar ou diminuir o numero de particoes de um RDD ou DataFrame",
      "Ordenar os data dentro de uma partição",
      "Filtrar data em uma partição específica"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "A operação 'Repartition' altera o número de partições de um RDD ou DataFrame, o what pode ajudar no balanceamento de carga.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No SQL, o what faz a cláusula 'LIMIT'?",
    "options": [
      "Agrupa resultados",
      "Ordena resultados",
      "Restringe o número de rows retornadas",
      "Filtra registros com base em uma condicao"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "A cláusula 'LIMIT' é usada para especificar o número máximo de rows what a query deve retornar.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Pipeline de data' em Data Engineering?",
    "options": [
      "Uma sequência de etapas de processamento de data",
      "Um tipo de banco de data relacional",
      "Uma ferramenta de view de data",
      "Um método de compactação de data"
    ],
    "answer": 0,
    "category": "Data Engineering",
    "explanation": "Um 'Pipeline de data' é um conjunto de processos what extrai, transforma e carrega data de uma fonte para um destino.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Spark SQL' no contexto do Databricks?",
    "options": [
      "Uma interface para interagir com o Spark usando SQL",
      "Um banco de data interno do Spark",
      "Uma ferramenta para view de data",
      "Um modulo de Machine Learning"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "O Spark SQL permite what os usuários executem querys SQL sobre data armazenados no Apache Spark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a diferença entre 'INNER JOIN' e 'LEFT JOIN' no SQL?",
    "options": [
      "'INNER JOIN' retorna todas as rows, 'LEFT JOIN' retorna apenas correspondências",
      "'INNER JOIN' retorna correspondências exatas, 'LEFT JOIN' inclui todas as rows da table da esquerda",
      "'LEFT JOIN' retorna correspondências exatas, 'INNER JOIN' inclui todas as rows da table da esquerda",
      "'LEFT JOIN' nao existe no SQL"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "'INNER JOIN' retorna apenas as rows com correspondências em ambas as tables, enquanto 'LEFT JOIN' retorna todas as rows da table da esquerda, incluindo as não correspondidas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Data Lineage' em Data Governance?",
    "options": [
      "O histórico de origens e transformações dos data",
      "A qualidade dos data em um sistema",
      "A segurança aplicada aos data",
      "A estrutura de storage dos data"
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "'Data Lineage' refere-se ao rastreamento da origem dos data e todas as transformações what eles sofreram.",
    "sources": [
      "https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html"
    ]
  },
  {
    "question": "Which é o papel do 'Cluster Mode' no Databricks?",
    "options": [
      "Determinar a interface do usuario",
      "Definir how o driver e os executores são alocados",
      "Especificar o sistema operacional",
      "Configurar a seguranca do cluster"
    ],
    "answer": 1,
    "category": "Conceitos do Lakehouse",
    "explanation": "O 'Cluster Mode' determina a distribuicao do driver e dos executores em um cluster de computacao.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Data Cleansing'?",
    "options": [
      "Processo de coletar data de diferentes fontes",
      "Análise de data para extrair insights",
      "Processo de detectar e corrigir ou remover data incorretos ou inconsistentes",
      "storage de data em um data warehouse"
    ],
    "answer": 2,
    "category": "Data Engineering",
    "explanation": "'Data Cleansing' é o processo de melhorar a qualidade dos data, corrigindo ou removendo informações imprecisas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o propósito do comando 'CREATE INDEX' no SQL?",
    "options": [
      "Criar uma nova table",
      "Adicionar uma column a uma table",
      "Melhorar a velocidade das querys em uma table",
      "Excluir registros duplicados"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "O comando 'CREATE INDEX' é usado para criar um índice what acelera a recuperação de data em uma table.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da cláusula 'HAVING' no SQL?",
    "options": [
      "Filtrar resultados apos uma agregacao",
      "Ordenar os resultados",
      "Limitar o numero de resultados",
      "Agrupar registros com valores identicos"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A clausula 'HAVING' e usada para filtrar resultados apos uma operacao de agregacao com 'GROUP BY'.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é uma 'View' no contexto de bancos de data SQL?",
    "options": [
      "Uma cópia física de uma table",
      "Uma table temporária what armazena data",
      "Uma query armazenada what se comporta how uma table",
      "Um índice para acelerar querys"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Uma View é uma query armazenada no banco de data what pode ser tratada how uma table virtual.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Which função SQL é usada para arredondar um número para um número específico de decimais?",
    "options": [
      "ROUND()",
      "FLOOR()",
      "CEIL()",
      "TRUNC()"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A funcao 'ROUND()' e usada para arredondar um numero para um numero especificado de casas decimais.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'COALESCE()' no SQL?",
    "options": [
      "Combinar várias columns em uma",
      "Substituir valores nulos por um valor especificado",
      "Dividir uma column em várias",
      "Contar o numero de valores nao nulos"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A funcao 'COALESCE()' retorna o primeiro valor nao nulo em uma lista de argumentos.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o propósito do 'SparkContext' no Spark?",
    "options": [
      "Gerenciar a configuracao e conexao com o cluster Spark",
      "Criar visualizações de data",
      "Executar querys SQL",
      "Armazenar data em tables"
    ],
    "answer": 0,
    "category": "Apache Spark",
    "explanation": "O 'SparkContext' e o ponto de entrada para a funcionalidade do Spark, permitindo a conexao com o cluster e a configuracao de parametros.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade do comando 'DELETE FROM' no SQL?",
    "options": [
      "Remover uma table",
      "Excluir registros específicos de uma table",
      "Limpar todos os data de uma table",
      "Excluir um banco de data inteiro"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "O 'DELETE FROM' remove registros específicos de uma table com base em uma condição.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which função SQL é usada para concatenar duas ou mais strings?",
    "options": [
      "CONCAT()",
      "MERGE()",
      "COMBINE()",
      "ATTACH()"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A funcao 'CONCAT()' e usada para unir duas ou mais strings em uma unica string.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what é 'Delta Lake Z-Ordering'?",
    "options": [
      "Um metodo de compactacao de arquivos",
      "Uma técnica de otimização what organiza os data para melhorar o desempenho de querys",
      "Uma funcao de seguranca",
      "Um tipo de storage em nuvem"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "'Z-Ordering' é uma técnica what ordena data em columns especificadas para otimizar o desempenho de querys no Delta Lake.",
    "sources": [
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "O what é 'Watermarking' no Spark Structured Streaming?",
    "options": [
      "Adicionar marcas d'água em data para segurança",
      "Limitar o estado necessario para agregacoes tardias",
      "Marcar data duplicados",
      "Priorizar certos data durante o processamento"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "'Watermarking' é usado para descartar data atrasados e limitar a quantidade de estado mantido para operações de agregação.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto do Spark, o what é uma 'Transformation'?",
    "options": [
      "Uma operação what retorna um valor ao driver",
      "Uma operação what cria um novo RDD a partir de um existente",
      "Um método para definir schemas",
      "Uma ferramenta de view"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "Uma 'Transformation' cria um novo RDD a partir de um existente, aplicando uma funcao de transformacao.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o papel do 'Driver' no Spark?",
    "options": [
      "Executar tarefas individuais",
      "Coordenar a execucao de jobs e tarefas",
      "Armazenar data em cache",
      "Visualizar resultados"
    ],
    "answer": 1,
    "category": "Apache Spark",
    "explanation": "O 'Driver' é o processo what coordena a execução do aplicativo Spark, gerenciando jobs e tarefas.",
    "sources": [
      "https://docs.databricks.com/en/workflows/jobs/jobs.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'NVL()' no SQL?",
    "options": [
      "Substituir valores nulos por um valor especificado",
      "Remover espacos em branco de uma string",
      "Converter numeros em strings",
      "Concatenar multiplas strings"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A funcao 'NVL()' retorna o segundo parametro se o primeiro for nulo; caso contrario, retorna o primeiro parametro.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a finalidade da função 'LOWER()' no SQL?",
    "options": [
      "Converter uma string para minusculas",
      "Encontrar o menor valor em uma column",
      "Remover caracteres especiais de uma string",
      "Comparar duas strings"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A funcao 'LOWER()' converte todos os caracteres de uma string para letras minusculas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, which recurso permite o processamento em tempo real de data de streaming?",
    "options": [
      "Delta Engine",
      "Structured Streaming",
      "Auto Loader",
      "Time Travel"
    ],
    "answer": 1,
    "category": "Streaming",
    "explanation": "O Structured Streaming é a API do Spark para processar fluxos de data em tempo real no Databricks.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks, o what é 'Delta Live Tables'?",
    "options": [
      "Uma ferramenta para criar pipelines de data em tempo real",
      "Uma biblioteca de view de data",
      "Um sistema de otimizacao de queries",
      "Um editor de codigo SQL"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "'Delta Live Tables' é usada para criar e gerenciar pipelines de data em tempo real no Databricks.",
    "sources": [
      "https://docs.databricks.com/en/workflows/delta-live-tables/delta-live-tables.html"
    ]
  },
  {
    "question": "An analyst is working with the following 'sales_data' table, which records sales amounts by region, product, and month. The analyst wants to generate a summary table that includes subtotals for all possible combinations of region, product, and month, as well as a grand total for all sales. Which of the following queries will produce the desired results?",
    "options": [
      "SELECT region, product, SUM(sales_amount) AS sales_amount FROM sales_data GROUP BY region, product;",
      "SELECT region, product, month, SUM(sales_amount) AS sales_amount FROM sales_data GROUP BY GROUPING SETS ((region, product, month), (region, product), (region), ());",
      "SELECT region, product, month, SUM(sales_amount) AS sales_amount FROM sales_data GROUP BY ROLLUP(region, product, month);",
      "SELECT region, product, month, SUM(sales_amount) AS sales_amount FROM sales_data GROUP BY CUBE(region, product, month);",
      "SELECT region, product, month, SUM(sales_amount) AS sales_amount FROM sales_data GROUP BY region, product, month;"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "A cláusula GROUP BY CUBE(region, product, month) gera todos os subtotais possíveis para as combinações das columns especificadas (region, product, month), bem how um total geral. Essa estrutura permite calcular subtotais para cada dimensão individual e suas combinações, atendendo ao requisito de incluir todos os subtotais e o total geral.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "An analyst needs to retrieve data from a 'customers' table in a database where the age is greater than 30 and the region is 'West'. The analyst wants to ensure that only customers who meet both conditions are included in the result. Which of the following SQL queries will correctly retrieve the desired data?",
    "options": [
      "SELECT * FROM customers WHERE age >= 30 AND region = 'West';",
      "SELECT * FROM customers WHERE age >= 30 AND region LIKE 'West%';",
      "SELECT * FROM customers WHERE age > 30 AND region = 'West';",
      "SELECT * FROM customers WHERE age > 30 OR region = 'West';"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "A query correta utiliza 'age > 30' e 'region = 'West'' com o operador AND para garantir what ambas as condições sejam satisfeitas simultaneamente. As outras opções incluem o operador OR, what retornaria registros what atendem apenas a uma das condições, ou o operador LIKE, what não é necessário, pois 'West' é uma correspondência exata.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "A data analyst is attempting to drop a table named 'sales_data'. The analyst wants to delete both the table's metadata and its data files. They execute the following command: DROP TABLE IF EXISTS sales_data; After running this command, the table no longer appears in the result of the SHOW TABLES command, but the data files still exist in the storage location. Which of the following explains why the data files still exist while the metadata was deleted?",
    "options": [
      "The table was replicated across multiple storage locations",
      "The table was unmanaged (external)",
      "The table was managed",
      "The table's data exceeded the storage limit",
      "The table had no defined schema"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A razão pela which os arquivos de data ainda existem é what a table era 'unmanaged' (externa). Em tables não gerenciadas, o comando DROP TABLE remove apenas o metadado, mas não deleta os arquivos físicos, what permanecem na localização de storage.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "No Databricks, o what é um 'Cluster de Alta Concurrência' e when deve ser usado?",
    "options": [
      "Um cluster otimizado para cargas de trabalho de streaming em tempo real.",
      "Um cluster what permite what múltiplos usuários compartilhem recursos de forma segura, ideal para ambientes multiusuário.",
      "Um cluster configurado para executar tarefas de machine learning intensivas.",
      "Um cluster dedicado exclusivamente a trabalhos agendados.",
      "Um cluster what automaticamente escala para o tamanho máximo sem restrições."
    ],
    "answer": 1,
    "category": "Gerenciamento de Clusters",
    "explanation": "A opção B está correta porwhat um 'Cluster de Alta Concurrência' no Databricks é projetado para suportar múltiplos usuários simultâneos, oferecendo isolamento e segurança. As outras opções não definem corretamente um cluster de alta concorrência.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No contexto de Data Lakes, o what significa o termo 'schema sob leitura'?",
    "options": [
      "Definir o schema dos data antes da ingestão.",
      "Aplicar o schema aos data somente no momento da leitura.",
      "Transformar data estruturados em não estruturados.",
      "Enforçar o schema durante a escrita dos data.",
      "Ignorar o schema completamente em todas as etapas."
    ],
    "answer": 1,
    "category": "Data Lakes",
    "explanation": "A opção B está correta porwhat 'schema sob leitura' significa what o schema é aplicado aos data somente when eles são lidos, permitindo flexibilidade na ingestão de data variados. As outras opções não descrevem corretamente o conceito.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em Spark, which é a diferença entre as operações 'map()' e 'flatMap()'?",
    "options": [
      "'map()' aplica uma funcao a cada elemento; 'flatMap()' faz o mesmo mas retorna apenas elementos unicos.",
      "'map()' pode alterar o numero de elementos; 'flatMap()' sempre retorna o mesmo numero de elementos.",
      "'map()' sempre retorna uma lista; 'flatMap()' retorna elementos individuais.",
      "'map()' aplica uma funcao a cada elemento; 'flatMap()' pode retornar multiplos elementos para cada entrada, achatando o resultado.",
      "Nao ha diferenca funcional entre 'map()' e 'flatMap()' em Spark."
    ],
    "answer": 3,
    "category": "Spark Transformacoes",
    "explanation": "A opção D está correta porwhat `map()` aplica uma função a cada elemento, enquanto `flatMap()` pode retornar múltiplos elementos para cada entrada e achata o resultado em uma única lista. As outras opções não descrevem corretamente as diferenças.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em aprendizagem de máquina, o what é 'regularização' e por what é importante?",
    "options": [
      "Um método para aumentar o tamanho do conjunto de data.",
      "Uma tecnica para reduzir o sobreajuste adicionando uma penalidade a funcao de custo.",
      "Uma forma de normalizar os data de entrada.",
      "Um algoritmo de otimizacao para acelerar o treinamento.",
      "Uma técnica para lidar com data faltantes."
    ],
    "answer": 1,
    "category": "Aprendizagem de Maquina",
    "explanation": "A opção B está correta porwhat a regularização adiciona uma penalidade à função de custo para reduzir a complexidade do modelo e evitar o sobreajuste. As outras opções não descrevem corretamente a regularização.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Spark SQL, how você converteria uma column 'data_str' com datas em formato string para o tipo Date?",
    "options": [
      "df.withColumn('data', df.data_str.cast('date'))",
      "df.select(to_date(df.data_str))",
      "df.withColumn('data', df.data_str.toDate())",
      "df.transform('data_str', 'date')",
      "df.convert(df.data_str, 'date')"
    ],
    "answer": 0,
    "category": "Transformação de data",
    "explanation": "A opção A está correta porwhat você pode usar `cast('date')` para converter uma column string em Date. As outras opções não são métodos padrão ou estão incorretas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Databricks SQL, which comando você usaria para exibir as funções disponíveis no ambiente atual?",
    "options": [
      "SHOW FUNCTIONS;",
      "LIST FUNCTIONS;",
      "DESCRIBE FUNCTIONS;",
      "DISPLAY FUNCTIONS;",
      "GET FUNCTIONS;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "A opção A está correta porwhat o comando `SHOW FUNCTIONS;` exibe a lista de funções disponíveis no ambiente SQL atual. As opções B, C, D e E não são comandos SQL padrão para listar funções.",
    "sources": [
      "https://docs.databricks.com/en/sql/admin/sql-warehouses.html"
    ]
  },
  {
    "question": "Which comando SQL você usaria para modificar o tipo de data da column 'preco' na table 'produtos' para DECIMAL(10,2)?",
    "options": [
      "ALTER TABLE produtos MODIFY COLUMN preco DECIMAL(10,2);",
      "CHANGE TABLE produtos COLUMN preco TO DECIMAL(10,2);",
      "ALTER TABLE produtos ALTER COLUMN preco DECIMAL(10,2);",
      "UPDATE produtos SET preco TYPE DECIMAL(10,2);",
      "MODIFY TABLE produtos SET preco DECIMAL(10,2);"
    ],
    "answer": [
      0,
      2
    ],
    "category": "SQL",
    "explanation": "As opções A e C estão corretas porwhat a sintaxe para alterar o tipo de data de uma column varia entre sistemas de banco de data. A opção A (`ALTER TABLE produtos MODIFY COLUMN preco DECIMAL(10,2);`) é usada em sistemas how MySQL, enquanto a opção C (`ALTER TABLE produtos ALTER COLUMN preco DECIMAL(10,2);`) é utilizada em sistemas how SQL Server. A opção B não é sintaticamente correta. A opção D está incorreta; `UPDATE` é usado para alterar data, não o schema da table. A opção E não é uma sintaxe SQL válida.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "No Delta Lake, which comando você usaria para remover arquivos órfãos não referenciados por uma table chamada 'eventos'?",
    "options": [
      "VACUUM eventos;",
      "CLEAN eventos;",
      "PURGE eventos;",
      "REMOVE ORPHANS FROM eventos;",
      "GC eventos;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "A opção A está correta porwhat o comando `VACUUM` remove arquivos antigos what não são mais referenciados pela table, liberando espaço em storage. As opções B, C, D e E não são comandos válidos para essa operação no Delta Lake.",
    "sources": [
      "https://docs.databricks.com/en/delta/optimizations/vacuum.html",
      "https://docs.databricks.com/en/delta/index.html"
    ]
  },
  {
    "question": "Em PySpark, which método você usaria para unir dois DataFrames 'df1' e 'df2' com base em uma column comum 'id'?",
    "options": [
      "df1.join(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.append(df2, 'id')",
      "df1.concat(df2, 'id')",
      "df1.union(df2, 'id')"
    ],
    "answer": 0,
    "category": "Transformação de data",
    "explanation": "A opção A está correta porwhat o método `join()` é usado para unir dois DataFrames com base em uma column comum. As opções B, C, D e E não são métodos padrão em PySpark para essa operação.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em SQL, which é a finalidade da cláusula 'WITH' no início de uma query?",
    "options": [
      "Definir querys auxiliares (CTEs) what podem ser referenciadas posteriormente na query.",
      "Estabelecer uma conexão com o banco de data.",
      "Especificar opções de configuração para a query.",
      "Aplicar filtros iniciais aos data.",
      "Indicar o usuário com permissões para executar a query."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A opção A está correta porwhat a cláusula `WITH` permite definir Common Table Expressions (CTEs), what são querys temporárias what podem ser referenciadas na query principal. As outras opções não refletem o uso da cláusula `WITH`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "In PySpark, how do you cache a DataFrame 'df' in memory to improve performance for multiple actions?",
    "options": [
      "df.cache()",
      "df.persist(storageLevel=StorageLevel.MEMORY_ONLY)",
      "df.storeInMemory()",
      "df.saveCache()",
      "df.memoryCache()"
    ],
    "answer": [
      0,
      1
    ],
    "category": "Spark Optimization",
    "explanation": "As opções A e B estão corretas. `df.cache()` armazena o DataFrame em cache usando o nível de storage padrão `MEMORY_AND_DISK`, o what significa what os data podem ser armazenados tanto na memória quanto no disco. Já `df.persist(storageLevel=StorageLevel.MEMORY_ONLY)` armazena o DataFrame somente na memória. Ambas as opções melhoram o desempenho ao reutilizar o DataFrame em múltiplas ações. As opções C, D e E não são métodos válidos em PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which cláusula SQL você usaria para ordenar os resultados de uma query pela column 'nome' em ordem alfabética?",
    "options": [
      "ORDER BY nome ASC;",
      "GROUP BY nome;",
      "SORT BY nome DESC;",
      "HAVING nome;",
      "FILTER BY nome;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A opção A está correta porwhat `ORDER BY nome ASC;` ordena os resultados pela column 'nome' em ordem crescente (alfabética). A opção B agrupa resultados, não os ordena. A opção C usa `DESC` para ordem decrescente. A opção D é usada para filtrar grupos após agregação. A opção E não é uma cláusula SQL padrão.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em Databricks, how você pode compartilhar notebooks com outros membros da equipe?",
    "options": [
      "Exportando o notebook how arquivo HTML e enviando por e-mail.",
      "Compartilhando um link direto para o notebook no workspace com as permissoes adequadas.",
      "Imprimindo o notebook e distribuindo copias fisicas.",
      "Reescrevendo o conteudo do notebook em um documento de texto.",
      "Nao e possivel compartilhar notebooks em Databricks."
    ],
    "answer": 1,
    "category": "Notebooks Databricks",
    "explanation": "A opção B está correta porwhat em Databricks você pode compartilhar notebooks diretamente através de links, desde what as permissões sejam configuradas adequadamente. As outras opções não são práticas ou estão incorretas.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "O what significa o conceito de 'Data Lakehouse' no contexto de gerenciamento de data?",
    "options": [
      "Uma plataforma what combina as capacidades de Data Lakes e Data Warehouses em um único sistema.",
      "Um repositório de data exclusivamente para data não estruturados.",
      "Uma ferramenta de view de data em tempo real.",
      "Um serviço de storage em nuvem para backup de data.",
      "Um algoritmo de aprendizado de máquina para limpeza de data."
    ],
    "answer": 0,
    "category": "Arquitetura de data",
    "explanation": "A opção A está correta porwhat o Data Lakehouse é uma arquitetura what une as funcionalidades de Data Lakes (storage de data brutos) e Data Warehouses (processamento e query de data estruturados) em uma única plataforma. As outras opções não definem corretamente o conceito de Data Lakehouse.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é o benefício de particionar uma table grande no contexto de big data?",
    "options": [
      "Melhorar a segurança dos data.",
      "Reduzir o espaco em disco utilizado.",
      "Acelerar querys filtradas em columns de partição.",
      "Simplificar o schema da table.",
      "Eliminar a necessidade de indices."
    ],
    "answer": 2,
    "category": "Processamento de data",
    "explanation": "A opção C está correta porwhat a partição de tables grandes permite what querys what filtram pelas columns de partição acessem apenas os data relevantes, melhorando o desempenho. As outras opções não são os principais benefícios da partição.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which das seguintes opções é uma prática recomendada ao escrever querys SQL para melhorar a legibilidade?",
    "options": [
      "Escrever todas as palavras-chave em letras minusculas.",
      "Evitar o uso de aliases para tables.",
      "Manter todas as cláusulas em uma única row longa.",
      "Usar recuo e quebras de row para separar as diferentes partes da query.",
      "Incluir várias instruções em uma única row separadas por ponto e vírgula."
    ],
    "answer": 3,
    "category": "Boas Praticas SQL",
    "explanation": "A opção D está correta porwhat usar recuo e quebras de row melhora a legibilidade de querys SQL complexas. As outras opções não são consideradas boas práticas para legibilidade.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which é a função principal do comando 'EXPLAIN' em uma query SQL?",
    "options": [
      "Executar a query e mostrar os resultados.",
      "Mostrar o plano de execução da query sem executá-la.",
      "Depurar erros de sintaxe na query.",
      "Converter a query em uma exibição armazenada.",
      "Otimizar automaticamente a query para melhor desempenho."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A opção B está correta porwhat o comando `EXPLAIN` exibe o plano de execução what o banco de data usará para executar a query, sem realmente executá-la. As outras opções não refletem o propósito do `EXPLAIN`.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em view de data, which tipo de gráfico é mais adequado para mostrar a tendência de uma variável ao longo do tempo?",
    "options": [
      "Grafico de barras",
      "Gráfico de rows",
      "Grafico de setores",
      "Histograma",
      "Grafico de dispersao"
    ],
    "answer": 1,
    "category": "view de data",
    "explanation": "A opção B está correta porwhat um gráfico de rows é ideal para mostrar tendências ao longo do tempo. As opções A, C, D e E não são as mais indicadas para representar tendências temporais.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
    ]
  },
  {
    "question": "Em PySpark, which método você usaria para filtrar um DataFrame 'df' para incluir apenas as rows where a column 'idade' é maior what 30?",
    "options": [
      "df.filter(df.idade > 30)",
      "df.select(df.idade > 30)",
      "df.where(df.idade > 30)",
      "df.limit(df.idade > 30)",
      "df.groupby(df.idade > 30)"
    ],
    "answer": [
      0,
      2
    ],
    "category": "Transformação de data",
    "explanation": "As opções A e C estão corretas porwhat os métodos `filter()` e `where()` são usados para filtrar rows com base em uma condição e são funcionalmente equivalentes em PySpark. A opção B está incorreta; `select()` é usado para selecionar columns. A opção D está incorreta; `limit()` é usado para limitar o número de rows sem condição. A opção E está incorreta; `groupby()` é usado para agrupar data.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Em um contexto de segurança de data, o what significa 'anonimização'?",
    "options": [
      "Criptografar os data para evitar acesso não autorizado.",
      "Remover ou alterar informacoes pessoalmente identificaveis para proteger a privacidade.",
      "Fazer backup dos data em um local seguro.",
      "Monitorar o acesso aos data em tempo real.",
      "Restringir o acesso aos data apenas a administradores."
    ],
    "answer": 1,
    "category": "Governança de data",
    "explanation": "A opção B está correta porwhat anonimização envolve remover ou alterar informações pessoais para what indivíduos não possam ser identificados. As outras opções não definem corretamente a anonimização.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },
  {
    "question": "Which of the following is an essential component of Apache Spark for processing structured data?",
    "options": [
      "Spark Streaming",
      "Spark SQL",
      "MLlib",
      "GraphX",
      "Spark Core"
    ],
    "answer": 1,
    "category": "Spark Concepts",
    "explanation": "Option B is correct because Spark SQL is the Apache Spark component used for processing structured data. The other options are components for other functionalities.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  },

  {
    "question": "In Databricks, which command is used to export a DataFrame 'df' to a CSV file at location '/tmp/data.csv'?",
    "options": [
      "df.write.csv('/tmp/data.csv')",
      "df.saveAsCsv('/tmp/data.csv')",
      "df.export.csv('/tmp/data.csv')",
      "df.to_csv('/tmp/data.csv')",
      "df.write.format('csv').save('/tmp/data.csv')"
    ],
    "answer": [
      0,
      4
    ],
    "category": "Data Export",
    "explanation": "As opções A e E estão corretas porwhat tanto `df.write.csv('/tmp/data.csv')` quanto `df.write.format('csv').save('/tmp/data.csv')` são métodos válidos em PySpark para escrever um DataFrame em um arquivo CSV no local especificado. Opção A é um atalho para salvar em CSV e funciona corretamente. Opções B e C não são métodos válidos em PySpark. Opção D é um método do pandas, não do PySpark.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table.html"
    ]
  },
  {
    "question": "In Spark, what is the purpose of 'persist()' compared to 'cache()'?",
    "options": [
      "'persist()' stores the data in memory; 'cache()' stores it on disk.",
      "'persist()' lets you choose the storage level; 'cache()' is equivalent to 'persist(StorageLevel.MEMORY_ONLY)'.",
      "'persist()' saves the DataFrame to the file system; 'cache()' saves it temporarily.",
      "'persist()' is used for structured data; 'cache()' for unstructured data.",
      "'persist()' is obsolete and has been replaced by 'cache()'."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because `persist()` allows you to specify the storage level (memory, disk, etc.), while `cache()` is a shortcut for `persist(StorageLevel.MEMORY_ONLY)`. The other options do not correctly describe the differences between the two methods.",
    "sources": [
      "https://docs.databricks.com/en/sql/language-manual/index.html"
    ]
  }

]
